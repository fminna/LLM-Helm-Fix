---
# Source: voltha-infra/charts/etcd/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: release-name-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.5.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  minAvailable: 51%
  selector:
    matchLabels:
      app.kubernetes.io/name: etcd
      app.kubernetes.io/instance: release-name
---
# Source: voltha-infra/charts/onos-classic/charts/atomix/templates/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: release-name-atomix-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: release-name-atomix
      chart: "atomix-0.1.9"
      release: "release-name"
---
# Source: voltha-infra/charts/bbsim-sadis-server/templates/permissions.yaml
apiVersion: "v1"
kind: "ServiceAccount"
metadata:
  name: "bbsim-sadis-server"
  namespace: "default"
---
# Source: voltha-infra/charts/kafka/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
  annotations:
automountServiceAccountToken: true
---
# Source: voltha-infra/charts/onos-classic/templates/onos-config-loader-permissions.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: onos-config-loader
---
# Source: voltha-infra/charts/etcd/templates/token-secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-etcd-jwt-token
  namespace: "default"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.5.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  jwt-token.pem: "LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlKSndJQkFBS0NBZ0VBdFYySG1tam1zb0xsQ2Vxd1F6eUxnWE94R2RISFBUT21HeDJjTVFUNDAvN3lMdEVVCjVBSDdLYWZGQWZLcEhHQWpJTGlMbWNMVnJtekxBeDQ3dDh0N0JjZ1BvY3Irb3lRbWxOTDBIMUFQQk5HVzF2YkMKZ1FDQzZ6SHpkckdQYWwvdHVTYUVKY1htdStwTDBUbGZWZ01MOE1TaFVyUVFwNTNyWkR4QUVmMDJLSGVYUkV5MQpPTlAzYm9mMERucG9CSXoxSmZ2cWREbVk3Ti9tVFVKb29aTEV1UDhWM1pUdm5GK0Z2Tk81MldkNWxWdTFNWEhUCnpqWmhoRHV1V0x1UkM0QWcyOGZmSmNzcDZYd1V6QU9Xd3g1NzhRK0IxM3FmRDhYZ0FDbkt5TUVvZndjcWEwR2cKVVBkQzhIMGVEMlJlNEEwZS9FNVlpcElGUXdLT0YyN2QzZ2xtbUErMzJOQWhrYk5pL1J3MjlxNmJ5MTM4L0VoVgpTaGZiaUE1Vi9DaE4yRjhQcDVQTDBuYUJ4a2hYME1QN25TTGY1TnpJRVlqaXdUMVBhaTROQjFKWmFmYVFmME9TCndEMXBKMHlhb0NodFExWndZeEQwZUhtY3E2cGZnUDNlcmZlUXVIemdqZWUwK0lCT3hKTjRDMTV3VjdHc1JpNHoKMWkyZUp4RXpkYmhQNXh2TDNVQ0V6cVZaT3IvTW1zV1piaXFqd25TQ3hzOU1GMlRpdjdObWNuRTM1aEx2em80MAoxb3NNUjlDNkZqT25sVDU4ZHBML2toRHpZWVZQSU96VUdEZXhyeDYyNk5Ia2lXakdVV3JDRXdzdmQrM01YMjk5CmRiVllBeTIwemNuY290NDBxcGpvb1pLa2RPL2ZDZGJUYlFMR3J0Szd2SkRVTEdtNnZQc1ZQK1IwTElFQ0F3RUEKQVFLQ0FnQmcvYXZQUDlwZ0RQQS81V3B1Q3Fyd2g5Wm1ZSEdVb1ZKcm5lSXh0Tjc5UUpKenh3ek5BdUkyM0tuVwo2TmJYZmZKTFJXWWl6TGRKbUpUWkRnb1J1SmdKOTN1a290S2FuSEtsbEV0Sms1NzBqTUdHSkdEUWJGMlhPaGRoCm1vTi85OTFCN3Y4czFERVl2YXZlTURoejN4MDduV3hJdGxOUjBIcGFHSDZMSUVjZVRSWndTRFdDTnBsVVd4Q0IKemFSakNwN3Z1VkVnNXN5Sm9ST2dyYXNXL1U2SFFLT0k1R1ZmMHhYS2JkQXVHOUJCdDlHbXA2RUYzcjgzNEZMVQowVG80VkVVK1JCMXFNTVF4UVZJOTZJMXZ1NVc4elBZN0xqQnlWdmY4SGwxOER4Q0JIbXZwQUNla0pSZC9WT29HCmdUN2J3eWlXOWE4VUxCTDBTeUdDRnFCQVpmYWU1UGZNUW84Nmc1T1VVcFFkNTAyNkhSUCtmMU9TR3U0TTlVY2IKcVFkdnZPWnNFWVh6eGczaVFIZUdadlJPSnlNU3gwemhtMnE2QTE1NlJHZnNEYlhUdU0vbDhRV0ZzR2J6MVY3WApzdG9zM0ozRURQOXR0U1dmNVdkYllaWUZaa0NFMDNGRDdlV3BNWjJJbGcya3ZNU1RlaUpGdklLTnRnUmI3YmoyCjlTSExHWWQrcTI2clN2VW1WNG1HRzVuVXRtb3RyaVNWQXh2Skw2dWxaeDdQQmxuTmZXMVRveGpHNGRCTk5BMVEKMkNraUdvMkZ2VDBPUW1ScCt2VFBMWnNpZXpvZWVRYUdKMFFGdURYNHpMS3NKa09naTRCU05MZlpUY05lQ2VHawpPUTRKd2VpSGZGU1JZWjNERHJEUU40STNEOFNoUzNLSmxvRzJWZWpybWRZNHJYWFlBUUtDQVFFQTNlVnIyS0NhClpTTFo3VXNSN2Z6UWdoOVVpdThVZ0VoVzEyaXBRMklSdVpHN2hvSnVUSWE0Sys4WGltRERVWkZPU2JPOHJxVTkKOHdPcllNa091MmdwOGUxbWl3SGF6d2duMklZd2dPK3FrRE9ibDNwZE1ZSk1kUFJ3ZkpGL1lYbTF5aDU4dE4zcwppUFhMWVlyMm5pOHF0WVBQUDNwampzY3NSN2dGQXdmWmxFUHFzV0RMMy9FT0taYk1LZ1RNR09TL2p3WndIZ0Q2CjNmYW1rMTZ1RVRrMDBqMCtZRUhwMlpnQTE1NUZtOWkvK0ROaTdSMFpuN2diZUxFSEZaNzFKL0dnZDRLQ3d0UmoKd3pMeXFQNExBcUhuY25OWldmMXlrR2dseXhDbTJCVnQ2VzRuQU5tRU91NTFmcjhHajBXeS9MZ2czMVRMbnpJZwpnTEg0c3JSYU1VaFlFUUtDQVFFQTBUMW8yL1N2NE84bUFHZVlXcUI5SllHMlV2bSt2M3YrcFF1ejVOY0d0NCs4CngzRndPd2dHZDloMU9BSGtNWmoxK3V0Y3pDQkJrTktEQUt2c0N5ZDNiSEs0NGRxSVhGam5LL1BaN3dBVkVwZDIKMExHWmFGV1dXUzkwdG5kZys5YkUySlF6Y25hVUg0QklsTEZYU2pWQTNLMnlCTEdLKzd3azV5R0l6QUhCUDZOYQpCazZ0VkJtU0hHODA0M1I5OUltRG45L0ZRSkZ2VFpBazZKcWdLcU55MmNZL2E1K0dUREdjYmpFNWNUb2FCazJECi95YlFHUG40Z3dPVjRQTDlKbFNra251RTNxbm95VEdMUTZPMkN5QXZNZ0d1ZEZMYmJTK1dzemxvSXlOMExxcS8KWlNJZlY3ZVpqd1BiQkJwVVpHYXFXV1p2MnUrNmc2Z01GQUlaUlRWOWNRS0NBUUJkbFh5K2lEMlZtZlUvVndIZwoxVmM3Q2V1bUROOGdtMkhFeDZkYmUranQwc0M4SWhaU3dCNjhxTUhNNHJKOW5FTGdZcVdaSXFmVjdoNUJXNHFUCkQ0TmhQVGFhT2RZR3F2SGpKL1VjNi9lTGliaDlVbVlyazBDYnN0d2VMVjhEeFVrZVRXMWErcDNmeFlwWlFUcHgKeklLK3V6Q1ZIUkVRRG9CbVh2MytJVGxxaHBwYzVPNFhMSnRFZnd6c1M4bW9WaEZVU09yUVBLbVJJNDF0YWVrUAo2b2lVZ3EvaitPaWZsTFFHUEFJdmtHS21VMFhOcXhBN2hISlJuSmZZVG9vbXBvbCszZFE0UjBzcjhEalhmUTAzCnR3RGVERGdLOUJGMVJmZHFoVU5ZcnZFRUdpam8rdFFCNXNBM0RHbS8weTJCMkVaRTZvWldSZFhmY1MvcWJwWDcKWXYrQkFvSUJBRmlZN2h1a3dndzJkeHJZd3JRdEhKQ1RHZ2FNWlc1akxISHFRS1AzUUlyZFBBd2ZSbXhZZTVTbgpmUXJZRWkzR0hQSEFVeXpEdzc5MHc2KzNIRjJrdW5hYkpSbk5GTXcxZ0wwbHBmUURUdWp1WllyOGlINmJMdXhZCm9jelpRNkdmVVA1NzBrVDNYTVR4NXNtL1JielFTOFNsYjZsNUcrWjN4aEJ3TEtKbFk5UjBjSndYR0dGeDZtSkwKM0ZNcmNtSG5FR3JUcjR3bGlNR0k5WGZnY1UxY3BEYitNTUI3YmJ3NWpZbERJcFdGQ3B2OG1wV2hjNS8veUhDQgpSaWRYL0VVTXRocW9YcHlCeVdYWHFxWmJYYXZjVnQrUHdyVFNtTUxrMGFUVisvaFUvTTc5MGNrYTU3YVdzVmpUClF4dTdvbHhZOUd5aWFWNkUvNjdicXpHVnFlbWFPcUVDZ2dFQVVCNTZUZnRoSytOVjdYczdMelNrQ2hQYzVKNnEKMUxncTNhMk1ZeG5PTVIxMU9mRmg1eGg3OU1LbVZtOHJDMkoxeDNva1pCTm9HSUZsKzUwa01TbjZDZXJQc3pmWApVUkplRTRqSHRSRGg5ck5RYysxKzdHeVFQOUFlSkxOTnBVZjNZcm5xb2I4YU1mTDVwa2tLVGhYM1FsNlU0ZnNECndpT0pQTHhhMlpYUVhMOGZxVk5ubjh1TW5hWGlIR1JYMUhXaFJPOEVLcFZNRG1CR0ZRNHhvVjloMHQrY01LQjMKUnhjdFlJTytvTW1uQmJJTUdtcDNsLysremUwNFFXY1RMblU3KzMxMUg1RmlscWZBR1hNbERPMGNNMmpNQXowWgo1UjlLOG5PdFpBeW8xeTZybmIzRWRjN08vQmRrMmNmYTlBQnpXRXRISjJ6cFd4YUJyRWhWVWFhK2t3PT0KLS0tLS1FTkQgUlNBIFBSSVZBVEUgS0VZLS0tLS0K"
---
# Source: voltha-infra/charts/freeradius/templates/freeradius-config.yaml
# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
data:
  clients.conf: |
    client 0.0.0.0/0 {
      secret = SECRET
    }
  users: |
    user Cleartext-Password := "password", MS-CHAP-Use-NTLM-Auth := 0

kind: ConfigMap
metadata:
  name: freeradius-config
---
# Source: voltha-infra/charts/kafka/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-zookeeper-scripts
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOST"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: voltha-infra/charts/kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-kafka-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  setup.sh: |-
    #!/bin/bash

    ID="${MY_POD_NAME#"release-name-kafka-"}"
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        export KAFKA_CFG_BROKER_ID="$(grep "broker.id" "/bitnami/kafka/data/meta.properties" | awk -F '=' '{print $2}')"
    else
        export KAFKA_CFG_BROKER_ID="$((ID + 0))"
    fi

    # Configure zookeeper client

    exec /entrypoint.sh /run.sh
---
# Source: voltha-infra/charts/onos-classic/charts/atomix/templates/configmap-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-atomix-config
  labels:
    app: release-name-atomix
    chart: "atomix-0.1.9"
    release: "release-name"
    heritage: "Helm"
data:
  atomix.conf: "cluster {\n  node: ${atomix.node}\n\n  discovery {\n    type: bootstrap\n    nodes: ${atomix.nodes}\n  }\n}\nmanagementGroup {\n  type: raft\n  name: system\n  partitionSize: 3\n  partitions: 1\n  members: ${atomix.members}\n}\npartitionGroups.raft {\n  type: raft\n  name: raft\n  partitionSize: 3\n  partitions: ${atomix.replicas}\n  members: ${atomix.members}\n}\n"
---
# Source: voltha-infra/charts/onos-classic/charts/atomix/templates/configmap-init.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-atomix-init-scripts
  labels:
    app: release-name-atomix
    chart: "atomix-0.1.9"
    release: "release-name"
    heritage: "Helm"
data:
  create-config.sh: |
    #!/usr/bin/env bash

    HOST=`hostname -s`
    DOMAIN=`hostname -d`
    SERVICE=${DOMAIN%%.}
    NODES=1

    function print_usage() {
    echo "\
    Usage: create-config [OPTIONS]
    Creates an Atomix configuration from the supplied arguments.
        --nodes             The number of nodes in the cluster. The default value is 1.
    "
    }

    function print_variables() {
        echo "atomix.service=$SERVICE"
        echo "atomix.node.id=$NAME-$ORD"
        echo "atomix.node.address=$NAME-$ORD.$DOMAIN:5679"
        echo "atomix.replicas=$NODES"
        for (( i=0; i<$NODES; i++ ))
        do
            echo "atomix.members.$i=$NAME-$((i))"
            echo "atomix.nodes.$i.id=$NAME-$((i))"
            echo "atomix.nodes.$i.address=$NAME-$((i)).$DOMAIN:5679"
        done
    }

    function create_config() {
        print_variables
    }

    optspec=":hv-:"
    while getopts "$optspec" optchar; do

        case "${optchar}" in
            -)
                case "${OPTARG}" in
                    nodes=*)
                        NODES=${OPTARG##*=}
                        ;;
                    *)
                        echo "Unknown option --${OPTARG}" >&2
                        exit 1
                        ;;
                esac;;
            h)
                print_usage
                exit
                ;;
            v)
                echo "Parsing option: '-${optchar}'" >&2
                ;;
            *)
                if [ "$OPTERR" != 1 ] || [ "${optspec:0:1}" = ":" ]; then
                    echo "Non-option argument: '-${OPTARG}'" >&2
                fi
                ;;
        esac
    done

    if [[ $HOST =~ (.*)-([0-9]+)$ ]]; then
        NAME=${BASH_REMATCH[1]}
        ORD=${BASH_REMATCH[2]}
    else
        echo "Failed to parse name and ordinal of Pod"
        exit 1
        #NAME=test
        #ORD=0
    fi

    create_config
---
# Source: voltha-infra/charts/onos-classic/templates/configmap-init.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-onos-classic-init-scripts
  labels:
    app: "onos-classic"
    chart: "onos-classic-0.1.31"
    release: "release-name"
    heritage: "Helm"
data:
  configure-onos.sh: |
    #!/usr/bin/env bash

    HOST=`hostname -s`
    DOMAIN=`hostname -d`
    ID=onos-`hostname -s | awk -F '-' '{print $NF}'`

    function print_config() {
        echo "{"
        print_name
        print_node
        print_storage
        echo "}"
    }

    function print_name() {
        echo "  \"name\": \"atomix\","
    }

    function print_node() {
        echo "  \"node\": {"
        echo "    \"id\": \"$ID\","
        echo "    \"host\": \"$HOST.$DOMAIN\","
        echo "    \"port\": 9876"
        echo "  },"
    }

    function print_storage() {
        echo "  \"storage\": ["
        for (( i=1; i<=$ATOMIX_REPLICAS; i++ ))
        do
            if [ $i -eq $ATOMIX_REPLICAS ]; then
                echo "    {"
                echo "      \"id\": \"$ATOMIX_SERVICE-$((i))\","
                echo "      \"host\": \"$ATOMIX_SERVICE-$((i-1)).$ATOMIX_SERVICE-hs\","
                echo "      \"port\": 5679"
                echo "    }"
            else
                echo "    {"
                echo "      \"id\": \"$ATOMIX_SERVICE-$((i))\","
                echo "      \"host\": \"$ATOMIX_SERVICE-$((i-1)).$ATOMIX_SERVICE-hs\","
                echo "      \"port\": 5679"
                echo "    },"
            fi
        done
        echo "  ]"
    }

    ATOMIX_SERVICE=$1
    ATOMIX_REPLICAS=$2

    until nslookup "$ATOMIX_SERVICE-api" > /dev/null 2>&1; do sleep 2; done;

    print_config
---
# Source: voltha-infra/charts/onos-classic/templates/configmap-probe.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-onos-classic-probe-scripts
  labels:
    app: "onos-classic"
    chart: "onos-classic-0.1.31"
    release: "release-name"
    heritage: "Helm"
data:
  check-onos-status: |
    #!/bin/bash

    # Wait for 5s before timing out
    host=`hostname -s`
    id=onos-`hostname -s | awk -F '-' '{print $NF}'`
    result=$(curl -m 5 -s -f http://$host:8181/onos/v1/cluster/$id --user onos:rocks)
    echo $result

    if ! printf '%q' $result | grep -q -i "READY"; then
      echo "Cluster is not yet ready"
      exit -1
    fi

    apps=()
    apps+=("org.onosproject.lldpprovider")
    apps+=("org.onosproject.openflow-base")
    apps+=("org.onosproject.gui2")
    apps+=("org.onosproject.drivers")
    apps+=("org.onosproject.mcast")
    apps+=("org.onosproject.segmentrouting")
    apps+=("org.opencord.kafka")
    apps+=("org.opencord.sadis")
    apps+=("org.opencord.dhcpl2relay")
    apps+=("org.opencord.igmpproxy")
    apps+=("org.opencord.mcast")
    apps+=("org.opencord.olt")
    apps+=("org.opencord.aaa")

    for app in ${apps[@]}; do
      result=$(curl -m 5 -s -f http://$host:8181/onos/v1/applications/$app/health --user onos:rocks)
      echo $result
      if ! printf %q $result | grep -q -i "READY"; then
        echo "$app is not yet ready"
        exit -1
      fi
    done
---
# Source: voltha-infra/charts/onos-classic/templates/onos-config-loader-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-onos-configs-data
data:
  
  netcfg.json: "{\n  \"apps\" : {\n    \"org.opencord.dhcpl2relay\" : {\n      \"dhcpl2relay\" : {\n        \"useOltUplinkForServerPktInOut\" : true\n      }\n    },\n    \"org.opencord.kafka\": {\n      \"kafka\" : {\n        \"bootstrapServers\" : \"release-name-kafka.default.svc:9092\"\n      }\n    },\n    \"org.opencord.aaa\" : {\n      \"AAA\": {\n        \"radiusConnectionType\" : \"socket\",\n        \"radiusHost\": \"release-name-freeradius.default.svc\",\n        \"radiusServerPort\": \"1812\",\n        \"radiusSecret\": \"SECRET\"\n      }\n    },\n    \"org.opencord.sadis\": {\n      \"sadis\": {\n        \"integration\": {\n          \"url\": \"http://bbsim-sadis-server.default.svc:58080/subscribers/%s\",\n          \"cache\": {\n            \"enabled\": true,\n            \"maxsize\": 50,\n            \"ttl\": \"PT1m\"\n          }\n        }\n      },\n      \"bandwidthprofile\": {\n        \"integration\": {\n          \"url\": \"http://bbsim-sadis-server.default.svc:58080/profiles/%s\",\n          \"cache\": {\n            \"enabled\": true,\n            \"maxsize\": 50,\n            \"ttl\": \"PT1m\"\n          }\n        }\n      }\n    }\n  }\n}\n"
  
  org.onosproject.net.device.impl.DeviceManager: "{\n  \"roleTimeoutSeconds\": \"120\"\n}\n"
  org.onosproject.net.flow.impl.FlowRuleManager: "{\n  \"purgeOnDisconnection\": \"false\"\n}\n"
  org.onosproject.net.group.impl.GroupManager: "{\n  \"purgeOnDisconnection\": \"false\"\n}\n"
  org.onosproject.net.meter.impl.MeterManager: "{\n  \"purgeOnDisconnection\": \"false\"\n}\n"
  org.onosproject.provider.lldp.impl.LldpLinkProvider: "{\n  \"enabled\": \"false\"\n}\n"
  org.opencord.olt.impl.OltFlowService: "{\n  \"enableDhcpOnNni\": \"true\",\n  \"defaultTechProfileId\": \"64\",\n  \"enableIgmpOnNni\": \"false\",\n  \"enableEapol\": \"true\",\n  \"enableDhcpV6\": \"false\",\n  \"enableDhcpV4\": \"true\"\n}\n"
---
# Source: voltha-infra/charts/onos-classic/templates/onos-config-loader-script.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-onos-configs-loader
data:
  status_check.sh: |
    #!/bin/sh
    set -euo pipefail

    REPLICAS=$(kubectl -n default get --no-headers sts -lapp=onos-classic -o custom-columns=':.spec.replicas')
    RUNNING_POD=$(kubectl -n default get --no-headers pods -lapp=onos-classic | grep Running | wc -l)
    READY_POD=$(kubectl -n default get --no-headers pods -lapp=onos-classic -o custom-columns=':.status.containerStatuses[*].ready' | grep true | wc -l)
    if ! [ $REPLICAS -eq $RUNNING_POD ]; then
      echo "Not all ONOS Pods are running"
      exit 1
    fi
    if ! [ $REPLICAS -eq $READY_POD ]; then
      echo "Not all ONOS Pods are ready"
      exit 1
    fi
    exit 0
  loader.sh: |
    #/bin/sh
    set -euo pipefail

    USER="karaf"
    PASSWORD="karaf"
    ONOS_V1_URL="http://release-name-onos-classic-hs:8181/onos/v1"
    NETCFG_URL="${ONOS_V1_URL}/network/configuration/"
    COMPONENT_CFG_URL="${ONOS_V1_URL}/configuration"
    NETCFG="/opt/configs/netcfg.json"
    RECONCILE_MODE="false";

    # run in tmp instead of root
    cd /tmp
    while true; do
      echo "-----------------------------------------"
      echo "Timestamp is "; date
      echo "-----------------------------------------"
      echo 'Waiting for ONOS Cluster to be ready';
      if ! sh "/opt/loader/status_check.sh" ; then
        sleep 10s;
        continue
      fi

      echo "-----------------------------------------"
      echo "Loading netcfg into ONOS";
      echo "-----------------------------------------"
      cat ${NETCFG}
      # Upper all MAC addresses
      # ONOS will convert all MAC addresses to upper so we have to ensure all MAC addresses are upper in netcfg
      sed -E "s/([[:xdigit:]]{2}:)[[:xdigit:]]{2}/\U&/g" ${NETCFG} > expect_config
      curl -sSL -u $USER:$PASSWORD -X GET --header 'Accept: application/json' $NETCFG_URL | jq > actual_config
      # Update if there is difference between two configs(actual_config and expect_config)
      # Do the one-direction comparison
      # Everything in expect_config should be in the actual_config
      if jd actual_config expect_config | grep "^+"; then
          echo "Update netcfg due to the difference"
          curl -sSL -u $USER:$PASSWORD -X POST -H 'Content-Type: application/json' $NETCFG_URL -d@${NETCFG}
          responseCode=$(curl --write-out '%{http_code}' --fail -sSL -u $USER:$PASSWORD -X POST -H 'Content-Type: application/json' $NETCFG_URL -d@${NETCFG})
          if [[ $responseCode == 207 ]]; then
            echo "Failed to load NETCFG, skip this round and try again later"
          else
            echo "Updated netcfg is:";
            curl -sSL -u $USER:$PASSWORD -X GET --header 'Accept: application/json' $NETCFG_URL | jq
          fi
      else
        echo "Update is unnecessary since the netcfg on the ONOS cluster is up to date"
      fi

      echo "-----------------------------------------"
      echo "Loading component configs into ONOS";
      echo "-----------------------------------------"
      CFGS=$(ls /opt/configs | grep -v netcfg.json);
      for CFG in ${CFGS};
      do
        CFG_FILE="/opt/configs/$CFG"
        echo "-----------------------------------------"
        echo "Check that component $CFG is active";
        echo "-----------------------------------------"
        until curl --fail -sSL -u $USER:$PASSWORD -X GET "${COMPONENT_CFG_URL}/$CFG";
        do
          echo "Waiting for $CFG to be active";
          sleep 5;
        done

        echo "Prepare to update $CFG config"
        cat $CFG_FILE
        curl --fail -sSL -u $USER:$PASSWORD -X GET "${COMPONENT_CFG_URL}/$CFG"  | jq --arg key "${CFG}" '.[$key]' > actual_config
        # Update if there is difference between two configs(actual_config and expect_config)
        # Do the one-direction comparison
        # Everything in expect_config should be in the actual_config
        if jd actual_config ${CFG_FILE} | grep "^+"; then
          echo "Update $CFG config"
          responseCode=$(curl --write-out '%{http_code}' --fail -sSL -u $USER:$PASSWORD -X POST "${COMPONENT_CFG_URL}/$CFG" -H Content-type:application/json -d @${CFG_FILE});
          if [[ $responseCode == 207 ]]; then
            echo "Failed to load $CFG, skip this round and try again later"
          else
            echo "Updated Component config for $CFG is:";
            curl --fail -sSL -u $USER:$PASSWORD -X GET "${COMPONENT_CFG_URL}/$CFG";
          fi
        else
          echo "Update is unnecessary since the $CFG on the ONOS cluster is up to date"
        fi
      done

      if [ "${RECONCILE_MODE}" != "true" ]; then
        echo "Break from reconcile loop";
        break;
      fi

      echo "Completed on: ";
      date;
      sleep 30s
    done

    echo "For non-reconcile_mode, keep running to act as daemon;"

    sleep infinite
---
# Source: voltha-infra/charts/bbsim-sadis-server/templates/permissions.yaml
apiVersion: "rbac.authorization.k8s.io/v1"
kind: ClusterRole
metadata:
  name: "release-name-bbsim-sadis-server-pod-svc-reader"
  namespace: "default"
rules:
  - apiGroups: [""]
    resources:
      - pods
      - services
    verbs:
      - get
      - list
      - watch
---
# Source: voltha-infra/templates/etcd-periodic-defrag-cluster-role.yaml
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "ClusterRole"
metadata:
  name: "release-name-etcd-defrag"
  namespace: "default"
rules:
  - apiGroups: [""]
    resources: ["pods", "pods/exec"]
    verbs: ["get", "list", "watch", "create"]
---
# Source: voltha-infra/charts/bbsim-sadis-server/templates/permissions.yaml
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "ClusterRoleBinding"
metadata:
  name: "release-name-bbsim-sadis-server-pod-svc-reader-binding"
  namespace: "default"
subjects:
  - kind: "ServiceAccount"
    name: "bbsim-sadis-server"
    namespace: "default"
roleRef:
    kind: "ClusterRole"
    name: "release-name-bbsim-sadis-server-pod-svc-reader"
    apiGroup: "rbac.authorization.k8s.io"
---
# Source: voltha-infra/templates/etcd-periodic-defrag-cluster-role-binding.yaml
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "ClusterRoleBinding"
metadata:
  name: "release-name-etcd-defrag"
  namespace: "default"
subjects:
  - kind: "ServiceAccount"
    name: "release-name-service-account"
    namespace: "default"
roleRef:
    kind: "ClusterRole"
    name: "release-name-etcd-defrag"
    apiGroup: "rbac.authorization.k8s.io"
---
# Source: voltha-infra/charts/onos-classic/templates/onos-config-loader-permissions.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: list-pods
rules:
  - apiGroups: [""]
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
  - apiGroups: ["apps"]
    resources:
      - statefulsets
    verbs:
      - get
      - list
      - watch
---
# Source: voltha-infra/charts/onos-classic/templates/onos-config-loader-permissions.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: list-pods-to-sa
subjects:
  - kind: ServiceAccount
    name: onos-config-loader
roleRef:
  kind: Role
  name: list-pods
  apiGroup: rbac.authorization.k8s.io
---
# Source: voltha-infra/charts/bbsim-sadis-server/templates/service.yaml
apiVersion: "v1"
kind: "Service"
metadata:
  name: "bbsim-sadis-server"
  namespace: "default"
spec:
  ports:
    - name: "http"
      port: 58080
      targetPort: 8080
  selector:
    app: "bbsim-sadis-server"
    release: "release-name"
---
# Source: voltha-infra/charts/etcd/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-etcd-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.5.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: client
      port: 2379
      targetPort: client
    - name: peer
      port: 2380
      targetPort: peer
  selector:
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: release-name
---
# Source: voltha-infra/charts/etcd/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.5.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: "client"
      port: 2379
      targetPort: client
      nodePort: null
    - name: "peer"
      port: 2380
      targetPort: peer
      nodePort: null
  selector:
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: release-name
---
# Source: voltha-infra/charts/freeradius/templates/freeradius-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: "release-name-freeradius"
  namespace: "default"
spec:
  ports:
    - name: radius-auth
      protocol: UDP
      port: 1812
      targetPort: 1812
    - name: radius-acc
      protocol: UDP
      port: 1813
      targetPort: 1813
    - name: radius
      port: 18120
      targetPort: 18120
  selector:
    app: radius
    release: "release-name"
---
# Source: voltha-infra/charts/kafka/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper-headless
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: zookeeper
---
# Source: voltha-infra/charts/kafka/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: zookeeper
---
# Source: voltha-infra/charts/kafka/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kafka-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: false
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
    - name: tcp-internal
      port: 9093
      protocol: TCP
      targetPort: kafka-internal
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: kafka
---
# Source: voltha-infra/charts/kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
      nodePort: null
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: kafka
---
# Source: voltha-infra/charts/onos-classic/charts/atomix/templates/service.yaml
# Headless service for stable DNS entries.
# The critical component of this service is support for publishing DNS entries before pods are ready.
# This is necessary for Atomix nodes to reach each other to form a quorum before transitioning to the ready state.
apiVersion: v1
kind: Service
metadata:
  name: release-name-atomix-hs
  labels:
    app: release-name-atomix
    chart: "atomix-0.1.9"
    release: "release-name"
    heritage: "Helm"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
  - name: release-name-atomix-node
    port: 5679
  publishNotReadyAddresses: true
  clusterIP: None
  selector:
    app: release-name-atomix
---
# Source: voltha-infra/charts/onos-classic/charts/atomix/templates/service.yaml
# Client service for connecting to any Atomix instance REST API.
apiVersion: v1
kind: Service
metadata:
  name: release-name-atomix-api
  labels:
    app: release-name-atomix
    chart: "atomix-0.1.9"
    release: "release-name"
    heritage: "Helm"
spec:
  ports:
  - name: release-name-atomix-api
    port: 5678
  selector:
    app: release-name-atomix
---
# Source: voltha-infra/charts/onos-classic/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-onos-classic-hs
  labels:
    app: "onos-classic"
    name: release-name-onos-classic
    chart: "onos-classic-0.1.31"
    release: "release-name"
    heritage: "Helm"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
  - name: openflow
    port: 6653
    protocol: "TCP"
  - name: ovsdb
    port: 6640
    protocol: "TCP"
  - name: east-west
    port: 9876
    protocol: "TCP"
  - name: cli
    port: 8101
    protocol: "TCP"
  - name: ui
    port: 8181
    protocol: "TCP"
  # ONOS and Atomix need to be able to talk to each other for ONOS to become ready
  publishNotReadyAddresses: true
  clusterIP: None
  selector:
    app: "onos-classic"
---
# Source: voltha-infra/charts/bbsim-sadis-server/templates/deployment.yaml
apiVersion: "apps/v1"
kind: "Deployment"
metadata:
  name: "bbsim-sadis-server"
  namespace: "default"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: "bbsim-sadis-server"
      release: "release-name"
  template:
    metadata:
      namespace: "default"
      labels:
        app: "bbsim-sadis-server"
        release: "release-name"
        app.kubernetes.io/name: "bbsim-sadis-server"
        app.kubernetes.io/version: "0.3.5"
        app.kubernetes.io/component: "sadis-server"
        app.kubernetes.io/part-of: "voltha"
        app.kubernetes.io/managed-by: "Helm"
        helm.sh/chart: "bbsim-sadis-server-0.3.4"
    spec:
      serviceAccountName: "bbsim-sadis-server"
      containers:
        - name: "sadis"
          image: "voltha/bbsim-sadis-server:0.3.5"
          imagePullPolicy: "Always"
          command: ["/app/bbsim-sadis-server"]
          args:
            - "-log_level=WARN"
            - "-log_format=json"
            - "-bbsim_sadis_port=50074"
---
# Source: voltha-infra/charts/freeradius/templates/freeradius-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "release-name-freeradius"
  namespace: "default"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: radius
      release: "release-name"
  template:
    metadata:
      labels:
        app: radius
        release: "release-name"
    spec:
      serviceAccountName: 
      containers:
        - name: radius
          image: freeradius/freeradius-server:3.0.21
          imagePullPolicy: Always
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: RADIUS_LISTEN_IP
              value: "*"
            - name: USERS_FILE
              value: "/etc/raddb/users"
            - name: RADIUS_CLIENTS
              value: "SECRET@0.0.0.0/0"
          ports:
            - containerPort: 1812
              name: radauth-port
            - containerPort: 1813
              name: radacc-port
            - containerPort: 18120
              name: radius-port
          volumeMounts:
            - name: freeradius-config
              mountPath: /etc/raddb/clients.conf
              subPath: clients.conf
            - name: freeradius-config
              mountPath: /etc/raddb/users
              subPath: users
      volumes:
        - name: freeradius-config
          configMap:
            name: freeradius-config
---
# Source: voltha-infra/charts/onos-classic/templates/onos-config-loader.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-onos-classic-onos-config-loader
  labels:
    app: onos-config-loader
    chart: onos-classic
    release: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: onos-config-loader
  template:
    metadata:
      labels:
        app: onos-config-loader
        release: release-name
      annotations:
        checksum/config: 6e2604ebbfec6e6405d2eed39c0de237971db0a7893f0b891447a6f8c83a042d
    spec:
      serviceAccountName: onos-config-loader
      containers:
        - name: onos-config-loader
          image: 'opencord/onos-classic-helm-utils:0.1.0'
          imagePullPolicy: IfNotPresent
          command:
            - "/bin/bash"
            - "/opt/loader/loader.sh"
          volumeMounts:
            - name: onos-configs
              mountPath: /opt/configs
            - name: onos-loader
              mountPath: /opt/loader
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - "/opt/loader/status_check.sh"
            initialDelaySeconds: 20
            periodSeconds: 15
            failureThreshold: 1
            timeoutSeconds: 5
      volumes:
        - name: onos-configs
          configMap:
            name: release-name-onos-configs-data
        - name: onos-loader
          configMap:
            name: release-name-onos-configs-loader
            defaultMode: 0777
---
# Source: voltha-infra/charts/etcd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-etcd
  namespace: "default"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.5.11
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: etcd
      app.kubernetes.io/instance: release-name
  serviceName: release-name-etcd-headless
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: etcd
        helm.sh/chart: etcd-8.5.11
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/token-secret: 94bd278ed704578e0a4dbe96c278a7abda813c7ced21db92e6cbc5ee97fa876f
    spec:
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: etcd
                    app.kubernetes.io/instance: release-name
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: "default"
      containers:
        - name: etcd
          image: docker.io/bitnami/etcd:3.5.6-debian-11-r10
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_STS_NAME
              value: "release-name-etcd"
            - name: ETCDCTL_API
              value: "3"
            - name: ETCD_ON_K8S
              value: "yes"
            - name: ETCD_START_FROM_SNAPSHOT
              value: "no"
            - name: ETCD_DISASTER_RECOVERY
              value: "no"
            - name: ETCD_NAME
              value: "$(MY_POD_NAME)"
            - name: ETCD_DATA_DIR
              value: "/bitnami/etcd/data"
            - name: ETCD_LOG_LEVEL
              value: "info"
            - name: ALLOW_NONE_AUTHENTICATION
              value: "yes"
            - name: ETCD_AUTH_TOKEN
              value: "jwt,priv-key=/opt/bitnami/etcd/certs/token/jwt-token.pem,sign-method=RS256,ttl=10m"
            - name: ETCD_ADVERTISE_CLIENT_URLS
              value: "http://$(MY_POD_NAME).release-name-etcd-headless.default.svc.cluster.local:2379,http://release-name-etcd.default.svc.cluster.local:2379"
            - name: ETCD_LISTEN_CLIENT_URLS
              value: "http://0.0.0.0:2379"
            - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
              value: "http://$(MY_POD_NAME).release-name-etcd-headless.default.svc.cluster.local:2380"
            - name: ETCD_LISTEN_PEER_URLS
              value: "http://0.0.0.0:2380"
            - name: ETCD_AUTO_COMPACTION_MODE
              value: "revision"
            - name: ETCD_AUTO_COMPACTION_RETENTION
              value: "1"
            - name: ETCD_CLUSTER_DOMAIN
              value: "release-name-etcd-headless.default.svc.cluster.local"
            - name: ETCD_ELECTION_TIMEOUT
              value: "5000"
            - name: ETCD_HEARTBEAT_INTERVAL
              value: "1000"
          envFrom:
          ports:
            - name: client
              containerPort: 2379
              protocol: TCP
            - name: peer
              containerPort: 2380
              protocol: TCP
          livenessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/etcd
            - name: etcd-jwt-token
              mountPath: /opt/bitnami/etcd/certs/token/
              readOnly: true
      volumes:
        - name: etcd-jwt-token
          secret:
            secretName: release-name-etcd-jwt-token
            defaultMode: 256
        - name: data
          emptyDir: {}
---
# Source: voltha-infra/charts/kafka/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: zookeeper
  serviceName: release-name-zookeeper-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-11.0.2
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: zookeeper
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.8.0-debian-11-r65
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: release-name-zookeeper-0.release-name-zookeeper-headless.default.svc.cluster.local:2888:3888::1 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_ENABLE_QUORUM_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: release-name-zookeeper-scripts
            defaultMode: 0755
        - name: data
          emptyDir: {}
---
# Source: voltha-infra/charts/kafka/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: kafka
  serviceName: release-name-kafka-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-20.0.2
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka
      annotations:
    spec:
      
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: kafka
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: kafka
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: release-name-kafka
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:3.3.1-debian-11-r25
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_CFG_ZOOKEEPER_CONNECT
              value: "release-name-zookeeper"
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: "INTERNAL"
            - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
              value: "INTERNAL:PLAINTEXT,CLIENT:PLAINTEXT"
            - name: KAFKA_CFG_LISTENERS
              value: "INTERNAL://:9093,CLIENT://:9092"
            - name: KAFKA_CFG_ADVERTISED_LISTENERS
              value: "INTERNAL://$(MY_POD_NAME).release-name-kafka-headless.default.svc.cluster.local:9093,CLIENT://$(MY_POD_NAME).release-name-kafka-headless.default.svc.cluster.local:9092"
            - name: ALLOW_PLAINTEXT_LISTENER
              value: "yes"
            - name: KAFKA_ZOOKEEPER_PROTOCOL
              value: PLAINTEXT
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_LOG_DIR
              value: "/opt/bitnami/kafka/logs"
            - name: KAFKA_CFG_DELETE_TOPIC_ENABLE
              value: "false"
            - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
              value: "true"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES
              value: "10000"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MS
              value: "1000"
            - name: KAFKA_CFG_LOG_RETENTION_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_RETENTION_CHECK_INTERVAL_MS
              value: "300000"
            - name: KAFKA_CFG_LOG_RETENTION_HOURS
              value: "168"
            - name: KAFKA_CFG_MESSAGE_MAX_BYTES
              value: "1000012"
            - name: KAFKA_CFG_LOG_SEGMENT_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_DIRS
              value: "/bitnami/kafka/data"
            - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
              value: "1"
            - name: KAFKA_CFG_NUM_IO_THREADS
              value: "8"
            - name: KAFKA_CFG_NUM_NETWORK_THREADS
              value: "3"
            - name: KAFKA_CFG_NUM_PARTITIONS
              value: "1"
            - name: KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR
              value: "1"
            - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
              value: "104857600"
            - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS
              value: "6000"
            - name: KAFKA_CFG_AUTHORIZER_CLASS_NAME
              value: ""
            - name: KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND
              value: "true"
            - name: KAFKA_CFG_SUPER_USERS
              value: "User:admin"
          ports:
            - name: kafka-client
              containerPort: 9092
            - name: kafka-internal
              containerPort: 9093
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
      volumes:
        - name: scripts
          configMap:
            name: release-name-kafka-scripts
            defaultMode: 0755
        - name: data
          emptyDir: {}
        - name: logs
          emptyDir: {}
---
# Source: voltha-infra/charts/onos-classic/charts/atomix/templates/statefulset.yaml
# This template provides a StatefulSet for Atomix nodes.
# The StatefulSet is used to ensure that nodes are assigned a persistent identity.
# The StatefulSet must be used with a headless service to ensure pods can communicate with each other through
# their persistent identities.
# The StatefulSet supports upgrades through a simple RollingUpdate strategy.
# An anti-affinity policy is used to ensure pods are scheduled on distinct hosts when enabled.
# To configure the pods, an init container is used to populate a properties file on an ephemeral volume. The
# configuration volume is shared with the primary container for configuration purposes.
# Additionally, a volume claim template is used for the Raft partition logs.
# It's recommended that local-storage be used with the StatefulSet.
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-atomix
  labels:
    app: release-name-atomix
    chart: "atomix-0.1.9"
    release: "release-name"
    heritage: "Helm"
spec:
  serviceName: release-name-atomix-hs
  selector:
    matchLabels:
      app: release-name-atomix
  replicas: 0
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: release-name-atomix
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - release-name-atomix
            topologyKey: kubernetes.io/hostname
      initContainers:
      - name: configure
        image: ubuntu:16.04
        env:
        - name: ATOMIX_NODES
          value: "1"
        command:
        - sh
        - -c
        - "/scripts/create-config.sh --nodes=$ATOMIX_NODES > /config/atomix.properties"
        volumeMounts:
        - name: init-scripts
          mountPath: /scripts
        - name: system-config
          mountPath: /config
      containers:
      - name: atomix
        image: "atomix/atomix:3.1.12"
        imagePullPolicy: IfNotPresent
        env:
        - name: JAVA_OPTS
          value: -Xmx2G -XX:-UseContainerSupport

        resources:
          requests:
            memory: 512Mi
            cpu: 0.5
          limits:
        ports:
        - name: client
          containerPort: 5678
        - name: server
          containerPort: 5679
        args:
        - --config
        - /etc/atomix/system/atomix.properties
        - /etc/atomix/user/atomix.conf
        - --ignore-resources
        - --data-dir=/var/lib/atomix/data
        - --log-level=INFO
        - --file-log-level=OFF
        - --console-log-level=INFO
        readinessProbe:
          httpGet:
            path: /v1/status
            port: 5678
          initialDelaySeconds: 10
          timeoutSeconds: 10
          failureThreshold: 6
        livenessProbe:
          httpGet:
            path: /v1/status
            port: 5678
          initialDelaySeconds: 60
          timeoutSeconds: 10
        volumeMounts:
        - name: system-config
          mountPath: /etc/atomix/system
        - name: user-config
          mountPath: /etc/atomix/user
      volumes:
      - name: init-scripts
        configMap:
          name: release-name-atomix-init-scripts
          defaultMode: 0744
      - name: user-config
        configMap:
          name: release-name-atomix-config
      - name: system-config
        emptyDir: {}
---
# Source: voltha-infra/charts/onos-classic/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-onos-classic
  labels:
    app: "onos-classic"
    name: release-name-onos-classic
    chart: "onos-classic-0.1.31"
    release: "release-name"
    heritage: "Helm"
spec:
  serviceName: release-name-onos-classic-hs
  selector:
    matchLabels:
      app: "onos-classic"
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: "onos-classic"
        name: release-name-onos-classic
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - onos-classic
            topologyKey: kubernetes.io/hostname
      initContainers:
        - name: onos-classic-init
          image: tutum/dnsutils:latest
          imagePullPolicy: IfNotPresent
          env:
            - name: ATOMIX_SERVICE
              value: release-name-atomix
            - name: ATOMIX_REPLICAS
              value: "0"
          command:
            - sh
            - -c
            - "/scripts/configure-onos.sh $ATOMIX_SERVICE $ATOMIX_REPLICAS > /config/cluster.json && touch /config/active"
          volumeMounts:
            - name: init-scripts
              mountPath: /scripts
            - name: config
              mountPath: /config
      containers:
      - name: onos-classic
        image: "voltha/voltha-onos:5.1.1"
        imagePullPolicy: Always
        resources:
          requests:
            memory: 512Mi
            cpu: 0.5
          limits:
        env:
          - name: JAVA_OPTS
            value: -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:-UseContainerSupport -Dkaraf.log.console=INFO -Dds.lock.timeout.milliseconds=10000 -Dlog4j2.formatMsgNoLookups=true

          - name: ONOS_APPS
            value: org.onosproject.lldpprovider,org.onosproject.openflow-base,org.onosproject.gui2,org.onosproject.drivers,org.onosproject.mcast,org.onosproject.segmentrouting,org.opencord.kafka,org.opencord.sadis,org.opencord.dhcpl2relay,org.opencord.igmpproxy,org.opencord.mcast,org.opencord.olt,org.opencord.aaa
        ports:
          - name: openflow
            containerPort: 6653
          - name: ovsdb
            containerPort: 6640
          - name: east-west
            containerPort: 9876
          - name: cli
            containerPort: 8101
          - name: ui
            containerPort: 8181
        readinessProbe:
          exec:
            command:
              - sh
              - -c
              - /root/onos/bin/check-onos-status
          initialDelaySeconds: 30
          periodSeconds: 15
          failureThreshold: 10
          # Workaround for the probes issue - https://github.com/kubernetes/kubernetes/issues/82987
          # we use in curl a timeout lower than this to avoid the deadline exceed. Also we dont't
          # allow the script to exit immeditately.
          timeoutSeconds: 10
        livenessProbe:
          exec:
            command:
              - sh
              - -c
              - /root/onos/bin/check-onos-status
          initialDelaySeconds: 300
          periodSeconds: 15
          timeoutSeconds: 10
        volumeMounts:
          - name: probe-scripts
            mountPath: /root/onos/bin/check-onos-status
            subPath: check-onos-status
          - name: config
            mountPath: /root/onos/config/cluster.json
            subPath: cluster.json
      volumes:
        - name: init-scripts
          configMap:
            name: release-name-onos-classic-init-scripts
            defaultMode: 484
        - name: probe-scripts
          configMap:
            name: release-name-onos-classic-probe-scripts
            defaultMode: 484
        - name: config
          emptyDir: {}
---
# Source: voltha-infra/charts/bbsim-sadis-server/templates/deployment.yaml
# Copyright 2020-2023 Open Networking Foundation (ONF) and the ONF Contributors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: voltha-infra/charts/bbsim-sadis-server/templates/permissions.yaml
# Copyright 2020-2023 Open Networking Foundation (ONF) and the ONF Contributors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: voltha-infra/charts/bbsim-sadis-server/templates/service.yaml
# Copyright 2020-2023 Open Networking Foundation (ONF) and the ONF Contributors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: voltha-infra/charts/freeradius/templates/freeradius-deploy.yaml
# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: voltha-infra/charts/freeradius/templates/freeradius-svc.yaml
# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: voltha-infra/charts/onos-classic/charts/atomix/templates/nodeports.yaml
# Sometimes, it is very hard monitoring or taking the snapshots through the proxy
---
# Source: voltha-infra/charts/onos-classic/templates/autoscaler.yaml
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/configmap-init.yaml
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/configmap-logging.yaml
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/configmap-probe.yaml
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/nodeports.yaml
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/nodeports.yaml
---

# workaround for . not working, see
# https://github.com/helm/helm/issues/1311

# Sometimes, it is very hard monitoring or taking the snapshots through the proxy
---
# Source: voltha-infra/charts/onos-classic/templates/onos-config-loader-configmap.yaml
# Copyright 2021-present Open Networking Foundation
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/onos-config-loader-permissions.yaml
# Copyright 2021-present Open Networking Foundation
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/onos-config-loader-script.yaml
# Copyright 2021-present Open Networking Foundation
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/onos-config-loader.yaml
# Copyright 2021-present Open Networking Foundation
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/podsecuritypolicy.yaml
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/pspclusterrole.yaml
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/psprolebinding.yaml
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/service.yaml
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0
---
# Source: voltha-infra/charts/onos-classic/templates/statefulset.yaml
# SPDX-FileCopyrightText: 2022 2020-present Open Networking Foundation <info@opennetworking.org>
#
# SPDX-License-Identifier: Apache-2.0

# This template provides a StatefulSet for ONOS nodes.
# The StatefulSet is used to ensure that nodes are assigned a persistent identity.
# The StatefulSet must be used with a headless service to ensure pods can communicate with each other through
# their persistent identities.
# The StatefulSet supports upgrades through a simple RollingUpdate strategy.
# An anti-affinity policy is used to ensure pods are scheduled on distinct hosts when enabled.
# To configure the pods, an init container is used to populate a properties file on an ephemeral volume. The
# configuration volume is shared with the primary container for configuration purposes.
---
# Source: voltha-infra/templates/etcd-periodic-defrag-cluster-role-binding.yaml
# Copyright 2020-2023 Open Networking Foundation (ONF) and the ONF Contributors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: voltha-infra/templates/etcd-periodic-defrag-cluster-role.yaml
# Copyright 2020-2023 Open Networking Foundation (ONF) and the ONF Contributors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: voltha-infra/templates/etcd-periodic-defrag.yaml
# Copyright 2021-2023 Open Networking Foundation (ONF) and the ONF Contributors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
# Source: voltha-infra/templates/ingress_etcd.yaml
# Copyright 2021-2023 Open Networking Foundation (ONF) and the ONF Contributors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

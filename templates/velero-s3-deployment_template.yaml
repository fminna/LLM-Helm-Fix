---
# Source: velero-s3-deployment/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: release-name-velero-s3-deployment
  labels:
    helm.sh/chart: velero-s3-deployment-0.2.3
    app.kubernetes.io/name: velero-s3-deployment
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  podSelector:
    matchLabels:
      helm.sh/chart: velero-s3-deployment-0.2.3
      app.kubernetes.io/name: velero-s3-deployment
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/version: "1.0"
      app.kubernetes.io/managed-by: Helm
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8
        - 192.168.0.0/16
        - 172.16.0.0/12
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: 'default'
      podSelector:
        matchLabels:
          app.kubernetes.io/name: minio
  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
    to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
  policyTypes:
  - Egress
---
# Source: velero-s3-deployment/charts/velero/templates/serviceaccount-server.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-velero-server
  namespace: default
  labels:
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
---
# Source: velero-s3-deployment/charts/velero/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-velero
  namespace: default
  labels:
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
type: Opaque
data:
  cloud: "W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkPWFjY2Vzc0tleQphd3Nfc2VjcmV0X2FjY2Vzc19rZXk9c2VjcmV0S2V5Cg=="
---
# Source: velero-s3-deployment/charts/velero/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-velero-server
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
subjects:
  - kind: ServiceAccount
    namespace: default
    name: release-name-velero-server
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
---
# Source: velero-s3-deployment/charts/velero/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-velero-server
  namespace: default
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
rules:
- apiGroups:
    - "*"
  resources:
    - "*"
  verbs:
    - "*"
---
# Source: velero-s3-deployment/charts/velero/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-velero-server
  namespace: default
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
subjects:
  - kind: ServiceAccount
    namespace: default
    name: release-name-velero-server
roleRef:
  kind: Role
  name: release-name-velero-server
  apiGroup: rbac.authorization.k8s.io
---
# Source: velero-s3-deployment/charts/velero/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-velero
  namespace: default
  labels:
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
spec:
  type: ClusterIP
  ports:
    - name: http-monitoring
      port: 8085
      targetPort: http-monitoring
  selector:
    name: velero
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
---
# Source: velero-s3-deployment/charts/velero/templates/restic-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: restic
  namespace: default
  labels:
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
spec:
  selector:
    matchLabels:
      name: restic
  template:
    metadata:
      labels:
        name: restic
        app.kubernetes.io/name: velero
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        helm.sh/chart: velero-2.30.1
    spec:
      serviceAccountName: release-name-velero-server
      securityContext:
        runAsUser: 0
      volumes:
        - name: cloud-credentials
          secret:
            secretName: release-name-velero
        - name: host-pods
          hostPath:
            path: /var/lib/kubelet/pods
        - name: scratch
          emptyDir: {}
      dnsPolicy: ClusterFirst
      containers:
        - name: restic
          image: "velero/velero:v1.9.0"
          imagePullPolicy: IfNotPresent
          command:
            - /velero
          args:
            - restic
            - server
          volumeMounts:
            - name: cloud-credentials
              mountPath: /credentials
            - name: host-pods
              mountPath: /host_pods
              mountPropagation: HostToContainer
            - name: scratch
              mountPath: /scratch
          env:
            - name: VELERO_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: VELERO_SCRATCH_DIR
              value: /scratch
            - name: AWS_SHARED_CREDENTIALS_FILE
              value: /credentials/cloud
          securityContext:
            privileged: false
          resources:
            limits:
              cpu: 1000m
              memory: 1024Mi
            requests:
              cpu: 500m
              memory: 512Mi
---
# Source: velero-s3-deployment/charts/velero/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-velero
  namespace: default
  labels:
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
    component: velero
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: velero
  template:
    metadata:
      labels:
        name: velero
        app.kubernetes.io/name: velero
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        helm.sh/chart: velero-2.30.1
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "8085"
        prometheus.io/scrape: "true"
    spec:
      restartPolicy: Always
      serviceAccountName: release-name-velero-server
      containers:
        - name: velero
          image: "velero/velero:v1.9.0"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http-monitoring
              containerPort: 8085
          command:
            - /velero
          args:
            - server
          resources:
            limits:
              cpu: 1000m
              memory: 512Mi
            requests:
              cpu: 500m
              memory: 128Mi
          volumeMounts:
            - name: plugins
              mountPath: /plugins
            - name: cloud-credentials
              mountPath: /credentials
            - name: scratch
              mountPath: /scratch
          env:
            - name: VELERO_SCRATCH_DIR
              value: /scratch
            - name: VELERO_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: LD_LIBRARY_PATH
              value: /plugins
            - name: AWS_SHARED_CREDENTIALS_FILE
              value: /credentials/cloud
      dnsPolicy: ClusterFirst
      initContainers:
        - image: velero/velero-plugin-for-aws:v1.5.0
          imagePullPolicy: IfNotPresent
          name: velero-plugin-for-aws
          volumeMounts:
          - mountPath: /target
            name: plugins
      volumes:
        - name: cloud-credentials
          secret:
            secretName: release-name-velero
        - name: plugins
          emptyDir: {}
        - name: scratch
          emptyDir: {}
---
# Source: velero-s3-deployment/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-velero-s3-deployment-object-storage-backup
  labels:
    helm.sh/chart: velero-s3-deployment-0.2.3
    app.kubernetes.io/name: velero-s3-deployment
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: velero-s3-deployment
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: velero-s3-deployment
        app.kubernetes.io/instance: release-name
    spec:
      securityContext:
        {}
      containers:
        - name: intermediate
          securityContext:
            {}
          image: "minio/mc:RELEASE.2022-07-06T14-54-36Z"
          env:
          - name: MC_HOST_local
            value: "http://accessKey:secretKey@minio.default:9000"
          - name: MC_HOST_backup
            value: "https://accessKey:secretKey@s3.example.com"
          command:
            - mc
            - mirror
            - --overwrite
            - --watch
            - --md5
            - local/radar-intermediate-storage
            - backup/radar-intermediate-storage
        - name: output
          securityContext:
            {}
          image: "minio/mc:RELEASE.2022-07-06T14-54-36Z"
          env:
          - name: MC_HOST_local
            value: "http://accessKey:secretKey@minio.default:9000"
          - name: MC_HOST_backup
            value: "https://accessKey:secretKey@s3.example.com"
          command:
            - mc
            - mirror
            - --overwrite
            - --watch
            - --md5
            - local/radar-output-storage
            - backup/radar-output-storage
---
# Source: velero-s3-deployment/charts/velero/templates/upgrade-crds.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-velero-upgrade-crds
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade,post-rollback
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
spec:
  backoffLimit: 3
  template:
    metadata:
      name: velero-upgrade-crds
    spec:
      serviceAccountName: release-name-velero-server
      initContainers:
        - name: kubectl
          image: "docker.io/bitnami/kubectl:1.26.14-debian-11-r6"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
          args:
            - -c
            - cp `which sh` /tmp && cp `which kubectl` /tmp
          volumeMounts:
            - mountPath: /tmp
              name: crds
      containers:
        - name: velero
          image: "velero/velero:v1.9.0"
          imagePullPolicy: IfNotPresent
          command:
            - /tmp/sh
          args:
            - -c
            - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -
          resources:
            limits:
              cpu: 1000m
              memory: 512Mi
            requests:
              cpu: 500m
              memory: 128Mi
          volumeMounts:
            - mountPath: /tmp
              name: crds
      volumes:
        - name: crds
          emptyDir: {}
      restartPolicy: OnFailure
---
# Source: velero-s3-deployment/charts/velero/templates/backupstoragelocation.yaml
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  name: default
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade,post-rollback
    "helm.sh/hook-delete-policy": before-hook-creation
  labels:
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
spec:
  provider: aws
  accessMode: ReadWrite
  objectStorage:
    bucket: "radar-base-backups"
  config:
    region: "eu-central-1"
    s3ForcePathStyle: "true"
    s3Url: "https://s3.amazon.com"
---
# Source: velero-s3-deployment/charts/velero/templates/schedule.yaml
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: release-name-velero-backup
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade,post-rollback
    "helm.sh/hook-delete-policy": before-hook-creation
  labels:
    app.kubernetes.io/name: velero
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    helm.sh/chart: velero-2.30.1
spec:
  schedule: "0 3 * * *"
  template:
    includeClusterResources: true
    includedNamespaces:
    - cert-manager
    - default
    - graylog
    - kubernetes-dashboard
    - monitoring
    - velero
    snapshotVolumes: false
    ttl: 240h

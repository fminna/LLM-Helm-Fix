---
# Source: apica-ascent/charts/flash-coffee/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flash-coffee
  namespace: default
  labels:
    app: flash-coffee
---
# Source: apica-ascent/charts/flash-discovery/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flash-discovery
  namespace: default
  labels:
    app: flash-discovery
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/controller-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    heritage: Helm
    release: release-name
  name: ingress-nginx
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/default-backend-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress-backend
---
# Source: apica-ascent/charts/logiq-flash/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: logiq-flash
  namespace: default
  labels:
    app: logiq-flash
---
# Source: apica-ascent/charts/logiqctl/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: logiqctl
  namespace: default
  labels:
    app: logiqctl
---
# Source: apica-ascent/charts/postgres/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: postgres
    chart: postgres-8.7.3
    release: "release-name"
    heritage: "Helm"
  name: postgres
  namespace: default
---
# Source: apica-ascent/charts/prometheus/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-kube-state-metrics
  namespace: default
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.1.19
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: apica-ascent/charts/prometheus/charts/node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-node-exporter
  namespace: default
  labels:
    app.kubernetes.io/name: node-exporter
    helm.sh/chart: node-exporter-2.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: apica-ascent/charts/prometheus/templates/alertmanager/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-prometheus-alertmanager
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: alertmanager
---
# Source: apica-ascent/charts/prometheus/templates/prometheus-operator/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-prometheus-operator
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: operator
---
# Source: apica-ascent/charts/prometheus/templates/prometheus/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-prometheus-prometheus
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: prometheus
---
# Source: apica-ascent/charts/redis/templates/redis-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: redis
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    release: release-name
    heritage: Helm
---
# Source: apica-ascent/charts/s3gateway/templates/post-install-prometheus-metrics-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: s3-gateway-update-prometheus-secret
  namespace: default
  labels:
    app: s3gateway-update-prometheus-secret
    chart: s3gateway-5.0.20
    release: release-name
    heritage: Helm
---
# Source: apica-ascent/charts/s3gateway/templates/serviceaccount-job.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: s3-gateway-job
  namespace: "default"
---
# Source: apica-ascent/charts/s3gateway/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "s3-gateway"
  namespace: "default"
---
# Source: apica-ascent/charts/thanos/templates/bucketweb/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-thanos-bucketweb
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: bucketweb
  annotations:
automountServiceAccountToken: true
---
# Source: apica-ascent/charts/thanos/templates/compactor/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-thanos-compactor
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: compactor
  annotations:
automountServiceAccountToken: true
---
# Source: apica-ascent/charts/thanos/templates/query/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-thanos-query
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
  annotations:
automountServiceAccountToken: true
---
# Source: apica-ascent/charts/thanos/templates/receive/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-thanos-receive
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: receive
  annotations:
automountServiceAccountToken: true
---
# Source: apica-ascent/charts/thanos/templates/ruler/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-thanos-ruler
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ruler
  annotations:
automountServiceAccountToken: true
---
# Source: apica-ascent/charts/thanos/templates/storegateway/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-thanos-storegateway
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: storegateway
  annotations:
automountServiceAccountToken: true
---
# Source: apica-ascent/charts/flash-coffee/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-flash-coffee
  namespace: default
  labels:
    app: flash-coffee
    chart: flash-coffee-1.0.0
    release: release-name
    heritage: Helm
type: Opaque
data:
  mail_server: "c210cC1tYWlsLXNlcnZlcg==" 
  mail_port: "NTg3" 
  mail_username: "dXNlcg==" 
  mail_password: "cGFzc3dvcmQ=" 
  mail_default_sender: "Zmxhc2gtYWRtaW5AZm9vLmNvbQ==" 
  postgres_coffee_db: "Y29mZmVl" 
  logiq_flash_host: "bG9naXEtZmxhc2g=" 
  logiq_flash_port: "OTk5OQ==" 
  s3_access: "bG9naXFfYWNjZXNz" 
  s3_secret: "bG9naXFfc2VjcmV0" 
  s3_bucket: "bG9naXE=" 
  s3_url: "aHR0cDovL3MzLWdhdGV3YXk6OTAwMA==" 
  admin_email: "Zmxhc2gtYWRtaW5AZm9vLmNvbQ=="
  admin_password: "Zmxhc2gtcGFzc3dvcmQ="
  admin_name: "Zmxhc2gtYWRtaW5AZm9vLmNvbQ=="
  admin_org: "Zmxhc2gtb3Jn"
  postgres_port: "NTQzMg==" 
  postgres_host: "cG9zdGdyZXM=" 
  postgres_password: "cG9zdGdyZXM=" 
  postgres_db: "cG9zdGdyZXM=" 
  postgres_user: "cG9zdGdyZXM=" 
  redis_host: "cmVkaXMtbWFzdGVy" 
  redis_port: "6379" 
  database_url: "cG9zdGdyZXNxbDovL3Bvc3RncmVzOnBvc3RncmVzQHBvc3RncmVzOjU0MzIvcG9zdGdyZXM="
  coffee_database_url: "cG9zdGdyZXNxbDovL3Bvc3RncmVzOnBvc3RncmVzQHBvc3RncmVzOjU0MzIvY29mZmVl"
---
# Source: apica-ascent/charts/flash-discovery/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: flash-discovery
  namespace: default
  labels:
    app: flash-discovery
    chart: flash-discovery-1.0.0
    release: release-name
    heritage: Helm
type: Opaque
data:
  postgres_port: "NTQzMg==" 
  postgres_host: "cG9zdGdyZXM=" 
  postgres_password: "cG9zdGdyZXM=" 
  postgres_db: "cG9zdGdyZXM=" 
  postgres_user: "cG9zdGdyZXM=" 
  database_url: "cG9zdGdyZXNxbDovL3Bvc3RncmVzOnBvc3RncmVzQHBvc3RncmVzOjU0MzIvcG9zdGdyZXM="
---
# Source: apica-ascent/charts/logiq-flash/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-logiq-flash
  namespace: default
  labels:
    app: logiq-flash
    chart: logiq-flash-1.0.0
    release: release-name
    heritage: Helm
type: Opaque
data:
  postgres_port: "NTQzMg==" 
  postgres_host: "cG9zdGdyZXM=" 
  postgres_password: "cG9zdGdyZXM=" 
  postgres_db: "cG9zdGdyZXM=" 
  postgres_user: "cG9zdGdyZXM=" 
  redis_host: "cmVkaXMtbWFzdGVy" 
  redis_port: "NjM3OQ==" 
  admin_email: "Zmxhc2gtYWRtaW5AZm9vLmNvbQ=="
  admin_password: "Zmxhc2gtcGFzc3dvcmQ="
---
# Source: apica-ascent/charts/logiqctl/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-logiqctl
  namespace: default
  labels:
    app: logiqctl
    chart: logiqctl-1.0.0
    release: release-name
    heritage: Helm
type: Opaque
data:
  admin_email: "Zmxhc2gtYWRtaW5AZm9vLmNvbQ=="
  admin_password: "Zmxhc2gtcGFzc3dvcmQ="
  admin_name: "Zmxhc2gtYWRtaW5AZm9vLmNvbQ=="
  admin_org: "Zmxhc2gtb3Jn"
---
# Source: apica-ascent/charts/postgres/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: postgres
  namespace: default
  labels:
    app: postgres
    chart: postgres-8.7.3
    release: "release-name"
    heritage: "Helm"
type: Opaque
data:
  postgresql-password: "cG9zdGdyZXM="
---
# Source: apica-ascent/charts/prometheus/templates/alertmanager/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-release-name-prometheus-alertmanager
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: alertmanager
data:
  alertmanager.yaml: "Z2xvYmFsOgogIHJlc29sdmVfdGltZW91dDogNW0KcmVjZWl2ZXJzOgotIG5hbWU6ICJudWxsIgpyb3V0ZToKICBncm91cF9ieToKICAtIGpvYgogIGdyb3VwX2ludGVydmFsOiA1bQogIGdyb3VwX3dhaXQ6IDMwcwogIHJlY2VpdmVyOiAibnVsbCIKICByZXBlYXRfaW50ZXJ2YWw6IDEyaAogIHJvdXRlczoKICAtIG1hdGNoOgogICAgICBhbGVydG5hbWU6IFdhdGNoZG9nCiAgICByZWNlaXZlcjogIm51bGwi"
---
# Source: apica-ascent/charts/s3gateway/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: s3-gateway
  namespace: default
  labels:
    app: s3gateway
    chart: s3gateway-5.0.20
    release: release-name
    heritage: Helm
type: Opaque
data:
  accesskey: "bG9naXFfYWNjZXNz"
  secretkey: "bG9naXFfc2VjcmV0"
  
  awsaccesskey: "bG9naXFfYWNjZXNz"
  
  
  awssecretkey: "bG9naXFfc2VjcmV0"
  
  
  mchosttarget: "aHR0cDovL2xvZ2lxX2FjY2Vzczpsb2dpcV9zZWNyZXRAczMtZ2F0ZXdheTo5MDAw"
---
# Source: apica-ascent/charts/thanos/templates/objstore-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-thanos-objstore-secret
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  objstore.yml: |-
    dHlwZTogczMKcHJlZml4OiBsb2dpcS1pbnN0YXN0b3JlLW1ldHJpY3MKY29uZmlnOgoKICBhY2Nlc3Nfa2V5OiBsb2dpcV9hY2Nlc3MKICBzZWNyZXRfa2V5OiBsb2dpcV9zZWNyZXQKCiAgYnVja2V0OiBsb2dpcSAKCiAgZW5kcG9pbnQ6IHMzLWdhdGV3YXk6OTAwMCAKICBpbnNlY3VyZTogdHJ1ZQoKICByZWdpb246IHVzLWVhc3QtMSA=
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/controller-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    component: ""
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress
data:
  use-gzip: "true" # ENABLE GZIP COMPRESSION
  gzip-types: "*" 
  gzip-level: "9"
  keep-alive: "10"
  keep-alive-requests: "50"
  map-hash-bucket-size: "128"
  ssl-protocols: SSLv2 SSLv3 TLSv1 TLSv1.1 TLSv1.2 TLSv1.3
  disable-access-log: "false"
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/tcp-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    component: ""
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress-tcp
data:
  "514": logiq-flash:514
  "515": logiq-flash:515
  "2514": logiq-flash:2514
  "7514": logiq-flash:7514
  "7515": logiq-flash:7515
  "8081": logiq-flash-ml:8081
  "9998": logiq-flash:9998
  "9999": logiq-flash:9999
  "14250": logiq-flash:14250
  "20514": logiq-flash:20514
  "24224": logiq-flash:24224
  "24225": logiq-flash:24225
  "25224": logiq-flash:25224
  "25225": logiq-flash:25225
---
# Source: apica-ascent/charts/logiq-flash/templates/configMap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: logiq-config
  namespace: default
data:
  config.yaml: |
    
        options:
    
            ca: "/flash/certs/ca.crt"
    
            cert: "/flash/certs/syslog.crt"
    
            key: "/flash/certs/syslog.key"
    
            host: 0.0.0.0
    
        partitions:
    
          -
    
            name: p_scheme
    
            fields:
    
              - namespace
    
              - appname
    
              - year
    
              - month
    
              - day
    
        credentials:
    
          -
    
            name: logiq
    
            s3:



              secret_key: logiq_secret
    
              access_key: logiq_access


    
        destinations:
    
          -
    
            name: default_log_store
    
            partition: p_scheme
    
            s3:
    
              bucket: logiq
    


              endpoint: http://s3-gateway:9000


    
              credential: logiq
    
        rules:
    
          -
    
            destination: default_log_store
---
# Source: apica-ascent/charts/logiqctl/templates/configmap.yaml
# Source: logiq/charts/logiqctl/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-logiqctl
  namespace: default
  labels:
    app: logiqctl
    chart: logiqctl-1.0.0
    release: release-name
    heritage: Helm
data:
  export: |-
  logiq.json: |
    {
        "datasources": {
            "1": {
                "name": "LogiqMonitoring",
                "options": {
                    "url": "http://default-thanos-query:9090"
                },
                "type": "logiq_prometheus"
                }
            }
    }
---
# Source: apica-ascent/charts/postgres/templates/extended-config-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-extended-configuration
  namespace: default
  labels:
    app: postgres
    chart: postgres-8.7.3
    release: "release-name"
    heritage: "Helm"
data:

  override.conf: |
    max_connections=400
    shared_buffers=1024MB
---
# Source: apica-ascent/charts/prometheus/templates/prometheus-operator/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-prometheus-operator
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: operator
data:
  prometheus-config-reloader: docker.io/bitnami/prometheus-operator:0.53.1-debian-10-r0
---
# Source: apica-ascent/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    heritage: Helm
    release: release-name
data:
  redis.conf: |-
    # User-supplied configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly no
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    repl-diskless-sync yes
  master.conf: |-
    dir /data
  replica.conf: |-
    dir /data
    slave-read-only yes
---
# Source: apica-ascent/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-health
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    heritage: Helm
    release: release-name
data:
  ping_readiness_local.sh: |-
    #!/bin/bash
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash
     response=$(
      timeout -s 9 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash
    response=$(
      timeout -s 9 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: apica-ascent/charts/s3gateway/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: s3-gateway
  namespace: default
  labels:
    app: s3gateway
    chart: s3gateway-5.0.20
    release: release-name
    heritage: Helm
data:
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for Minio service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=59 ; # Allow 60 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/accesskey) ; SECRET=$(cat /config/secretkey) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to Minio server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} config host add myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of `mc ls`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(${MC} ls myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }
    
    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
    
      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for `true`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo "Purging bucket '$BUCKET'."
          set +e ; # don't exit if this fails
          ${MC} rm -r --force myminio/$BUCKET
          set -e ; # reset `e` as active
        else
          echo "Bucket '$BUCKET' does not exist, skipping purge."
        fi
      fi
    
      # Create the bucket if it does not exist
      if ! checkBucketExists $BUCKET ; then
        echo "Creating bucket '$BUCKET'"
        ${MC} mb myminio/$BUCKET
      else
        echo "Bucket '$BUCKET' already exists."
      fi
    
      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
      ${MC} policy set $POLICY myminio/$BUCKET
    }
    
    # Try connecting to Minio instance
    scheme=http
    connectToMinio $scheme
    # Create the bucket
    createBucket logiq none false
---
# Source: apica-ascent/charts/thanos/templates/receive/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-thanos-receive
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: receive
data:
  hashrings.json: |-
    [
      {
        "endpoints": [
            "127.0.0.1:10901"
        ]
      }
    ]
---
# Source: apica-ascent/charts/thanos/templates/ruler/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-thanos-ruler-configmap
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ruler
data:
  ruler.yml: |-
    groups:
      - name: "metamonitoring"
        rules:
          - alert: "PrometheusDown"
            expr: absent(up{prometheus="apica/apica-prometheus-prometheus"})
---
# Source: apica-ascent/templates/tcpConfigMap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ingress-tcp-ports
  namespace: default
data:

  9998:
    default/logiq-flash:9998
  8081:
    default/logiq-flash-ml:8081
  7514:
    default/logiq-flash:7514
  517:
    default/logiq-flash:517
  25225:
    default/logiq-flash:25225
  20514:
    default/logiq-flash:20514
  2514:
    default/logiq-flash:2514
  14250: 
    default/logiq-flash:14250
  14268: 
    default/logiq-flash:14268
  
  14250: default/logiq-flash:14250
  20514: default/logiq-flash:20514
  24224: default/logiq-flash:24224
  24225: default/logiq-flash:24225
  2514: default/logiq-flash:2514
  25224: default/logiq-flash:25224
  25225: default/logiq-flash:25225
  514: default/logiq-flash:514
  515: default/logiq-flash:515
  7514: default/logiq-flash:7514
  7515: default/logiq-flash:7515
  8081: default/logiq-flash-ml:8081
  9998: default/logiq-flash:9998
  9999: default/logiq-flash:9999
---
# Source: apica-ascent/charts/thanos/templates/compactor/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: release-name-thanos-compactor
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: compactor
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "8Gi"
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
      - update
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - update
      - watch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses/status
    verbs:
      - update
---
# Source: apica-ascent/charts/logiq-flash/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: logiq-flash
    chart: logiq-flash-1.0.0
    heritage: Helm
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  name: release-name-logiq-flash
rules:
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - create
- apiGroups:
  - apiextensions.k8s.io
  resourceNames:
  - alertmanagers.monitoring.coreos.com
  - podmonitors.monitoring.coreos.com
  - prometheuses.monitoring.coreos.com
  - prometheusrules.monitoring.coreos.com
  - servicemonitors.monitoring.coreos.com
  - thanosrulers.monitoring.coreos.com
  resources:
  - customresourcedefinitions
  verbs:
  - get
  - update
- apiGroups:
  - monitoring.coreos.com
  resources:
  - alertmanagers
  - alertmanagers/finalizers
  - prometheuses
  - prometheuses/finalizers
  - thanosrulers
  - thanosrulers/finalizers
  - servicemonitors
  - podmonitors
  - prometheusrules
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - pods
  - pods/log
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - watch
  - create
  - delete
  - update
  - patch
  - list
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - get
  - list
  - watch
  - create
  - patch
  - delete
  - update
- apiGroups:
  - ""
  resources:
  - nodes
  - persistentvolumes
  - persistentvolumeclaims
  - namespaces
  - pods
  - deployments
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - patch
  - update
---
# Source: apica-ascent/charts/prometheus/charts/kube-state-metrics/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-kube-state-metrics
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.1.19
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: ["certificates.k8s.io"]
    resources:
      - certificatesigningrequests
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - configmaps
    verbs: ["list", "watch"]
  - apiGroups: ["batch"]
    resources:
      - cronjobs
    verbs: ["list", "watch"]
  - apiGroups: ["extensions", "apps"]
    resources:
      - daemonsets
    verbs: ["list", "watch"]
  - apiGroups: ["extensions", "apps"]
    resources:
      - deployments
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - endpoints
    verbs: ["list", "watch"]
  - apiGroups: ["autoscaling"]
    resources:
      - horizontalpodautoscalers
    verbs: ["list", "watch"]
  - apiGroups: ["extensions", "networking.k8s.io"]
    resources:
      - ingresses
    verbs: ["list", "watch"]
  - apiGroups: ["batch"]
    resources:
      - jobs
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - limitranges
    verbs: ["list", "watch"]
  - apiGroups: ["admissionregistration.k8s.io"]
    resources:
      - mutatingwebhookconfigurations
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - namespaces
    verbs: ["list", "watch"]
  - apiGroups: ["networking.k8s.io"]
    resources:
      - networkpolicies
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - nodes
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - persistentvolumeclaims
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - persistentvolumes
    verbs: ["list", "watch"]
  - apiGroups: ["policy"]
    resources:
      - poddisruptionbudgets
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - pods
    verbs: ["list", "watch"]
  - apiGroups: ["extensions", "apps"]
    resources:
      - replicasets
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - replicationcontrollers
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - resourcequotas
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - secrets
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources:
      - services
    verbs: ["list", "watch"]
  - apiGroups: ["apps"]
    resources:
      - statefulsets
    verbs: ["list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources:
      - storageclasses
    verbs: ["list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources:
      - volumeattachments
    verbs: ["list", "watch"]
---
# Source: apica-ascent/charts/prometheus/templates/prometheus-operator/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-prometheus-operator
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: operator
rules:
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - create
  - apiGroups:
      - apiextensions.k8s.io
    resourceNames:
      - alertmanagers.monitoring.coreos.com
      - podmonitors.monitoring.coreos.com
      - prometheuses.monitoring.coreos.com
      - prometheusrules.monitoring.coreos.com
      - servicemonitors.monitoring.coreos.com
      - thanosrulers.monitoring.coreos.com
      - probes.monitoring.coreos.com
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - update
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - alertmanagers
      - alertmanagers/finalizers
      - alertmanagerconfigs
      - prometheuses
      - prometheuses/finalizers
      - thanosrulers
      - thanosrulers/finalizers
      - servicemonitors
      - podmonitors
      - probes
      - prometheusrules
    verbs:
      - '*'
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - configmaps
      - secrets
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - list
      - delete
  - apiGroups:
      - ""
    resources:
      - services
      - services/finalizers
      - endpoints
    verbs:
      - get
      - create
      - update
      - delete
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
---
# Source: apica-ascent/charts/prometheus/templates/prometheus/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-prometheus-prometheus
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: prometheus
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/metrics
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - services
      - endpoints
      - pods
    verbs:
      - "get"
      - "list"
      - "watch"
  - apiGroups:
      - extensions
      - "networking.k8s.io"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - "get"
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kubernetes-ingress
subjects:
  - kind: ServiceAccount
    name: ingress-nginx
    namespace: default
---
# Source: apica-ascent/charts/logiq-flash/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: logiq-flash
    chart: logiq-flash-1.0.0
    heritage: Helm
  name: release-name-logiq-flash
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-logiq-flash
subjects:
- kind: ServiceAccount
  name: logiq-flash
  namespace: default
---
# Source: apica-ascent/charts/prometheus/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-kube-state-metrics
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.1.19
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-state-metrics
subjects:
  - kind: ServiceAccount
    name: release-name-kube-state-metrics
    namespace: default
---
# Source: apica-ascent/charts/prometheus/templates/prometheus-operator/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-prometheus-operator
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-prometheus-operator
subjects:
  - kind: ServiceAccount
    name: release-name-prometheus-operator
    namespace: default
---
# Source: apica-ascent/charts/prometheus/templates/prometheus/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-prometheus-prometheus
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-prometheus-prometheus
subjects:
  - kind: ServiceAccount
    name: release-name-prometheus-prometheus
    namespace: default
---
# Source: apica-ascent/charts/flash-coffee/templates/role.yaml
# Cluster role which grants access to the default pod security policy
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: flash-coffee
  namespace: default
  labels:
    app: flash-coffee
rules:
- apiGroups:
  - policy
  resourceNames:
  - flash-coffee
  resources:
  - podsecuritypolicies
  verbs:
  - use
- apiGroups:
  - extensions
  resourceNames:
  - flash-coffee
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
# Source: apica-ascent/charts/flash-discovery/templates/role.yaml
# Cluster role which grants access to the default pod security policy
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: flash-discovery
  namespace: default
  labels:
    app: flash-discovery
rules:
- apiGroups:
  - policy
  resourceNames:
  - flash-discovery
  resources:
  - podsecuritypolicies
  verbs:
  - use
- apiGroups:
  - extensions
  resourceNames:
  - flash-discovery
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/controller-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - update
      - watch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - "networking.k8s.io" # k8s 1.14+
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      - ingress-controller-leader-logiq-ingress-default
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - create
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:      ['policy']
    resources:      ['podsecuritypolicies']
    verbs:          ['use']
    resourceNames:  [release-name-kubernetes-ingress]
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/default-backend-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress-backend
rules:
  - apiGroups:      ['policy']
    resources:      ['podsecuritypolicies']
    verbs:          ['use']
    resourceNames:  [release-name-kubernetes-ingress-backend]
---
# Source: apica-ascent/charts/logiq-flash/templates/role.yaml
# Cluster role which grants access to the default pod security policy
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: logiq-flash
  namespace: default
  labels:
    app: logiq-flash
rules:
- apiGroups:
  - policy
  resourceNames:
  - logiq-flash
  resources:
  - podsecuritypolicies
  verbs:
  - use
- apiGroups:
  - extensions
  resourceNames:
  - logiq-flash
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
# Source: apica-ascent/charts/logiqctl/templates/role.yaml
# Role which grants access to the default pod security policy
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: logiqctl
  namespace: default
  labels:
    app: logiqctl
rules:
- apiGroups:
  - policy
  resourceNames:
  - logiqctl
  resources:
  - podsecuritypolicies
  verbs:
  - use
- apiGroups:
  - extensions
  resourceNames:
  - logiqctl
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
# Source: apica-ascent/charts/postgres/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: postgres
  namespace: default
  labels:
    app: popstgres
rules:
- apiGroups:
    - policy
  resourceNames:
    - postgres
  resources:
    - podsecuritypolicies
  verbs:
    - use
---
# Source: apica-ascent/charts/redis/templates/redis-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: redis
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    release: release-name
    heritage: Helm
rules:
  - apiGroups: ['policy']
    resources: ['podsecuritypolicies']
    verbs: ['use']
    resourceNames: [redis]
---
# Source: apica-ascent/charts/s3gateway/templates/post-install-prometheus-metrics-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: s3-gateway-update-prometheus-secret
  namespace: default
  labels:
    app: s3gateway-update-prometheus-secret
    chart: s3gateway-5.0.20
    release: release-name
    heritage: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
      - update
      - patch
    resourceNames:
      - s3-gateway-prometheus
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - servicemonitors
    verbs:
      - get
    resourceNames:
      - s3-gateway
---
# Source: apica-ascent/charts/s3gateway/templates/role-job.yaml
# Cluster role which grants access to the default pod security policy
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: s3-gateway-job
  namespace: default
  labels:
    app: s3-gateway-job
rules:
- apiGroups:
  - policy
  resourceNames:
  - s3-gateway-job
  resources:
  - podsecuritypolicies
  verbs:
  - use
- apiGroups:
  - extensions
  resourceNames:
  - s3-gateway-job
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
# Source: apica-ascent/charts/s3gateway/templates/role.yaml
# Cluster role which grants access to the default pod security policy
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: s3-gateway
  namespace: default
  labels:
    app: s3-gateway
rules:
- apiGroups:
  - policy
  resourceNames:
  - s3-gateway
  resources:
  - podsecuritypolicies
  verbs:
  - use
- apiGroups:
  - extensions
  resourceNames:
  - s3-gateway
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
# Source: apica-ascent/charts/flash-coffee/templates/role-binding.yaml
# Cluster role binding for default pod security policy granting all authenticated users access
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: flash-coffee
  namespace: default
  labels:
    app: flash-coffee
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: flash-coffee
subjects:
- kind: ServiceAccount
  name: flash-coffee
  namespace: default
---
# Source: apica-ascent/charts/flash-discovery/templates/role-binding.yaml
# Cluster role binding for default pod security policy granting all authenticated users access
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: flash-discovery
  namespace: default
  labels:
    app: flash-discovery
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: flash-discovery
subjects:
- kind: ServiceAccount
  name: flash-discovery
  namespace: default
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/controller-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-kubernetes-ingress
subjects:
  - kind: ServiceAccount
    name: ingress-nginx
    namespace: default
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/default-backend-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress-backend
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-kubernetes-ingress-backend
subjects:
  - kind: ServiceAccount
    name: release-name-kubernetes-ingress-backend
    namespace: default
---
# Source: apica-ascent/charts/logiq-flash/templates/role-binding.yaml
# Cluster role binding for default pod security policy granting all authenticated users access
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: logiq-flash
  namespace: default
  labels:
    app: logiq-flash
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: logiq-flash
subjects:
- kind: ServiceAccount
  name: logiq-flash
  namespace: default
---
# Source: apica-ascent/charts/logiqctl/templates/role-binding.yaml
# Role binding for default pod security policy granting all authenticated users access
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: logiqctl
  namespace: default
  labels:
    app: logiqctl
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: logiqctl
subjects:
- kind: ServiceAccount
  name: logiqctl
  namespace: default
---
# Source: apica-ascent/charts/postgres/templates/role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: postgres
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: postgres
subjects:
- kind: ServiceAccount
  name: postgres
---
# Source: apica-ascent/charts/redis/templates/redis-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: redis
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    release: release-name
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: redis
subjects:
- kind: ServiceAccount
  name: redis
---
# Source: apica-ascent/charts/s3gateway/templates/post-install-prometheus-metrics-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: s3-gateway-update-prometheus-secret
  namespace: default
  labels:
    app: s3gateway-update-prometheus-secret
    chart: s3gateway-5.0.20
    release: release-name
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: s3-gateway-update-prometheus-secret
subjects:
  - kind: ServiceAccount
    name: s3-gateway-update-prometheus-secret
    namespace: default
---
# Source: apica-ascent/charts/s3gateway/templates/role-binding-job.yaml
# Cluster role binding for default pod security policy granting all authenticated users access
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: s3-gateway-job
  namespace: default
  labels:
    app: s3-gateway-job
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: s3-gateway-job
subjects:
- kind: ServiceAccount
  name: s3-gateway-job
  namespace: default
---
# Source: apica-ascent/charts/s3gateway/templates/role-binding.yaml
# Cluster role binding for default pod security policy granting all authenticated users access
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: s3-gateway
  namespace: default
  labels:
    app: s3-gateway
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: s3-gateway
subjects:
- kind: ServiceAccount
  name: "s3-gateway"
  namespace: default
---
# Source: apica-ascent/charts/flash-coffee/templates/002-coffee-server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: coffee
  namespace: default
spec:
  type: ClusterIP
  ports:
  - name: "http"
    port: 80
    targetPort: 5000
  selector:
    app: coffee-server
status:
  loadBalancer: {}
---
# Source: apica-ascent/charts/flash-discovery/templates/discoveryService.yaml
apiVersion: v1
kind: Service
metadata:
  name: flash-discovery
  namespace: default
  labels:
    app: flash-discovery
    chart: flash-discovery-1.0.0
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8081
      targetPort: 8081
      protocol: TCP
      name: discovery-health
    - port: 4000
      targetPort: 4000
      protocol: TCP
      name: discovery-api

  selector:
    app: flash-discovery
    release: release-name
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/controller-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    component: ""
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    - name: "14250-tcp"
      port: 14250
      protocol: TCP
      targetPort: "14250-tcp"
    - name: "20514-tcp"
      port: 20514
      protocol: TCP
      targetPort: "20514-tcp"
    - name: "24224-tcp"
      port: 24224
      protocol: TCP
      targetPort: "24224-tcp"
    - name: "24225-tcp"
      port: 24225
      protocol: TCP
      targetPort: "24225-tcp"
    - name: "2514-tcp"
      port: 2514
      protocol: TCP
      targetPort: "2514-tcp"
    - name: "25224-tcp"
      port: 25224
      protocol: TCP
      targetPort: "25224-tcp"
    - name: "25225-tcp"
      port: 25225
      protocol: TCP
      targetPort: "25225-tcp"
    - name: "514-tcp"
      port: 514
      protocol: TCP
      targetPort: "514-tcp"
    - name: "515-tcp"
      port: 515
      protocol: TCP
      targetPort: "515-tcp"
    - name: "7514-tcp"
      port: 7514
      protocol: TCP
      targetPort: "7514-tcp"
    - name: "7515-tcp"
      port: 7515
      protocol: TCP
      targetPort: "7515-tcp"
    - name: "8081-tcp"
      port: 8081
      protocol: TCP
      targetPort: "8081-tcp"
    - name: "9998-tcp"
      port: 9998
      protocol: TCP
      targetPort: "9998-tcp"
    - name: "9999-tcp"
      port: 9999
      protocol: TCP
      targetPort: "9999-tcp"
  selector:
    app: kubernetes-ingress
    release: release-name
    app.kubernetes.io/component: controller
  type: "LoadBalancer"
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/default-backend-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    component: "default-backend"
    heritage: Helm
    release: release-name
  name: release-name-kubernetes-ingress-default-backend
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app: kubernetes-ingress
    release: release-name
    app.kubernetes.io/component: default-backend
  type: "ClusterIP"
---
# Source: apica-ascent/charts/logiq-flash/templates/service-headless-ml.yaml
apiVersion: v1
kind: Service
metadata:
  name: logiq-flash-ml
  namespace: default
  labels:
    app: logiq-flash-ml
    chart: logiq-flash-1.0.0
    release: release-name
    heritage: Helm
    promMonitor: logiq-flash
spec:
  clusterIP: None
  type: ClusterIP
  ports:
    - port: 16686
      targetPort: 16686
      protocol: TCP
      name: tracingui
    - port: 8081
      targetPort: 50054
      protocol: TCP
      name: grpc
    - port: 9998
      targetPort: 9998
      protocol: TCP
      name: webcli
    - port: 9999
      targetPort: 9999
      protocol: TCP
      name: api
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: liveness-port
  selector:
    app: logiq-flash-ml
    release: release-name
---
# Source: apica-ascent/charts/logiq-flash/templates/service-headless-sync.yaml
apiVersion: v1
kind: Service
metadata:
  name: logiq-flash-sync
  namespace: default
  labels:
    app: logiq-flash-sync
    chart: logiq-flash-1.0.0
    release: release-name
    heritage: Helm
    promMonitor: logiq-flash
spec:
  clusterIP: None
  type: ClusterIP
  ports:
    - port: 8081
      targetPort: 50054
      protocol: TCP
      name: grpc
    - port: 9998
      targetPort: 9998
      protocol: TCP
      name: webcli
    - port: 9999
      targetPort: 9999
      protocol: TCP
      name: api
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: liveness-port
  selector:
    app: logiq-flash-sync
    release: release-name
---
# Source: apica-ascent/charts/logiq-flash/templates/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: logiq-flash-headless
  namespace: default
  labels:
    app: logiq-flash-headless
    chart: logiq-flash-1.0.0
    release: release-name
    heritage: Helm
    promMonitor: logiq-flash
spec:
  clusterIP: None
  ports:
    - port: 14250
      targetPort: 14250
      protocol: TCP
      name: tracinggrpc
    - port: 8081
      targetPort: 50054
      protocol: TCP
      name: grpc
    - port: 9999
      targetPort: 9999
      protocol: TCP
      name: api
    - port: 9998
      targetPort: 9998
      protocol: TCP
      name: webcli
    - port: 514
      targetPort: 1514
      protocol: TCP
      name: syslog
    - port: 7514
      targetPort: 7514
      protocol: TCP
      name: syslogtls
    - port: 515
      targetPort: 1515
      protocol: TCP
      name: cefport
    - port: 7515
      targetPort: 7515
      protocol: TCP
      name: cefportls
    - port: 20514
      targetPort: 20514
      protocol: TCP
      name: relp
    - port: 2514
      targetPort: 2514
      protocol: TCP
      name: relptls
    - port: 24225
      targetPort: 24225
      protocol: TCP
      name: fluentfwdtls
    - port: 24224
      targetPort: 24224
      protocol: TCP
      name: fluentforward
    - port: 25224
      targetPort: 25224
      protocol: TCP
      name: filebeat
    - port: 25225
      targetPort: 25225
      protocol: TCP
      name: filebeatls
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: liveness-port
    - port: 517
      targetPort: 1517
      protocol: TCP
      name: rawtcp
  selector:
    app: logiq-flash
    release: release-name
---
# Source: apica-ascent/charts/logiq-flash/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: logiq-flash
  namespace: default
  labels:
    app: logiq-flash
    chart: logiq-flash-1.0.0
    release: release-name
    heritage: Helm
spec:
  type: NodePort
  ports:
    - port: 14250
      targetPort: 14250
      protocol: TCP
      name: tracinggrpc
    - port: 14268
      targetPort: 14268
      protocol: TCP
      name: tracingapi
    - port: 8081
      targetPort: 50054
      protocol: TCP
      name: grpc
    - port: 9998
      targetPort: 9998
      protocol: TCP
      name: webcli
    - port: 9999
      targetPort: 9999
      protocol: TCP
      name: api
    - port: 514
      targetPort: 1514
      protocol: TCP
      name: syslog
    - port: 7514
      targetPort: 7514
      protocol: TCP
      name: syslogtls
    - port: 515
      targetPort: 1515
      protocol: TCP
      name: cefport
    - port: 7515
      targetPort: 7515
      protocol: TCP
      name: cefportls
    - port: 2514
      targetPort: 2514
      protocol: TCP
      name: relptls
    - port: 20514
      targetPort: 20514
      protocol: TCP
      name: relp
    - port: 24225
      targetPort: 24225
      protocol: TCP
      name: fluentfwdtls
    - port: 24224
      targetPort: 24224
      protocol: TCP
      name: fluentforward
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: liveness-port
    - port: 25224
      targetPort: 25224
      protocol: TCP
      name: filebeat
    - port: 25225
      targetPort: 25225
      protocol: TCP
      name: filebeatls
    - port: 517
      targetPort: 1517
      protocol: TCP
      name: rawtcp
  selector:
    app: logiq-flash
    release: release-name
---
# Source: apica-ascent/charts/postgres/templates/metrics-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-metrics
  namespace: default
  labels:
    app: postgres
    chart: postgres-8.7.3
    release: "release-name"
    heritage: "Helm"
  annotations:
    prometheus.io/port: "9187"
    prometheus.io/scrape: "true"
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 9187
      targetPort: http-metrics
  selector:
    app: postgres
    release: release-name
    role: master
---
# Source: apica-ascent/charts/postgres/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-headless
  namespace: default
  labels:
    app: postgres
    chart: postgres-8.7.3
    release: "release-name"
    heritage: "Helm"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgres
    release: "release-name"
---
# Source: apica-ascent/charts/postgres/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: default
  labels:
    app: postgres
    chart: postgres-8.7.3
    release: "release-name"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgres
    release: "release-name"
    role: master
---
# Source: apica-ascent/charts/prometheus/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-state-metrics
  namespace: default
  annotations:
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.1.19
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: http
      nodePort: null
  selector:
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
---
# Source: apica-ascent/charts/prometheus/charts/node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-node-exporter
  namespace: default
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/name: node-exporter
    helm.sh/chart: node-exporter-2.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    jobLabel: node-exporter
spec:
  type: ClusterIP
  ports:
    - name: metrics
      port: 9100
      targetPort: metrics
      nodePort: null
  selector:
    app.kubernetes.io/name: node-exporter
    app.kubernetes.io/instance: release-name
---
# Source: apica-ascent/charts/prometheus/templates/alertmanager/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-prometheus-alertmanager
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: alertmanager
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9093
      targetPort: 9093
  selector:
    app.kubernetes.io/name: alertmanager
    alertmanager: release-name-prometheus-alertmanager
---
# Source: apica-ascent/charts/prometheus/templates/prometheus-operator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-prometheus-operator
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: operator
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: operator
---
# Source: apica-ascent/charts/prometheus/templates/prometheus/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-prometheus-prometheus
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: prometheus
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9090
      targetPort: 9090
      protocol: TCP
  selector:
    app.kubernetes.io/name: prometheus
    prometheus: release-name-prometheus-prometheus
---
# Source: apica-ascent/charts/prometheus/templates/prometheus/thanos-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-prometheus-prometheus-thanos
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: prometheus
    app.kubernetes.io/subcomponent: thanos
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: grpc
      port: 10901
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: prometheus
    prometheus: release-name-prometheus-prometheus
---
# Source: apica-ascent/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-headless
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: redis
    release: release-name
---
# Source: apica-ascent/charts/redis/templates/metrics-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-metrics
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
  - name: metrics
    port: 9121
    targetPort: metrics
  selector:
    app: redis
    release: release-name
---
# Source: apica-ascent/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-master
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: redis
    release: release-name
    role: master
---
# Source: apica-ascent/charts/redis/templates/redis-slave-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
  - name: redis
    port: 6379
    targetPort: redis
  selector:
    app: redis
    release: release-name
    role: slave
---
# Source: apica-ascent/charts/s3gateway/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: s3-gateway
  namespace: default
  labels:
    app: s3gateway
    chart: s3gateway-5.0.20
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: s3gateway
    release: release-name
---
# Source: apica-ascent/charts/s3gateway/templates/statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: s3-gateway-svc
  namespace: default
  labels:
    app: s3gateway
    chart: s3gateway-5.0.20
    release: "release-name"
    heritage: "Helm"
spec:
  publishNotReadyAddresses: true
  clusterIP: None
  ports:
    - name: http
      port: 9000
      protocol: TCP
  selector:
    app: s3gateway
    release: release-name
---
# Source: apica-ascent/charts/thanos/templates/bucketweb/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-thanos-bucketweb
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: bucketweb
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
      nodePort: null
  selector:
    app.kubernetes.io/name: thanos
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: bucketweb
---
# Source: apica-ascent/charts/thanos/templates/compactor/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-thanos-compactor
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: compactor
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 9090
      targetPort: http
      protocol: TCP
      name: http
      nodePort: null
  selector:
    app.kubernetes.io/name: thanos
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: compactor
---
# Source: apica-ascent/charts/thanos/templates/query/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-thanos-query
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 9090
      targetPort: http
      protocol: TCP
      name: http
      nodePort: null
    - port: 10901
      targetPort: grpc
      protocol: TCP
      name: grpc
      nodePort: null
  selector:
    app.kubernetes.io/name: thanos
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: query
---
# Source: apica-ascent/charts/thanos/templates/receive/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-thanos-receive
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: receive
  annotations:    
spec:
  type: ClusterIP
  ports:
    - port: 10902
      targetPort: http
      protocol: TCP
      name: http
      nodePort: null
    - port: 10901
      targetPort: grpc
      protocol: TCP
      name: grpc
      nodePort: null
    - port: 19291
      targetPort: remote-write
      protocol: TCP
      name: remote
      nodePort: null
  selector:
    app.kubernetes.io/name: thanos
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: receive
---
# Source: apica-ascent/charts/thanos/templates/ruler/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-thanos-ruler
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ruler
    prometheus-operator/monitor: 'true'
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 9090
      targetPort: http
      protocol: TCP
      name: http
      nodePort: null
    - port: 10901
      targetPort: grpc
      protocol: TCP
      name: grpc
      nodePort: null
  selector:
    app.kubernetes.io/name: thanos
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: ruler
---
# Source: apica-ascent/charts/thanos/templates/storegateway/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-thanos-storegateway
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: storegateway
    prometheus-operator/monitor: 'true'
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 9090
      targetPort: http
      protocol: TCP
      name: http
      nodePort: null
    - port: 10901
      targetPort: grpc
      protocol: TCP
      name: grpc
      nodePort: null
  selector:
    app.kubernetes.io/name: thanos
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: storegateway
---
# Source: apica-ascent/charts/prometheus/charts/node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-node-exporter
  namespace: default
  labels:
    app.kubernetes.io/name: node-exporter
    helm.sh/chart: node-exporter-2.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/instance: release-name
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  minReadySeconds: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: node-exporter
        helm.sh/chart: node-exporter-2.3.17
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      
      serviceAccountName: release-name-node-exporter
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: node-exporter
                    app.kubernetes.io/instance: release-name
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        runAsUser: 1001
        fsGroup: 1001
        runAsNonRoot: 
      containers:
        - name: node-exporter
          image: docker.io/bitnami/node-exporter:1.3.1-debian-10-r0
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --web.listen-address=0.0.0.0:9100
            - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
            - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /
              port: metrics
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /
              port: metrics
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly: true
            - name: sys
              mountPath: /host/sys
              readOnly: true
      hostNetwork: true
      hostPID: true
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/controller-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    heritage: Helm
    release: release-name
    app.kubernetes.io/component: controller
  name: release-name-kubernetes-ingress
  annotations:
    {}
spec:
  selector:
    matchLabels:
      app: kubernetes-ingress
      release: release-name
  replicas: 2
  revisionHistoryLimit: 10
  strategy:
    {}
  minReadySeconds: 0
  template:
    metadata:
      labels:
        app: kubernetes-ingress
        release: release-name
        component: ""
        app.kubernetes.io/component: controller
    spec:
      dnsPolicy: ClusterFirst
      containers:
        - name: release-name-kubernetes-ingress
          image: "k8s.gcr.io/ingress-nginx/controller:v1.2.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - /nginx-ingress-controller
            - --default-backend-service=default/release-name-kubernetes-ingress-default-backend
            - --election-id=ingress-controller-leader
            - --ingress-class=logiq-ingress-default
            - --configmap=default/release-name-kubernetes-ingress
            - --tcp-services-configmap=default/ingress-tcp-ports
          securityContext:
            capabilities:
                drop:
                - ALL
                add:
                - NET_BIND_SERVICE
            runAsUser: 101
            allowPrivilegeEscalation: true
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          livenessProbe:
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 15
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
            - name: "14250-tcp"
              containerPort: 14250
              protocol: TCP
            - name: "20514-tcp"
              containerPort: 20514
              protocol: TCP
            - name: "24224-tcp"
              containerPort: 24224
              protocol: TCP
            - name: "24225-tcp"
              containerPort: 24225
              protocol: TCP
            - name: "2514-tcp"
              containerPort: 2514
              protocol: TCP
            - name: "25224-tcp"
              containerPort: 25224
              protocol: TCP
            - name: "25225-tcp"
              containerPort: 25225
              protocol: TCP
            - name: "514-tcp"
              containerPort: 514
              protocol: TCP
            - name: "515-tcp"
              containerPort: 515
              protocol: TCP
            - name: "7514-tcp"
              containerPort: 7514
              protocol: TCP
            - name: "7515-tcp"
              containerPort: 7515
              protocol: TCP
            - name: "8081-tcp"
              containerPort: 8081
              protocol: TCP
            - name: "9998-tcp"
              containerPort: 9998
              protocol: TCP
            - name: "9999-tcp"
              containerPort: 9999
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          resources:
            {}
      hostNetwork: false


      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 60
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/default-backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kubernetes-ingress
    chart: kubernetes-ingress-1.37.0
    heritage: Helm
    release: release-name
    app.kubernetes.io/component: default-backend
  name: release-name-kubernetes-ingress-default-backend
spec:
  selector:
    matchLabels:
      app: kubernetes-ingress
      release: release-name
  replicas: 2
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:
        app: kubernetes-ingress
        release: release-name
        app.kubernetes.io/component: default-backend
    spec:
      containers:
        - name: kubernetes-ingress-default-backend
          image: "k8s.gcr.io/defaultbackend-amd64:1.5"
          imagePullPolicy: "IfNotPresent"
          args:
          securityContext:
            runAsUser: 65534
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 15
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          resources:
            {}
      serviceAccountName: release-name-kubernetes-ingress-backend


      terminationGracePeriodSeconds: 60
---
# Source: apica-ascent/charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kube-state-metrics
  namespace: default
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.1.19
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: release-name
  replicas: 1
  strategy:
    {}
  minReadySeconds: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kube-state-metrics
        helm.sh/chart: kube-state-metrics-2.1.19
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
    spec:
      
      securityContext:
        runAsUser: 1001
        fsGroup: 1001
      hostNetwork: false
      serviceAccountName: release-name-kube-state-metrics
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: kube-state-metrics
                    app.kubernetes.io/instance: release-name
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      containers:
        - name: kube-state-metrics
          image: docker.io/bitnami/kube-state-metrics:2.3.0-debian-10-r0
          imagePullPolicy: IfNotPresent
          args:
            - --resources=certificatesigningrequests
            - --resources=configmaps
            - --resources=cronjobs
            - --resources=daemonsets
            - --resources=deployments
            - --resources=endpoints
            - --resources=horizontalpodautoscalers
            - --resources=ingresses
            - --resources=jobs
            - --resources=limitranges
            - --resources=mutatingwebhookconfigurations
            - --resources=namespaces
            - --resources=networkpolicies
            - --resources=nodes
            - --resources=persistentvolumeclaims
            - --resources=persistentvolumes
            - --resources=poddisruptionbudgets
            - --resources=pods
            - --resources=replicasets
            - --resources=replicationcontrollers
            - --resources=resourcequotas
            - --resources=secrets
            - --resources=services
            - --resources=statefulsets
            - --resources=storageclasses
            - --resources=volumeattachments
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits: {}
            requests: {}
---
# Source: apica-ascent/charts/prometheus/templates/prometheus-operator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-prometheus-operator
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: operator
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: operator
  template:
    metadata:
      labels:
        app.kubernetes.io/name: prometheus
        helm.sh/chart: prometheus-6.5.3
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: operator
    spec:
      serviceAccountName: release-name-prometheus-operator
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: prometheus
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: operator
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
  

      containers:
        - name: prometheus-operator
          image: docker.io/bitnami/prometheus-operator:0.53.1-debian-10-r0
          imagePullPolicy: IfNotPresent
          env:
            - name: PROMETHEUS_CONFIG_RELOADER
              valueFrom:
                configMapKeyRef:
                  name: release-name-prometheus-operator
                  key: prometheus-config-reloader
          args:
            - --kubelet-service=kube-system/release-name-prometheus-kubelet
            - --log-format=logfmt
            - --log-level=info
            - --localhost=127.0.0.1
            - --prometheus-config-reloader=$(PROMETHEUS_CONFIG_RELOADER)
            - --namespaces=default
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /metrics
              port: http
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 25
            timeoutSeconds: 125
            failureThreshold: 6
            successThreshold: 1
          readinessProbe:
            httpGet:
              path: /metrics
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 25
            timeoutSeconds: 125
            failureThreshold: 6
            successThreshold: 1
          # yamllint disable rule:indentation
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          # yamllint enable rule:indentation
---
# Source: apica-ascent/charts/thanos/templates/bucketweb/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-thanos-bucketweb
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: bucketweb
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: bucketweb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: thanos
        helm.sh/chart: thanos-8.2.5
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: bucketweb
      annotations:
        checksum/ojbstore-configuration: 0fd914ad4b0cc0cd0ce6236d28d97653a16114885fa7afc9804b101501901740
    spec:
      
      serviceAccount: release-name-thanos-bucketweb
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: thanos
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: bucketweb
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1


      securityContext:
        fsGroup: 1001
      containers:
        - name: bucketweb
          image: docker.io/bitnami/thanos:0.29.0-scratch-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          args:
            - tools
            - bucket
            - web
            - --http-address=0.0.0.0:8080
            - --log.level=info
            - --log.format=logfmt
            - --objstore.config-file=/conf/objstore.yml
            - --refresh=30m
            - --timeout=5m
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/healthy
              port: http
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/ready
              port: http
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: objstore-config
              mountPath: /conf
      volumes:
        - name: objstore-config
          secret:
            secretName: release-name-thanos-objstore-secret
---
# Source: apica-ascent/charts/thanos/templates/compactor/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-thanos-compactor
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: compactor
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: compactor
  template:
    metadata:
      labels:
        app.kubernetes.io/name: thanos
        helm.sh/chart: thanos-8.2.5
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: compactor
      annotations:
        checksum/ojbstore-configuration: 0fd914ad4b0cc0cd0ce6236d28d97653a16114885fa7afc9804b101501901740
    spec:
      
      serviceAccount: release-name-thanos-compactor
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: thanos
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: compactor
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1


      securityContext:
        fsGroup: 1001
      containers:
        - name: compactor
          image: docker.io/bitnami/thanos:0.29.0-scratch-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          args:
            - compact
            - --log.level=info
            - --log.format=logfmt
            - --http-address=0.0.0.0:10902
            - --data-dir=/data
            - --retention.resolution-raw=30d
            - --retention.resolution-5m=30d
            - --retention.resolution-1h=10y
            - --consistency-delay=30m
            - --objstore.config-file=/conf/objstore.yml
            - --wait
          ports:
            - name: http
              containerPort: 10902
              protocol: TCP
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/healthy
              port: http
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/ready
              port: http
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: objstore-config
              mountPath: /conf
            - name: data
              mountPath: /data
      volumes:
        - name: objstore-config
          secret:
            secretName: release-name-thanos-objstore-secret
        - name: data
          persistentVolumeClaim:
            claimName: release-name-thanos-compactor
---
# Source: apica-ascent/charts/thanos/templates/query/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-thanos-query
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: query
  template:
    metadata:
      labels:
        app.kubernetes.io/name: thanos
        helm.sh/chart: thanos-8.2.5
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: query
    spec:
      
      serviceAccount: release-name-thanos-query
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: thanos
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: query
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1


      securityContext:
        fsGroup: 1001
      containers:
        - name: query
          image: docker.io/bitnami/thanos:0.29.0-scratch-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          args:
            - query
            - --log.level=info
            - --log.format=logfmt
            - --grpc-address=0.0.0.0:10901
            - --http-address=0.0.0.0:10902
            - --query.replica-label=prometheus_replica
            - --store=dnssrv+_grpc._tcp.default-prometheus-prometheus-thanos.default.svc.cluster.local
            - --store=dnssrv+_grpc._tcp.release-name-thanos-storegateway.default.svc.cluster.local
            - --store=dnssrv+_grpc._tcp.release-name-thanos-ruler.default.svc.cluster.local
            - --store=dnssrv+_grpc._tcp.release-name-thanos-receive.default.svc.cluster.local
          ports:
            - name: http
              containerPort: 10902
              protocol: TCP
            - name: grpc
              containerPort: 10901
              protocol: TCP
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/healthy
              port: http
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/ready
              port: http
          resources:
            limits: {}
            requests: {}
          volumeMounts:
      volumes:
---
# Source: apica-ascent/charts/flash-coffee/templates/001-coffee-server-sts.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: coffee-server-deployment
  name: coffee-server
  namespace: default
spec:
  replicas: 1
  serviceName: coffee
  selector:
    matchLabels:
      app: coffee-server
  template:
    metadata:
      labels:
        app: coffee-server
    spec:
      serviceAccountName: flash-coffee

   
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsGroup: 1000
      
      initContainers:
      - name: check-redis
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        command:
        - sh
        - -c
        - until redis-cli -h redis-master ping; do echo "waiting for redis-master"; sleep 1;
          done;
      - name: check-postgres
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_password
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_host
          - name: DATABASE_URL
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: database_url
        command: ['sh', '-c',
          'until pg_isready -h $(POSTGRES_HOST) -p 5432 -d $(DATABASE_URL);
          do echo waiting for database; sleep 1; done;']
      volumes:
        
        - name: sharedcert
          secret:
            secretName: logiq-shared-secret
        
      containers:
        - env:
          - name: LOGIQ_UI_GOOGLE_ANALYTICS_CODE
            value: 
          - name: CONFIG_HASH
            value: "7b67a19281a5c06880cf509c4c4ba44c0e242a71897bae579f0fbc97cf8f69d9"
          - name: PYTHONUNBUFFERED
            value: "0"
          - name: COOKIE_SECRET
            value: d84c0edd-ab5e-4664-b0ee-2cd15a9ae5f0 
          - name: LOG_LEVEL
            value: ERROR
          
          - name: MY_APP_NAME
            value: coffee-server
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP                
            
          - name: MY_CLUSTER_ID
            value: LOGIQ
          
          - name: LOGIQ_FLASH_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: logiq_flash_host
          - name: LOGIQ_FLASH_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: logiq_flash_port
          - name: REDIS_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: redis_host
          - name: REDIS_URL
            value: redis://redis-master:6379/0
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_user
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_password
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_host
          - name: POSTGRES_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_port
          - name: POSTGRES_DB
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_db
          - name: POSTGRES_COFFEE_DB
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_coffee_db
          - name: DATABASE_URL
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: coffee_database_url
          - name: S3_URL
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: s3_url
          - name: S3_ACCESS
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_access
          - name: S3_SECRET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_secret
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_bucket
          
          - name: ADMIN_NAME
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: admin_name
          
          
          - name: ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: admin_password
          
          
          - name: ADMIN_ORG
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: admin_org
          
          
          - name: ADMIN_EMAIL
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: admin_email
          
          
          - name: MAIL_SERVER
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: mail_server
          
          
          - name: MAIL_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: mail_port
          
          
          - name: MAIL_USERNAME
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: mail_username
          
          
          - name: MAIL_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: mail_password
          
          
          - name: MAIL_DEFAULT_SENDER
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: mail_default_sender
          
          - name: WEB_WORKERS
            value: "4"
          - name: CONFIGURE_LOGIQ_DS
            value: "true"
          - name:  COFFEE_UI_SERVER_URL
            value: http://coffee:80
          - name: CONFIGURE_LOGIQEVENTS_DS
            value: "false"
            
          - name: PROM_ALERT_MGR_URL
            value: release-name-prometheus-alertmanager:9093
               
          - name: EVENT_REPORTING_WEBHOOKS
            value: ""
          - name: LOGIQ_AUDIT_EVENT_GROUP
            value: logiq-audit
          volumeMounts:
          
          - mountPath: /app/sharedcerts
            name: sharedcert
          
          image: "docker.io/logiqai/flash-brew-coffee:brew.v3.7.1"
          imagePullPolicy: IfNotPresent
          name: coffee
          command: ["/app/flash-brew/setup_coffee.sh"]
          ports:
            - containerPort: 5000
          readinessProbe:
            httpGet:
              port:  5000
              path: /static/js/jquery.min.js
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 125
            successThreshold: 1
            failureThreshold: 5
          resources:
            requests:
              cpu: 1000m
              memory: 4Gi
      restartPolicy: Always
---
# Source: apica-ascent/charts/flash-coffee/templates/003-coffee-worker-sts.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: coffee-worker-deployment
  name: coffee-worker
  namespace: default
spec:
  replicas: 1
  serviceName: coffee
  selector:
    matchLabels:
      app: coffee-worker
  template:
    metadata:
      labels:
        app: coffee-worker
    spec:
      serviceAccountName: flash-coffee

   
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsGroup: 1000
      
      initContainers:
      - name: check-redis
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        command:
        - sh
        - -c
        - until redis-cli -h redis-master ping; do echo "waiting for redis-master"; sleep 1;
          done;
      - name: check-postgres
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_password
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_host
          - name: DATABASE_URL
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: database_url
        command: ['sh', '-c',
          'until pg_isready -h $(POSTGRES_HOST) -p 5432 -d $(DATABASE_URL);
          do echo waiting for database; sleep 1; done;']
      volumes:
        
        - name: sharedcert
          secret:
            secretName: logiq-shared-secret
        
      containers:
        - args:
          - scheduler
          env:
          - name: CONFIG_HASH
            value: "7b67a19281a5c06880cf509c4c4ba44c0e242a71897bae579f0fbc97cf8f69d9"
          - name: PYTHONUNBUFFERED
            value: "0"
          - name: COOKIE_SECRET
            value: d84c0edd-ab5e-4664-b0ee-2cd15a9ae5f0 
          - name: QUEUES
            value: queries,scheduled_queries,celery
          - name: LOG_LEVEL
            value: ERROR
          
          - name: MY_APP_NAME
            value: coffee-worker
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP                
            
          - name: MY_CLUSTER_ID
            value: LOGIQ
           
          - name: LOGIQ_FLASH_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: logiq_flash_host
          - name: LOGIQ_FLASH_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: logiq_flash_port
          - name: WORKERS_COUNT
            value: "1"
          - name: REDIS_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: redis_host
          - name: REDIS_URL
            value: redis://redis-master:6379/0
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_user
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_password
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_host
          - name: POSTGRES_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_port
          - name: POSTGRES_DB
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_db
          - name: POSTGRES_COFFEE_DB
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: postgres_coffee_db
          - name: DATABASE_URL
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: coffee_database_url
          - name: S3_URL
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: s3_url
          - name: S3_ACCESS
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_access
          - name: S3_SECRET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_secret
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_bucket
          
          - name: ADMIN_NAME
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: admin_name
          
          
          - name: ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: admin_password
          
          
          - name: ADMIN_ORG
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: admin_org
          
          
          - name: ADMIN_EMAIL
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: admin_email
          
          
          - name: MAIL_SERVER
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: mail_server
          
          
          - name: MAIL_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: mail_port
          
          
          - name: MAIL_USERNAME
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: mail_username
          
          
          - name: MAIL_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: mail_password
          
          
          - name: MAIL_DEFAULT_SENDER
            valueFrom:
              secretKeyRef:
                name: release-name-flash-coffee
                key: mail_default_sender
          
          
          - name: PROM_ALERT_MGR_URL
            value: release-name-prometheus-alertmanager:9093
           
          - name: EVENT_REPORTING_WEBHOOKS
            value: ""
          - name: LOGIQ_AUDIT_EVENT_GROUP
            value: logiq-audit
          volumeMounts:
          
          - mountPath: /app/sharedcerts
            name: sharedcert
          
          image: "docker.io/logiqai/flash-brew-coffee:brew.v3.7.1"
          imagePullPolicy: IfNotPresent
          name: coffee-worker
          resources:
            requests:
              cpu: 1000m
              memory: 1Gi
      restartPolicy: Always
---
# Source: apica-ascent/charts/flash-discovery/templates/discoveryStatefulSet.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: flash-discovery
  namespace: default
  labels:
    app: flash-discovery
    chart: flash-discovery-1.0.0
    release: release-name
    heritage: Helm
spec:
  serviceName: flash-discovery
  replicas: 2
  selector:
    matchLabels:
      app: flash-discovery
      release: release-name
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: "2Gi"
      storageClassName: standard
  template:
    metadata:
      labels:
        app: flash-discovery
        release: release-name
    spec:
      serviceAccountName: flash-discovery


      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsGroup: 1000
      initContainers:
      - name: check-postgres
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: flash-discovery
                key: postgres_password
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: flash-discovery
                key: postgres_host
          - name: DATABASE_URL
            valueFrom:
              secretKeyRef:
                name: flash-discovery
                key: database_url
        command: ['sh', '-c', 
          'until pg_isready -h $(POSTGRES_HOST) -p 5432 -d $(DATABASE_URL);
          do echo waiting for database; sleep 1; done;']
      
      volumes:
        
        
        
        - name: license
          secret:
            secretName: flash-discovery-license
        
        
        - name: sharedcert
          secret:
            secretName: logiq-shared-secret
        
      containers:
        - name: flash-discovery
          image: "docker.io/logiqai/flash-discovery:v2.0.3"
          imagePullPolicy: IfNotPresent
          env:
          - name: CONFIG_HASH
            value: "d02a0870ae064707207c138f940869a81f522b28810da7510ce4aabe98447f63"
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: flash-discovery
                key: postgres_user
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: flash-discovery
                key: postgres_password
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: flash-discovery
                key: postgres_host
          - name: POSTGRES_PORT
            valueFrom:
              secretKeyRef:
                name: flash-discovery
                key: postgres_port
          - name: POSTGRES_DB
            valueFrom:
              secretKeyRef:
                name: flash-discovery
                key: postgres_db
          - name: MY_APP_NAME
            value: flash-discovery
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP                
            
          - name: MY_CLUSTER_ID
            value: LOGIQ
                
          volumeMounts:
          - mountPath: /flash/db
            name: data
          
          
          - mountPath: /flash/license
            name: license
          
          
          - mountPath: /flash/sharedcerts
            name: sharedcert
          
          ports:
            - name: healthcheck
              containerPort: 8081
              protocol: TCP
            - name: discovery
              containerPort: 4000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /live
              port: healthcheck
            initialDelaySeconds: 10
            failureThreshold: 3
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: healthcheck
            initialDelaySeconds: 5
            failureThreshold: 3
            periodSeconds: 10             
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
---
# Source: apica-ascent/charts/logiq-flash/templates/statefulSet.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logiq-flash
  namespace: default
  labels:
    app: logiq-flash
    chart: logiq-flash-1.0.0
    release: release-name
    heritage: Helm
spec:
  serviceName: logiq-flash-headless
  replicas: 2
  selector:
    matchLabels:
      app: logiq-flash
      release: release-name
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: "25Gi"
      storageClassName: standard
  template:
    metadata:
      annotations:
        checksum/config: 46033bef16827062e1ccca627639030fa11f3e3cc1dec2e8edcbe71f32a5dda5
      labels:
        app: logiq-flash
        release: release-name
    spec:
      serviceAccountName: logiq-flash

      
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsGroup: 1000
      initContainers:
      - command:
        - sh
        - -c
        - until $(curl --output /dev/null --silent --fail http://flash-discovery:8081/ready); do echo "waiting for discovery"; sleep 1;
          done;
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        name: check-discovery
      - name: check-redis
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        command:
        - sh
        - -c
        - until redis-cli -h redis-master ping; do echo "waiting for redis-master"; sleep 1;
          done;
                
      - command:
          - sh
          - -c
          - until $(curl --output /dev/null --silent --fail http://s3-gateway:9000/minio/health/ready); do echo "waiting for s3-gateway"; sleep 1;
            done;
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        name: check-s3-gateway
            
      volumes:
        
        - name: config
          configMap:
            
            name: logiq-config
            
        
        
        
        - name: sharedcert
          secret:
            secretName: logiq-shared-secret
        
      
      terminationGracePeriodSeconds: 60
      containers:
        - name: logiq-flash
          image: "docker.io/logiqai/flash:v3.6.4"
          imagePullPolicy: IfNotPresent
          args:
          - -c
          - /flash/bin/flash -config /flash/config/config.yaml -install_root /flash -runtime_folder /flash/db -discovery_domain $(DISCOVERY_DOMAIN) -discovery_url $(DISCOVERY_URL) -discovery_id flash1 -pg_max_idle_conns 190 -pg_max_open_Conns 195 -postgres_timeout 20s -workers 150 -connection_channel_size 10 --pg_conn_max_lifetime 5m 
          command:
          - /bin/bash
          env:
          - name: CONFIG_HASH
            value: "dd289722ef34988ce24097cd2d8ef1f147a40728fdbd62f6a97c86256d97ab67"
          - name: DISCOVERY_URL
            value: http://flash-discovery:4000
          - name: DISCOVERY_ID
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: DISCOVERY_DOMAIN
            value: logiq-flash-headless
          
          - name: REDIS_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: redis_host
          - name: REDIS_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: redis_port
          
          
          - name: ADMIN_EMAIL
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: admin_email
          
          
          - name: ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: admin_password
          
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_user
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_password
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_host
          - name: POSTGRES_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_port
          - name: POSTGRES_DB
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_db
          - name: S3_ACCESS
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_access
          - name: S3_SECRET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_secret
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_bucket
            
          - name: MY_CLUSTER_ID
            value: LOGIQ
          
          - name: MY_APP_NAME
            value: logiq-flash
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          volumeMounts:
          - mountPath: /flash/config
            name: config
          - mountPath: /flash/db
            name: data
          
          
          - mountPath: /flash/sharedcerts
            name: sharedcert
          
          ports:
            - name: grpc
              containerPort: 50054
              protocol: TCP
            - name: webcli
              containerPort: 9998
              protocol: TCP
            - name: api
              containerPort: 9999
              protocol: TCP
            - name: syslog
              containerPort: 1514
              protocol: TCP
            - name: syslogtls
              containerPort: 7514
              protocol: TCP
            - name: cefport
              containerPort: 1515
              protocol: TCP
            - name: cefportls
              containerPort: 7515
              protocol: TCP
            - name: relp
              containerPort: 20514
              protocol: TCP
            - name: relptls
              containerPort: 2514
              protocol: TCP
            - name: fluentforward
              containerPort: 24224
              protocol: TCP
            - name: fluentfwdtls
              containerPort: 24225
              protocol: TCP
            - name: filebeat
              containerPort: 25224
              protocol: TCP
            - name: filebeatls
              containerPort: 25225
              protocol: TCP
            - name: healthcheck
              containerPort: 8080
              protocol: TCP
            - name: rawtcp
              containerPort: 517
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 25
            timeoutSeconds: 125
          resources:
            limits:
              cpu: 1250m
            requests:
              cpu: 250m
              memory: 2750Mi
        - name: logiq-flash-query
          image: "docker.io/logiqai/flash:v3.6.4"
          imagePullPolicy: IfNotPresent
          args:
          - -c
          - /flash/bin/flash -config /flash/config/config.yaml -install_root /flash -runtime_folder /flash/db -discovery_domain $(DISCOVERY_DOMAIN) -discovery_url $(DISCOVERY_URL) -discovery_id $(DISCOVERY_ID) -pg_max_idle_conns 10 -pg_max_open_Conns 10 -postgres_timeout 20s --pg_conn_max_lifetime 5m -grpc_port 51054 -health_check_port 18080 --sidecar-node -api_port 19999 
          command:
          - /bin/bash
          env:
          - name: CONFIG_HASH
            value: "dd289722ef34988ce24097cd2d8ef1f147a40728fdbd62f6a97c86256d97ab67"
          - name: DISCOVERY_URL
            value: http://flash-discovery:4000
          - name: DISCOVERY_ID
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: DISCOVERY_DOMAIN
            value: logiq-flash-headless
          
          - name: REDIS_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: redis_host
          - name: REDIS_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: redis_port
          
          
          - name: ADMIN_EMAIL
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: admin_email
          
          
          - name: ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: admin_password
          
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_user
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_password
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_host
          - name: POSTGRES_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_port
          - name: POSTGRES_DB
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_db
          - name: S3_ACCESS
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_access
          - name: S3_SECRET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_secret
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_bucket
            
          - name: MY_CLUSTER_ID
            value: LOGIQ
          
          - name: MY_APP_NAME
            value: logiq-flash
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          ports:
          - containerPort: 51054
            name: grpc
            protocol: TCP
          - containerPort: 8899
            name: webcli
            protocol: TCP
          - containerPort: 19999
            name: api
            protocol: TCP
          - containerPort: 18080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 25
            httpGet:
              path: /ready
              port: 18080
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 125
          volumeMounts:
          - mountPath: /flash/config
            name: config
          - mountPath: /flash/db
            name: data
          
          
          - mountPath: /flash/sharedcerts
            name: sharedcert
          
          resources:
            {}
        - name: logiq-flash-tracing
          image: "docker.io/logiqai/tracing:v1.35.2-lq1-c"
          imagePullPolicy: 
          command:
          - /bin/sh
          - -c
          - |
            while true 
            do
            /go/bin/collector-linux --grpc-storage.server=localhost:50054
            sleep 5
            done
          env:
          - name: SPAN_STORAGE_TYPE
            value: grpc-plugin
          readinessProbe:
            tcpSocket:
              port: 14250
            initialDelaySeconds: 25
            periodSeconds: 25
            successThreshold: 1
            failureThreshold: 3
            timeoutSeconds: 125
          resources:
            {}
          ports:
          - containerPort: 14250
            name: tracing-2
            protocol: TCP
          - containerPort: 14268
            name: tracing-1
            protocol: TCP
---
# Source: apica-ascent/charts/logiq-flash/templates/statefulset-ml.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logiq-flash-ml
  namespace: default
  labels:
    app: logiq-flash-ml
    chart: logiq-flash-1.0.0
    release: release-name
    heritage: Helm
spec:
  serviceName: logiq-flash-ml
  replicas: 2
  selector:
    matchLabels:
      app: logiq-flash-ml
      release: release-name
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: "25Gi"
      storageClassName: standard
  template:
    metadata:
      annotations:
        checksum/config: 46033bef16827062e1ccca627639030fa11f3e3cc1dec2e8edcbe71f32a5dda5
      labels:
        app: logiq-flash-ml
        release: release-name
    spec:
      serviceAccountName: logiq-flash
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsGroup: 1000
      initContainers:
      - name: check-redis
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        command:
        - sh
        - -c
        - until redis-cli -h redis-master ping; do echo "waiting for redis-master"; sleep 1;
          done;
          
      - command:
          - sh
          - -c
          - until $(curl --output /dev/null --silent --fail http://s3-gateway:9000/minio/health/ready); do echo "waiting for s3-gateway"; sleep 1;
            done;
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        name: check-s3-gateway
        
      - command:
        - sh
        - -c
        - until $(curl --output /dev/null --silent --fail http://logiq-flash:8080/ready); do echo "waiting for flash"; sleep 1;
          done;
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        name: check-flash
      volumes:
        
        - name: config
          configMap:
            
            name: logiq-config
            
        
        
        
        - name: sharedcert
          secret:
            secretName: logiq-shared-secret
        
      
      containers:
        - name: logiq-flash-ml
          image: "docker.io/logiqai/flash:v3.6.4"
          imagePullPolicy: IfNotPresent
          args:
          - -c
          - /flash/bin/flash -config /flash/config/config.yaml -install_root /flash -runtime_folder /flash/db -is-kube=true -discovery_domain=$(DISCOVERY_DOMAIN) -pg_max_idle_conns 50 -pg_max_open_Conns 50 -metadata-db-ttl 11m -postgres_timeout 20s -api-node  -redis_max_connections 3000 --pg_conn_max_lifetime 5m -ml-use-sidecar 
          command:
          - /bin/bash
          env:
          - name: host
            value: 
          - name: CONFIG_HASH
            value: "dd289722ef34988ce24097cd2d8ef1f147a40728fdbd62f6a97c86256d97ab67"
          - name: DISCOVERY_DOMAIN
            value: logiq-flash-ml
          
          - name: REDIS_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: redis_host
          - name: REDIS_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: redis_port
          
          
          - name: ADMIN_EMAIL
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: admin_email
          
          
          - name: ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: admin_password
          
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_user
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_password
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_host
          - name: POSTGRES_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_port
          - name: POSTGRES_DB
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_db
          - name: S3_ACCESS
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_access
          - name: S3_SECRET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_secret
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_bucket
            
          - name: MY_CLUSTER_ID
            value: LOGIQ
          
          - name: MY_APP_NAME
            value: logiq-flash-ml
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          volumeMounts:
          - mountPath: /flash/config
            name: config
          - mountPath: /flash/db
            name: data
          
          
          - mountPath: /flash/sharedcerts
            name: sharedcert
          
          ports:
            - name: grpc
              containerPort: 50054
              protocol: TCP
            - name: webcli
              containerPort: 9998
              protocol: TCP
            - name: api
              containerPort: 9999
              protocol: TCP
            - name: healthcheck
              containerPort: 8080
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 25
            successThreshold: 1
            failureThreshold: 3
            timeoutSeconds: 125
          resources:
            {}
        - name: logiq-flash-tracing
          image: "docker.io/logiqai/tracing:v1.35.2-lq1-q"
          imagePullPolicy: 
          command:
          - /bin/sh
          - -c
          - |
            while true 
            do   
            /go/bin/query-linux --grpc-storage.server=localhost:50054 --query.bearer-token-propagation=true --query.base-path=/dtracing
            sleep 5
            done
          env:          
          - name: SPAN_STORAGE_TYPE
            value: grpc-plugin
          ports:
          - containerPort: 16686
            name: tracing
            protocol: TCP
          readinessProbe:
            httpGet:
              path: /dtracing
              port: 16686
            initialDelaySeconds: 25
            periodSeconds: 25
            successThreshold: 1
            failureThreshold: 3
            timeoutSeconds: 125
          resources:
            {}
---
# Source: apica-ascent/charts/logiq-flash/templates/statefulset-sync.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: logiq-flash-sync
  namespace: default
  labels:
    app: logiq-flash-sync
    chart: logiq-flash-1.0.0
    release: release-name
    heritage: Helm
spec:
  serviceName: logiq-flash-sync
  replicas: 2
  selector:
    matchLabels:
      app: logiq-flash-sync
      release: release-name
  template:
    metadata:
      annotations:
        checksum/config: 46033bef16827062e1ccca627639030fa11f3e3cc1dec2e8edcbe71f32a5dda5
      labels:
        app: logiq-flash-sync
        release: release-name
    spec:
      serviceAccountName: logiq-flash
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsGroup: 1000
      initContainers:
      - command:
        - sh
        - -c
        - until $(curl --output /dev/null --silent --fail http://logiq-flash:8080/ready); do echo "waiting for flash"; sleep 1;
          done;
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        name: check-flash
      - name: check-redis
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        command:
        - sh
        - -c
        - until redis-cli -h redis-master ping; do echo "waiting for redis-master"; sleep 1;
          done;
                 
      - command:
          - sh
          - -c
          - until $(curl --output /dev/null --silent --fail http://s3-gateway:9000/minio/health/ready); do echo "waiting for s3-gateway"; sleep 1;
            done;
        image: docker.io/logiqai/toolbox:2.0.1
        imagePullPolicy: IfNotPresent
        name: check-s3-gateway
            
      volumes:
        
        - name: config
          configMap:
            
            name: logiq-config
            
        
        
        
        - name: sharedcert
          secret:
            secretName: logiq-shared-secret
        
        - name: data
          emptyDir: {}
      
      containers:
        - name: logiq-flash-sync
          image: "docker.io/logiqai/flash:v3.6.4"
          imagePullPolicy: IfNotPresent
          args:
          - -c
          - /flash/bin/flash -config /flash/config/config.yaml -install_root /flash -runtime_folder /flash/db -is-kube=true -discovery_domain=$(DISCOVERY_DOMAIN) -pg_max_idle_conns 100 -pg_max_open_Conns 100 -postgres_timeout 40s -metadata-db-ttl 11m -metadata-text-upload-jobs 50 -sync-node
          command:
          - /bin/bash
          env:
          - name: CONFIG_HASH
            value: "dd289722ef34988ce24097cd2d8ef1f147a40728fdbd62f6a97c86256d97ab67"
          - name: DISCOVERY_DOMAIN
            value: logiq-flash-sync
          
          - name: REDIS_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: redis_host
          - name: REDIS_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: redis_port
          
          
          - name: ADMIN_EMAIL
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: admin_email
          
          
          - name: ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: admin_password
          
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_user
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_password
          - name: POSTGRES_HOST
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_host
          - name: POSTGRES_PORT
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_port
          - name: POSTGRES_DB
            valueFrom:
              secretKeyRef:
                name: release-name-logiq-flash
                key: postgres_db
          - name: S3_ACCESS
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_access
          - name: S3_SECRET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_secret
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: logiq-shared-secret
                key: s3_bucket
            
          - name: MY_CLUSTER_ID
            value: LOGIQ
          
          - name: MY_APP_NAME
            value: logiq-flash-sync
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          volumeMounts:
          - mountPath: /flash/config
            name: config
          - mountPath: /flash/db
            name: data
          
          
          - mountPath: /flash/sharedcerts
            name: sharedcert
          
          ports:
            - name: grpc
              containerPort: 50054
              protocol: TCP
            - name: webcli
              containerPort: 9998
              protocol: TCP
            - name: api
              containerPort: 9999
              protocol: TCP
            - name: healthcheck
              containerPort: 8080
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 25
            successThreshold: 1
            failureThreshold: 3
            timeoutSeconds: 125
          resources:
            limits:
              cpu: 250m
            requests:
              cpu: 50m
---
# Source: apica-ascent/charts/postgres/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: default
  labels:
    app: postgres
    chart: postgres-8.7.3
    release: "release-name"
    heritage: "Helm"
spec:
  serviceName: postgres-headless
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: postgres
      release: "release-name"
      role: master
  template:
    metadata:
      name: postgres
      labels:
        app: postgres
        chart: postgres-8.7.3
        release: "release-name"
        heritage: "Helm"
        role: master
    spec:      


      securityContext:
        fsGroup: 1001
      serviceAccountName: postgres
      containers:
        - name: postgres
          image: docker.io/bitnami/postgresql:12.3.0-debian-10-r18
          imagePullPolicy: "IfNotPresent"
          resources:
            requests:
              cpu: 1000m
              memory: 4Gi
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "postgres"
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -d "postgres" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "postgres" -d "postgres" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: postgresql-extended-config
              mountPath: /bitnami/postgresql/conf/conf.d/
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
              subPath: 
        - name: metrics
          image: docker.io/bitnami/postgres-exporter:0.8.0-debian-10-r72
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: DATA_SOURCE_URI
              value: "127.0.0.1:5432/postgres?sslmode=disable"
            - name: DATA_SOURCE_PASS
              valueFrom:
                secretKeyRef:
                  name: postgres
                  key: postgresql-password
            - name: DATA_SOURCE_USER
              value: postgres
          livenessProbe:
            httpGet:
              path: /
              port: http-metrics
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /
              port: http-metrics
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
          ports:
            - name: http-metrics
              containerPort: 9187
      volumes:
        - name: postgresql-extended-config
          configMap:
            name: postgres-extended-configuration
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "10Gi"
        storageClassName: standard
---
# Source: apica-ascent/charts/redis/templates/redis-master-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-master
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    release: release-name
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: redis
      release: release-name
      role: master
  serviceName: redis-headless
  template:
    metadata:
      labels:
        app: redis
        chart: redis-10.6.5
        release: release-name
        role: master
      annotations:
        checksum/health: cf000c970446804143bfb5d53d724e6003986b40006d6770c65561e7aed41ea0
        checksum/configmap: 9c6c21bd918331fed858c9f838c7859b530bf8855434588380e14d5a3a25c412
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        prometheus.io/port: "9121"
        prometheus.io/scrape: "true"
    spec:      
      securityContext:
        fsGroup: 1001
      serviceAccountName: "redis"
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis
            topologyKey: kubernetes.io/hostname


      containers:
      - name: redis
        image: "docker.io/bitnami/redis:6.0.4-debian-10-r6"
        imagePullPolicy: "IfNotPresent"
        securityContext:
          runAsUser: 1001
        command:
        - /bin/bash
        - -c
        - |
          if [[ -n $REDIS_PASSWORD_FILE ]]; then
            password_aux=`cat ${REDIS_PASSWORD_FILE}`
            export REDIS_PASSWORD=$password_aux
          fi
          if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
          fi
          if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
          fi
          ARGS=("--port" "${REDIS_PORT}")
          ARGS+=("--protected-mode" "no")
          ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
          ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
          /run.sh ${ARGS[@]}
        env:
        - name: REDIS_REPLICATION_MODE
          value: master
        - name: ALLOW_EMPTY_PASSWORD
          value: "yes"
        - name: REDIS_PORT
          value: "6379"
        ports:
        - name: redis
          containerPort: 6379
        livenessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_liveness_local.sh 5
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 1
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_readiness_local.sh 5
        resources:
          null
        volumeMounts:
        - name: health
          mountPath: /health
        - name: redis-data
          mountPath: /data
          subPath: 
        - name: config
          mountPath: /opt/bitnami/redis/mounted-etc
        - name: redis-tmp-conf
          mountPath: /opt/bitnami/redis/etc/
      - name: metrics
        image: docker.io/bitnami/redis-exporter:1.5.2-debian-10-r24
        imagePullPolicy: "IfNotPresent"
        command:
        - /bin/bash
        - -c
        - |
          if [[ -f '/secrets/redis-password' ]]; then
           export REDIS_PASSWORD=$(cat /secrets/redis-password)
          fi
          redis_exporter
        env:
        - name: REDIS_ALIAS
          value: redis
        volumeMounts:
        ports:
        - name: metrics
          containerPort: 9121
        resources:
          null
      volumes:
      - name: health
        configMap:
          name: redis-health
          defaultMode: 0755
      - name: config
        configMap:
          name: redis
      - name: "redis-data"
        emptyDir: {}
      - name: redis-tmp-conf
        emptyDir: {}
  updateStrategy:
    type: RollingUpdate
---
# Source: apica-ascent/charts/redis/templates/redis-slave-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-slave
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    release: release-name
    heritage: Helm
spec:
  replicas: 0
  serviceName: redis-headless
  selector:
    matchLabels:
      app: redis
      release: release-name
      role: slave
  template:
    metadata:
      labels:
        app: redis
        release: release-name
        chart: redis-10.6.5
        role: slave
      annotations:
        checksum/health: cf000c970446804143bfb5d53d724e6003986b40006d6770c65561e7aed41ea0
        checksum/configmap: 9c6c21bd918331fed858c9f838c7859b530bf8855434588380e14d5a3a25c412
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
        prometheus.io/port: "9121"
        prometheus.io/scrape: "true"
    spec:      
      securityContext:
        fsGroup: 1001
      serviceAccountName: "redis"


      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis
            topologyKey: kubernetes.io/hostname
      containers:
      - name: redis
        image: docker.io/bitnami/redis:6.0.4-debian-10-r6
        imagePullPolicy: "IfNotPresent"
        securityContext:
          runAsUser: 1001
        command:
        - /bin/bash
        - -c
        - |
          if [[ -n $REDIS_PASSWORD_FILE ]]; then
            password_aux=`cat ${REDIS_PASSWORD_FILE}`
            export REDIS_PASSWORD=$password_aux
          fi
          if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then
            password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`
            export REDIS_MASTER_PASSWORD=$password_aux
          fi
          if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
          fi
          if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
            cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
          fi
          ARGS=("--port" "${REDIS_PORT}")
          ARGS+=("--slaveof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
          ARGS+=("--protected-mode" "no")
          ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
          ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
          /run.sh "${ARGS[@]}"
        env:
        - name: REDIS_REPLICATION_MODE
          value: slave
        - name: REDIS_MASTER_HOST
          value: redis-master-0.redis-headless.default.svc.cluster.local
        - name: REDIS_PORT
          value: "6379"
        - name: REDIS_MASTER_PORT_NUMBER
          value: "6379"
        - name: ALLOW_EMPTY_PASSWORD
          value: "yes"
        ports:
        - name: redis
          containerPort: 6379
        livenessProbe:
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_liveness_local_and_master.sh 5
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 10
          successThreshold: 1
          failureThreshold: 5
          exec:
            command:
            - sh
            - -c
            - /health/ping_readiness_local_and_master.sh 5
        resources:
          null
        volumeMounts:
        - name: health
          mountPath: /health
        - name: redis-data
          mountPath: /data
        - name: config
          mountPath: /opt/bitnami/redis/mounted-etc
        - name: redis-tmp-conf
          mountPath: /opt/bitnami/redis/etc
      - name: metrics
        image: docker.io/bitnami/redis-exporter:1.5.2-debian-10-r24
        imagePullPolicy: "IfNotPresent"
        command:
        - /bin/bash
        - -c
        - |
          if [[ -f '/secrets/redis-password' ]]; then
           export REDIS_PASSWORD=$(cat /secrets/redis-password)
          fi
          redis_exporter
        env:
        - name: REDIS_ALIAS
          value: redis
        volumeMounts:
        ports:
        - name: metrics
          containerPort: 9121
        resources:
          null
      volumes:
      - name: health
        configMap:
          name: redis-health
          defaultMode: 0755
      - name: config
        configMap:
          name: redis
      - name: sentinel-tmp-conf
        emptyDir: {}
      - name: redis-tmp-conf
        emptyDir: {}
      - name: redis-data
        emptyDir: {}
  updateStrategy:
    type: RollingUpdate
---
# Source: apica-ascent/charts/s3gateway/templates/statefulset.yaml
apiVersion: apps/v1beta2
kind: StatefulSet
metadata:
  name: s3-gateway
  labels:
    app: s3gateway
    chart: s3gateway-5.0.20
    release: release-name
    heritage: Helm
spec:
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: "Parallel"
  serviceName: s3-gateway-svc
  replicas: 4
  selector:
    matchLabels:
      app: s3gateway
      release: release-name
  template:
    metadata:
      name: s3-gateway
      labels:
        app: s3gateway
        release: release-name
      annotations:
        checksum/secrets: b80164f7be2ed65a28c3e468e4849b1d31cdc3a26796e6d2a1a7d7b6e8ee986a
        checksum/config: 7d918fc5d45a2f768e3ab58e63f3147b801414191565b6350b2045789d6b2823
    spec:
      serviceAccountName: "s3-gateway"
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      containers:
        - name: s3gateway
          image: minio/minio:RELEASE.2020-09-17T04-49-20Z
          imagePullPolicy: IfNotPresent
          command: [ "/bin/sh",
            "-ce",
            "/usr/bin/docker-entrypoint.sh minio -S /etc/minio/certs/ server  http://s3-gateway-{0...3}.s3-gateway-svc.default.svc.cluster.local/export" ]
          volumeMounts:
            - name: export
              mountPath: /export
            
          ports:
            - name: http
              containerPort: 9000
          env:
            - name: MINIO_PROMETHEUS_AUTH_TYPE 
              value: "public"
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-gateway
                  key: accesskey
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-gateway
                  key: secretkey
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /minio/health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 15
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          resources:
            {}


      volumes:
        - name: minio-user
          secret:
            secretName: s3-gateway
  volumeClaimTemplates:
    - metadata:
        name: export
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: standard
        resources:
          requests:
            storage: 25Gi
---
# Source: apica-ascent/charts/thanos/templates/receive/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-thanos-receive
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: receive
spec:
  replicas: 1
  podManagementPolicy: OrderedReady
  serviceName: release-name-thanos-receive-headless
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: receive
  template:
    metadata:
      labels:
        app.kubernetes.io/name: thanos
        helm.sh/chart: thanos-8.2.5
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: receive
      annotations:
        checksum/ojbstore-configuration: 0fd914ad4b0cc0cd0ce6236d28d97653a16114885fa7afc9804b101501901740
        checksum/receive-configuration: 265c48c4c46498aa340f494d8e21b7a5dc2c48ffb337135bb76c6fca2c3ab1a6
    spec:
      
      serviceAccount: release-name-thanos-receive
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: thanos
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: receive
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1


      securityContext:
        fsGroup: 1001
      containers:
        - name: receive
          image: docker.io/bitnami/thanos:0.29.0-scratch-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          args:
            - receive
            - --log.level=debug
            - --log.format=logfmt
            - --grpc-address=0.0.0.0:10901
            - --http-address=0.0.0.0:10902
            - --remote-write.address=0.0.0.0:19291
            - --receive.replication-factor=1
            - --objstore.config=$(OBJSTORE_CONFIG)
            - --tsdb.path=/var/thanos/receive
            - --label=prometheus_replica="$(NAME)"
            - --label=receive="true"
            - --tsdb.retention=30d
            - --receive.local-endpoint=127.0.0.1:10901
            - --receive.hashrings-file=/var/lib/thanos-receive/hashrings.json
          env:
            - name: NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OBJSTORE_CONFIG
              valueFrom:
                secretKeyRef:
                  key: objstore.yml
                  name: release-name-thanos-objstore-secret
          ports:
            - containerPort: 10901
              name: grpc
              protocol: TCP
            - containerPort: 10902
              name: http
              protocol: TCP
            - containerPort: 19291
              name: remote-write
              protocol: TCP
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/healthy
              port: http
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/ready
              port: http
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: hashring-config
              mountPath: /var/lib/thanos-receive
            - name: data
              mountPath: /var/thanos/receive
      volumes:
        - name: hashring-config
          configMap:
            name: release-name-thanos-receive
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: apica-ascent/charts/thanos/templates/ruler/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-thanos-ruler
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ruler
spec:
  replicas: 1
  podManagementPolicy: OrderedReady
  serviceName: release-name-thanos-ruler-headless
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: ruler
  template:
    metadata:
      labels:
        app.kubernetes.io/name: thanos
        helm.sh/chart: thanos-8.2.5
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: ruler
      annotations:
        checksum/ojbstore-configuration: 0fd914ad4b0cc0cd0ce6236d28d97653a16114885fa7afc9804b101501901740
        checksum/ruler-configuration: 168c719f5bc295390a79b4e1cbfb78b3b08624e228c8d51420b361eed81bd026
    spec:
      
      serviceAccount: release-name-thanos-ruler
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: thanos
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: ruler
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1


      securityContext:
        fsGroup: 1001
      containers:
        - name: ruler
          image: docker.io/bitnami/thanos:0.29.0-scratch-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          args:
            - rule
            - --log.level=info
            - --log.format=logfmt
            - --grpc-address=0.0.0.0:10901
            - --http-address=0.0.0.0:10902
            - --data-dir=/data
            - --eval-interval=1m
            - --alertmanagers.url=http://default-prometheus-alertmanager.default.svc.cluster.local:9093
            - --query=dnssrv+_http._tcp.release-name-thanos-query.default.svc.cluster.local
            - --label=prometheus_replica="$(POD_NAME)"
            - --label=ruler_cluster=""
            - --alert.label-drop=prometheus_replica
            - --objstore.config-file=/conf/objstore/objstore.yml
            - --rule-file=/conf/rules/*.yml
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          ports:
            - name: http
              containerPort: 10902
              protocol: TCP
            - name: grpc
              containerPort: 10901
              protocol: TCP
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/healthy
              port: http
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/ready
              port: http
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: ruler-config
              mountPath: /conf/rules
            - name: objstore-config
              mountPath: /conf/objstore
            - name: data
              mountPath: /data
      volumes:
        - name: ruler-config
          configMap:
            name: release-name-thanos-ruler-configmap
        - name: objstore-config
          secret:
            secretName: release-name-thanos-objstore-secret
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: apica-ascent/charts/thanos/templates/storegateway/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-thanos-storegateway
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: storegateway
spec:
  replicas: 1
  podManagementPolicy: OrderedReady
  serviceName: release-name-thanos-storegateway-headless
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: storegateway
  template:
    metadata:
      labels:
        app.kubernetes.io/name: thanos
        helm.sh/chart: thanos-8.2.5
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: storegateway
      annotations:
        checksum/ojbstore-configuration: 0fd914ad4b0cc0cd0ce6236d28d97653a16114885fa7afc9804b101501901740
    spec:
      
      serviceAccount: release-name-thanos-storegateway
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: thanos
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: storegateway
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1


      securityContext:
        fsGroup: 1001
      containers:
        - name: storegateway
          image: docker.io/bitnami/thanos:0.29.0-scratch-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          args:
            - store
            - --log.level=info
            - --log.format=logfmt
            - --grpc-address=0.0.0.0:10901
            - --http-address=0.0.0.0:10902
            - --data-dir=/data
            - --objstore.config-file=/conf/objstore.yml
          ports:
            - name: http
              containerPort: 10902
              protocol: TCP
            - name: grpc
              containerPort: 10901
              protocol: TCP
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/healthy
              port: http
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 120
            httpGet:
              path: /-/ready
              port: http
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: objstore-config
              mountPath: /conf
            - name: data
              mountPath: /data
      volumes:
        - name: objstore-config
          secret:
            secretName: release-name-thanos-objstore-secret
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: apica-ascent/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: logiq-ingress
  namespace: default
  labels:
    helm.sh/chart: apica-ascent-2.0.1
    app.kubernetes.io/name: apica-ascent
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v3.7.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    kubernetes.io/ingress.class: logiq-ingress-default
    ingress.kubernetes.io/proxy-body-size: 100M
    ingress.kubernetes.io/app-root: "/"
    nginx.ingress.kubernetes.io/connection-proxy-header: keep-alive
    nginx.ingress.kubernetes.io/load-balance: round_robin
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
    nginx.ingress.kubernetes.io/session-cookie-expires: "300"
    nginx.ingress.kubernetes.io/session-cookie-max-age: "300"
    nginx.ingress.kubernetes.io/client-body-buffer-size: 1M
    nginx.org/proxy-connect-timeout: 30s
    nginx.org/proxy-read-timeout: 175s
    nginx.ingress.kubernetes.io/configuration-snippet: |
      if ($request_uri ~* \.(js|css|gif|jpe?g|png)) {
        expires 1M;
        add_header Cache-Control "public";
      }
spec:
  rules:
    
    - http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: coffee
              port: 
                 number: 80
        - path: /v1/json_batch
          pathType: Prefix
          backend:
            service:
                name: logiq-flash
                port: 
                   number: 9999
        - path: /v1/json
          pathType: Prefix
          backend:
            service:
               name: logiq-flash
               port: 
                  number: 9999
        - path: /v1/tenant
          pathType: Prefix
          backend:
            service: 
              name: logiq-flash
              port: 
                number: 9999
        - path: /api/traces
          pathType: Prefix
          backend:
            service:
              name: logiq-flash
              port:
                number: 14268
        - path: /v1
          pathType: Prefix
          backend:
            service:
               name: logiq-flash-ml
               port: 
                 number: 9999
        - path: /v2
          pathType: Prefix
          backend:
            service:
               name: logiq-flash-ml
               port: 
                   number: 9999
        - path: /dtracing
          pathType: Prefix
          backend:
            service:
               name: logiq-flash-ml
               port: 
                   number: 16686          
         
        - path: /api/v1/receive
          pathType: Prefix
          backend:
            service:
               name: default-thanos-receive
               port:
                  number: 19291
---
# Source: apica-ascent/charts/prometheus/templates/alertmanager/alertmanager.yaml
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: release-name-prometheus-alertmanager
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: alertmanager
spec:
  replicas: 1
  serviceAccountName: release-name-prometheus-alertmanager
  image: docker.io/bitnami/alertmanager:0.23.0-debian-10-r113
  listenLocal: false
  externalUrl: http://release-name-prometheus-alertmanager.default:9093/
  portName: "web"
  paused: false
  logFormat: logfmt
  logLevel: info
  retention: 120h
  resources:
    {}
  routePrefix: "/"
  securityContext:
    fsGroup: 1001
    runAsUser: 1001
  storage:
    volumeClaimTemplate:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "5Gi"
        
  podMetadata:
    labels:
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: alertmanager
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: prometheus
                app.kubernetes.io/instance: release-name
                app.kubernetes.io/component: alertmanager
            namespaces:
              - "default"
            topologyKey: kubernetes.io/hostname
          weight: 1
  
  containers:
    ## This monkey patching is needed until the securityContexts are
    ## directly patchable via the CRD.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/issues/3947
    ## currently implemented with strategic merge
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/strategic-merge-patch.md
    - name: alertmanager
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      livenessProbe:
        httpGet:
          path: /-/healthy
          port: web
          scheme: HTTP
        initialDelaySeconds: 0
        periodSeconds: 25
        timeoutSeconds: 25
        failureThreshold: 120
        successThreshold: 1
      readinessProbe:
        httpGet:
          path: /-/ready
          port: web
          scheme: HTTP
        initialDelaySeconds: 0
        periodSeconds: 25
        timeoutSeconds: 25
        failureThreshold: 120
        successThreshold: 1
    ## This monkey patching is needed until the securityContexts are
    ## directly patchable via the CRD.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/issues/3947
    ## currently implemented with strategic merge
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/strategic-merge-patch.md
    - name: config-reloader
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      livenessProbe:
        tcpSocket:
          port: reloader-web
        initialDelaySeconds: 10
        periodSeconds: 10
        timeoutSeconds: 125
        failureThreshold: 6
        successThreshold: 1
      readinessProbe:
        tcpSocket:
          port: reloader-web
        initialDelaySeconds: 15
        periodSeconds: 20
        timeoutSeconds: 120
        failureThreshold: 6
        successThreshold: 1
---
# Source: apica-ascent/charts/prometheus/templates/prometheus/prometheus.yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: release-name-prometheus-prometheus
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: prometheus
spec:
  replicas: 2
  serviceAccountName: release-name-prometheus-prometheus
  serviceMonitorSelector: {}
  podMonitorSelector: {}
  probeSelector: {}
  alerting:
    alertmanagers:
      - namespace: default
        name: release-name-prometheus-alertmanager
        port: http
        pathPrefix: "/"
  image: docker.io/bitnami/prometheus:2.32.1-debian-10-r2
  externalUrl: http://release-name-prometheus-prometheus.default:9090/
  paused: false
  logLevel: info
  logFormat: logfmt
  listenLocal: false
  enableAdminAPI: false
  resources:
    limits:
      cpu: 1500m
      memory: 10Gi
    requests:
      cpu: 100m
      memory: 50Mi
  retention: 30d
  portName: "web"
  routePrefix: "/"
  serviceMonitorNamespaceSelector: {}
  podMonitorNamespaceSelector: {}
  probeNamespaceSelector: {}
  securityContext:
    fsGroup: 1001
    runAsUser: 1001
  ruleNamespaceSelector: {}
  ruleSelector: {}
  storage:
    volumeClaimTemplate:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "5Gi"
        
  podMetadata:
    labels:
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: prometheus
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: prometheus
                app.kubernetes.io/instance: release-name
                app.kubernetes.io/component: prometheus
            namespaces:
              - "default"
            topologyKey: kubernetes.io/hostname
          weight: 1

  
  containers:
    - name: thanos-sidecar
      image: docker.io/bitnami/thanos:0.23.1-scratch-r4
      imagePullPolicy: IfNotPresent
      args:
        - sidecar
        - --prometheus.url=http://localhost:9090
        - --grpc-address=0.0.0.0:10901
        - --http-address=0.0.0.0:10902
        - --tsdb.path=/prometheus/
        - --objstore.config=$(OBJSTORE_CONFIG)
      env:
        - name: OBJSTORE_CONFIG
          valueFrom:
            secretKeyRef:
              name: thanos-objectstorage-config
              key: thanos.yaml
      resources:
        limits:
          cpu: 1500m
          memory: 10Gi
        requests:
          cpu: 100m
          memory: 50Mi
      ports:
        - name: grpc
          containerPort: 10901
          protocol: TCP
        - name: http
          containerPort: 10902
          protocol: TCP
      volumeMounts:
        - mountPath: /prometheus
          name: prometheus-release-name-prometheus-prometheus-db
          subPath: prometheus-db
      # yamllint disable rule:indentation
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      # yamllint enable rule:indentation
      livenessProbe:
        httpGet:
          path: /-/healthy
          port: http
          scheme: HTTP
        initialDelaySeconds: 0
        periodSeconds: 25
        timeoutSeconds: 25
        failureThreshold: 120
        successThreshold: 1
      readinessProbe:
        httpGet:
          path: /-/ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 0
        periodSeconds: 25
        timeoutSeconds: 25
        failureThreshold: 120
        successThreshold: 1
    ## This monkey patching is needed until the securityContexts are
    ## directly patchable via the CRD.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/issues/3947
    ## currently implemented with strategic merge
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/strategic-merge-patch.md
    - name: prometheus
      args:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=30d
      - --web.enable-lifecycle
      - --web.external-url=http://prometheus-prometheus.logiq:9090/
      - --web.route-prefix=/
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      - --storage.tsdb.min-block-duration=2h
      - --storage.tsdb.max-block-duration=2h
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      livenessProbe:
        httpGet:
          path: /-/healthy
          port: web
          scheme: HTTP
        initialDelaySeconds: 0
        periodSeconds: 10
        timeoutSeconds: 3
        failureThreshold: 10
        successThreshold: 1
      readinessProbe:
        httpGet:
          path: /-/ready
          port: web
          scheme: HTTP
        initialDelaySeconds: 0
        periodSeconds: 10
        timeoutSeconds: 3
        failureThreshold: 10
        successThreshold: 1
    ## This monkey patching is needed until the securityContexts are
    ## directly patchable via the CRD.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/issues/3947
    ## currently implemented with strategic merge
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/strategic-merge-patch.md
    - name: config-reloader
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      livenessProbe:
        tcpSocket:
          port: reloader-web
        initialDelaySeconds: 10
        periodSeconds: 10
        timeoutSeconds: 125
        failureThreshold: 6
        successThreshold: 1
      readinessProbe:
        tcpSocket:
          port: reloader-web
        initialDelaySeconds: 15
        periodSeconds: 20
        timeoutSeconds: 120
        failureThreshold: 6
        successThreshold: 1
---
# Source: apica-ascent/charts/prometheus/templates/alertmanager/logiq-alert-rules.yaml
# Generated from 'kubernetes-storage' group from https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-rules.yaml
# Do not change in-place! In order to change this file first read following link:
# https://github.com/helm/charts/tree/master/stable/prometheus-operator/hack
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: release-name-prometheus-alertmanager
  labels:
    prometheus: k8s
    role: alert-rules
spec:
  groups:
  - name: logiq-exporter.rules
    rules:
    - expr: |
        (round(sum(rate(logiq_message_count[5m])),1))
      record: logiq:messages_per_second:rate
    - expr: |
        (round(sum(increase(logiq_data_received_bytes[30d]))/1000000000000,0.01))
      record: logiq:data_ingested_monthly:sum
    - expr: |
        (round(sum(increase(logiq_message_count[24h]))/1000000,1))
      record: logiq:messages_ingested_daily:sum
    - expr: |
        (round(avg(increase(logiq_data_received_bytes[5m])*12/1000000000)/1,0.01)*100)
      record: logiq:ingest_capacity_utilization:percent
    - expr: |
        (sum by (exported_namespace,app) (increase(logiq_namespace_app_received_bytes[1h])))
      record: logiq:namespace_app_bytes_hourly:increase
    - expr: |
        (sum by (exported_namespace,app) (increase(logiq_namespace_app_message_count[1h])))
      record: logiq:namespace_app_count_hourly:increase
    - expr: |
        (sum by (exported_namespace,app) (increase(logiq_namespace_app_received_bytes[24h])))
      record: logiq:namespace_app_bytes_daily:increase
    - expr: |
        (sum by (exported_namespace,app) (increase(logiq_namespace_app_message_count[24h])))
      record: logiq:namespace_app_count_daily:increase
    - expr: |
        (sum by (exported_namespace,app) (increase(logiq_namespace_app_received_bytes[7d])))
      record: logiq:namespace_app_bytes_weekly:increase
    - expr: |
        (sum by (exported_namespace,app) (increase(logiq_namespace_app_message_count[7d])))
      record: logiq:namespace_app_count_weekly:increase
    - expr: |
        (sum by (exported_namespace,app) (increase(logiq_namespace_app_received_bytes[30d])))
      record: logiq:namespace_app_bytes_monthly:increase
    - expr: |
        (sum by (exported_namespace,app) (increase(logiq_namespace_app_message_count[30d])))
      record: logiq:namespace_app_count_monthly:increase
  
  - name: node-exporter.rules
    rules:
    - expr: |
        count without (cpu) (
          count without (mode) (
            node_cpu_seconds_total{job="node-exporter"}
          )
        )
      record: instance:node_num_cpu:sum
    - expr: |
        1 - avg without (cpu, mode) (
          rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[1m])
        )
      record: instance:node_cpu_utilisation:rate1m
    - expr: |
        (
          node_load1{job="node-exporter"}
        /
          instance:node_num_cpu:sum{job="node-exporter"}
        )
      record: instance:node_load1_per_cpu:ratio
    - expr: |
        1 - (
          node_memory_MemAvailable_bytes{job="node-exporter"}
        /
          node_memory_MemTotal_bytes{job="node-exporter"}
        )
      record: instance:node_memory_utilisation:ratio
    - expr: |
        rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
      record: instance:node_vmstat_pgmajfault:rate1m
    - expr: |
        rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
      record: instance_device:node_disk_io_time_seconds:rate1m
    - expr: |
        rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
      record: instance_device:node_disk_io_time_weighted_seconds:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_bytes_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_bytes_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_drop_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_drop_excluding_lo:rate1m
  - name: kube-scheduler.rules
    rules:
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
  - name: node.rules
    rules:
    - expr: |
        sum(min(kube_pod_info{node!=""}) by (cluster, node))
      record: ':kube_pod_info_node_count:'
    - expr: |
        topk by(namespace, pod) (1,
          max by (node, namespace, pod) (
            label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
        ))
      record: 'node_namespace_pod:kube_pod_info:'
    - expr: |
        count by (cluster, node) (sum by (node, cpu) (
          node_cpu_seconds_total{job="node-exporter"}
        * on (namespace, pod) group_left(node)
          node_namespace_pod:kube_pod_info:
        ))
      record: node:node_num_cpu:sum
    - expr: |
        sum(
          node_memory_MemAvailable_bytes{job="node-exporter"} or
          (
            node_memory_Buffers_bytes{job="node-exporter"} +
            node_memory_Cached_bytes{job="node-exporter"} +
            node_memory_MemFree_bytes{job="node-exporter"} +
            node_memory_Slab_bytes{job="node-exporter"}
          )
        ) by (cluster)
      record: :node_memory_MemAvailable_bytes:sum
  - name: kube-prometheus-node-recording.rules
    rules:
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m])) BY
        (instance)
      record: instance:node_cpu:rate:sum
    - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
      record: instance:node_network_receive_bytes:rate:sum
    - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
      record: instance:node_network_transmit_bytes:rate:sum
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m])) WITHOUT
        (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total)
        BY (instance, cpu)) BY (instance)
      record: instance:node_cpu:ratio
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
      record: cluster:node_cpu:sum_rate5m
    - expr: cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total)
        BY (instance, cpu))
      record: cluster:node_cpu:ratio
  - name: kube-prometheus-general.rules
    rules:
    - expr: count without(instance, pod, node) (up == 1)
      record: count:up1
    - expr: count without(instance, pod, node) (up == 0)
      record: count:up0
  - name: kube-state-metrics
    rules:
    - alert: KubeStateMetricsListErrors
      annotations:
        message: kube-state-metrics is experiencing errors at an elevated rate in
          list operations. This is likely causing it to not be able to expose metrics
          about Kubernetes objects correctly or at all.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricslisterrors
      expr: |
        (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m]))
          /
        sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])))
        > 0.01
      for: 15m
      labels:
        severity: critical
    - alert: KubeStateMetricsWatchErrors
      annotations:
        message: kube-state-metrics is experiencing errors at an elevated rate in
          watch operations. This is likely causing it to not be able to expose metrics
          about Kubernetes objects correctly or at all.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricswatcherrors
      expr: |
        (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m]))
          /
        sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])))
        > 0.01
      for: 15m
      labels:
        severity: critical
  - name: node-exporter
    rules:
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available space left and is filling
          up.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
        summary: Filesystem is predicted to run out of space within the next 24 hours.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 40
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available space left and is filling
          up fast.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
        summary: Filesystem is predicted to run out of space within the next 4 hours.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 15
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available space left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace
        summary: Filesystem has less than 5% space left.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available space left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace
        summary: Filesystem has less than 3% space left.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeFilesystemFilesFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available inodes left and is filling
          up.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
        summary: Filesystem is predicted to run out of inodes within the next 24 hours.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 40
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemFilesFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available inodes left and is filling
          up fast.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
        summary: Filesystem is predicted to run out of inodes within the next 4 hours.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 20
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeFilesystemAlmostOutOfFiles
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available inodes left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles
        summary: Filesystem has less than 5% inodes left.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: warning
    - alert: NodeFilesystemAlmostOutOfFiles
      annotations:
        description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
          has only {{ printf "%.2f" $value }}% available inodes left.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles
        summary: Filesystem has less than 3% inodes left.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        )
      for: 1h
      labels:
        severity: critical
    - alert: NodeNetworkReceiveErrs
      annotations:
        description: '{{$labels.instance }} interface {{ $labels.device }} has encountered
          {{ printf "%.0f" $value }} receive errors in the last two minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworkreceiveerrs
        summary: Network interface is reporting many receive errors.
      expr: |
        increase(node_network_receive_errs_total[2m]) > 10
      for: 1h
      labels:
        severity: warning
    - alert: NodeNetworkTransmitErrs
      annotations:
        description: '{{$labels.instance }} interface {{ $labels.device }} has encountered
          {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworktransmiterrs
        summary: Network interface is reporting many transmit errors.
      expr: |
        increase(node_network_transmit_errs_total[2m]) > 10
      for: 1h
      labels:
        severity: warning
    - alert: NodeHighNumberConntrackEntriesUsed
      annotations:
        description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodehighnumberconntrackentriesused
        summary: Number of conntrack are getting close to the limit.
      expr: |
        (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75
      labels:
        severity: warning
    - alert: NodeTextFileCollectorScrapeError
      annotations:
        description: Node Exporter text file collector failed to scrape.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodetextfilecollectorscrapeerror
        summary: Node Exporter text file collector failed to scrape.
      expr: |
        node_textfile_scrape_error{job="node-exporter"} == 1
      labels:
        severity: warning
    - alert: NodeClockSkewDetected
      annotations:
        message: Clock on {{ $labels.instance }} is out of sync by more than 300s.
          Ensure NTP is configured correctly on this host.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeclockskewdetected
        summary: Clock skew detected.
      expr: |
        (
          node_timex_offset_seconds > 0.05
        and
          deriv(node_timex_offset_seconds[5m]) >= 0
        )
        or
        (
          node_timex_offset_seconds < -0.05
        and
          deriv(node_timex_offset_seconds[5m]) <= 0
        )
      for: 10m
      labels:
        severity: warning
    - alert: NodeClockNotSynchronising
      annotations:
        message: Clock on {{ $labels.instance }} is not synchronising. Ensure NTP
          is configured on this host.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeclocknotsynchronising
        summary: Clock not synchronising.
      expr: |
        min_over_time(node_timex_sync_status[5m]) == 0
      for: 10m
      labels:
        severity: warning
  - name: kubernetes-apps
    rules:
    - alert: KubePodCrashLooping
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} {{ $labels.container }} is restarting {{ printf "%.2f" $value }} times / 5 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
      expr: |
        rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[5m]) * 60 * 5 > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubePodNotReady
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
          state for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
      expr: |
        sum by (namespace, pod) (
          max by(namespace, pod) (
            kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}
          ) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (
            1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})
          )
        ) > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        message: Deployment generation for {{ $labels.namespace }}/{{$labels.deployment}} does not match,
         this indicates that the Deployment has failed but has not been rolled back.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
      expr: |
        kube_deployment_status_observed_generation{job="kube-state-metrics"}
          !=
        kube_deployment_metadata_generation{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        message: Deployment {{$labels.namespace}}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
      expr: |
        (
          kube_deployment_spec_replicas{job="kube-state-metrics"}
            !=
          kube_deployment_status_replicas_available{job="kube-state-metrics"}
        ) and (
          changes(kube_deployment_status_replicas_updated{job="kube-state-metrics"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
          not matched the expected number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
      expr: |
        (
          kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
            !=
          kube_statefulset_status_replicas{job="kube-state-metrics"}
        ) and (
          changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        message: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has
          not been rolled back.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
      expr: |
        kube_statefulset_status_observed_generation{job="kube-state-metrics"}
          !=
        kube_statefulset_metadata_generation{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
          has not been rolled out.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
      expr: |
        (
          max without (revision) (
            kube_statefulset_status_current_revision{job="kube-state-metrics"}
              unless
            kube_statefulset_status_update_revision{job="kube-state-metrics"}
          )
            *
          (
            kube_statefulset_replicas{job="kube-state-metrics"}
              !=
            kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
          )
        )  and (
          changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        message: Only {{ $value }} of the desired Pods of DaemonSet
          {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
      expr: |
        kube_daemonset_status_number_ready{job="kube-state-metrics"}
          /
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"} < 1.00
      for: 15m
      labels:
        severity: warning
    - alert: KubeContainerWaiting
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container }}
          has been in waiting state for longer than 1 hour.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontainerwaiting
      expr: |
        sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics"}) > 0
      for: 1h
      labels:
        severity: warning
    - alert: KubeDaemonSetNotScheduled
      annotations:
        message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
      expr: |
        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
          -
        kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
      for: 10m
      labels:
        severity: warning
    - alert: KubeDaemonSetMisScheduled
      annotations:
        message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
      expr: |
        kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeJobCompletion
      annotations:
        message: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
          than 12 hours to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
      expr: |
        kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"}  > 0
      for: 12h
      labels:
        severity: warning
    - alert: KubeJobFailed
      annotations:
        message: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
      expr: |
        kube_job_failed{job="kube-state-metrics"}  > 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeHpaReplicasMismatch
      annotations:
        message: HPA {{ $labels.namespace }}/{{ $labels.hpa }} has not matched the
          desired number of replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpareplicasmismatch
      expr: |
        (kube_hpa_status_desired_replicas{job="kube-state-metrics"}
          !=
        kube_hpa_status_current_replicas{job="kube-state-metrics"})
          and
        changes(kube_hpa_status_current_replicas[15m]) == 0
      for: 15m
      labels:
        severity: warning
    - alert: KubeHpaMaxedOut
      annotations:
        message: HPA {{ $labels.namespace }}/{{ $labels.hpa }} has been running at
          max replicas for longer than 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpamaxedout
      expr: |
        kube_hpa_status_current_replicas{job="kube-state-metrics"}
          ==
        kube_hpa_spec_max_replicas{job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
  - name: kubernetes-resources
    rules:
    - alert: KubeCPUOvercommit
      annotations:
        message: Cluster has overcommitted CPU resource requests for Pods and cannot
          tolerate node failure.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
      expr: |
        sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum{})
          /
        sum(kube_node_status_allocatable_cpu_cores)
          >
        (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
      for: 5m
      labels:
        severity: warning
    - alert: KubeMemoryOvercommit
      annotations:
        message: Cluster has overcommitted memory resource requests for Pods and cannot
          tolerate node failure.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememoryovercommit
      expr: |
        sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{})
          /
        sum(kube_node_status_allocatable_memory_bytes)
          >
        (count(kube_node_status_allocatable_memory_bytes)-1)
          /
        count(kube_node_status_allocatable_memory_bytes)
      for: 5m
      labels:
        severity: warning
    - alert: KubeCPUQuotaOvercommit
      annotations:
        message: Cluster has overcommitted CPU resource requests for Namespaces.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuquotaovercommit
      expr: |
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
          /
        sum(kube_node_status_allocatable_cpu_cores)
          > 1.5
      for: 5m
      labels:
        severity: warning
    - alert: KubeMemoryQuotaOvercommit
      annotations:
        message: Cluster has overcommitted memory resource requests for Namespaces.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememoryquotaovercommit
      expr: |
        sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
          /
        sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"})
          > 1.5
      for: 5m
      labels:
        severity: warning
    - alert: KubeQuotaFullyUsed
      annotations:
        message: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotafullyused
      expr: |
        kube_resourcequota{job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
          >= 1
      for: 15m
      labels:
        severity: info
    - alert: CPUThrottlingHigh
      annotations:
        message: '{{ $value | humanizePercentage }} throttling of CPU in namespace
          {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh
      expr: |
        sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (container, pod, namespace)
          /
        sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)
          > ( 50 / 100 )
      for: 15m
      labels:
        severity: info
    - alert: KubePersistentVolumeErrors
      annotations:
        message: The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
      expr: |
        kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
      for: 5m
      labels:
        severity: critical
  - name: kubernetes-system
    rules:
    - alert: KubeVersionMismatch
      annotations:
        message: There are {{ $value }} different semantic versions of Kubernetes
          components running.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
      expr: |
        count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*).*"))) > 1
      for: 15m
      labels:
        severity: warning
    - alert: KubeClientErrors
      annotations:
        message: Kubernetes API server client {{ $labels.job }}/{{ $labels.instance }} is experiencing {{ $value | humanizePercentage }} errors.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
      expr: |
        (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
          /
        sum(rate(rest_client_requests_total[5m])) by (instance, job))
        > 0.01
      for: 15m
      labels:
        severity: warning
    - alert: AggregatedAPIErrors
      annotations:
        message: An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has
          reported errors. The number of errors have increased for it in the past
          five minutes. High values indicate that the availability of the service
          changes too often.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapierrors
      expr: |
        sum by(name, namespace)(increase(aggregator_unavailable_apiservice_count[5m])) > 2
      labels:
        severity: warning
    - alert: AggregatedAPIDown
      annotations:
        message: An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has
          been only {{ $value }}% available over the last 5m.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapidown
      expr: |
        (1 - max by(name, namespace)(avg_over_time(aggregator_unavailable_apiservice[5m]))) * 100 < 90
      for: 5m
      labels:
        severity: warning
    - alert: KubeNodeUnreachable
      annotations:
        message: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodeunreachable
      expr: |
        (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
      labels:
        severity: warning
    - alert: KubeNodeReadinessFlapping
      annotations:
        message: The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodereadinessflapping
      expr: |
        sum(changes(kube_node_status_condition{status="true",condition="Ready"}[15m])) by (node) > 2
      for: 15m
      labels:
        severity: warning
  - name: prometheus
    rules:
    - alert: PrometheusBadConfig
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed to
          reload its configuration.
        summary: Failed Prometheus configuration reload.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_config_last_reload_successful{job="prometheus-k8s",namespace="monitoring"}[5m]) == 0
      for: 10m
      labels:
        severity: critical
    - alert: PrometheusNotificationQueueRunningFull
      annotations:
        description: Alert notification queue of Prometheus {{ $labels.namespace }}/{{ $labels.pod }}
          is running full.
        summary: Prometheus alert notification queue predicted to run full in less
          than 30m.
      expr: |
        # Without min_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          predict_linear(prometheus_notifications_queue_length{job="prometheus-k8s",namespace="monitoring"}[5m], 60 * 30)
        >
          min_over_time(prometheus_notifications_queue_capacity{job="prometheus-k8s",namespace="monitoring"}[5m])
        )
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
      annotations:
        description: '{{ printf "%.1f"  $value }}% errors while sending alerts from
          Prometheus {{ $labels.namespace }}/{{ $labels.pod }} to Alertmanager {{$labels.alertmanager}}.'
        summary: Prometheus has encountered more than 1% errors sending alerts to
          a specific Alertmanager.
      expr: |
        (
          rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="monitoring"}[5m])
        /
          rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="monitoring"}[5m])
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
      annotations:
        description: '{{ printf "%.1f" $value }}% minimum errors while sending alerts
          from Prometheus {{ $labels.namespace }}/{{ $labels.pod }} to any Alertmanager.'
        summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
      expr: |
        min without(alertmanager) (
          rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="monitoring"}[5m])
        /
          rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="monitoring"}[5m])
        )
        * 100
        > 3
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusNotConnectedToAlertmanagers
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not connected
          to any Alertmanagers.
        summary: Prometheus is not connected to any Alertmanagers.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_notifications_alertmanagers_discovered{job="prometheus-k8s",namespace="monitoring"}[5m]) < 1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusTSDBReloadsFailing
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has detected
          {{ $value }} reload failures over the last 3h.
        summary: Prometheus has issues reloading blocks from disk.
      expr: |
        increase(prometheus_tsdb_reloads_failures_total{job="prometheus-k8s",namespace="monitoring"}[3h]) > 0
      for: 4h
      labels:
        severity: warning
    - alert: PrometheusTSDBCompactionsFailing
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has detected
          {{ $value }} compaction failures over the last 3h.
        summary: Prometheus has issues compacting blocks.
      expr: |
        increase(prometheus_tsdb_compactions_failed_total{job="prometheus-k8s",namespace="monitoring"}[3h]) > 0
      for: 4h
      labels:
        severity: warning
    - alert: PrometheusNotIngestingSamples
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is not ingesting
          samples.
        summary: Prometheus is not ingesting samples.
      expr: |
        rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s",namespace="monitoring"}[5m]) <= 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusDuplicateTimestamps
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is dropping
          {{ printf "%.4g"  $value }} samples/s with different values but duplicated
          timestamp.
        summary: Prometheus is dropping samples with duplicate timestamps.
      expr: |
        rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOutOfOrderTimestamps
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} is dropping
          {{ printf "%.4g"  $value }} samples/s with timestamps arriving out of order.
        summary: Prometheus drops samples with out-of-order timestamps.
      expr: |
        rate(prometheus_target_scrapes_sample_out_of_order_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusRemoteStorageFailures
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} failed to send
          {{ printf "%.1f" $value }}% of the samples to {{$labels.remote_name}}:{{$labels.url}}
        summary: Prometheus fails to send samples to remote storage.
      expr: |
        (
          rate(prometheus_remote_storage_failed_samples_total{job="prometheus-k8s",namespace="monitoring"}[5m])
        /
          (
            rate(prometheus_remote_storage_failed_samples_total{job="prometheus-k8s",namespace="monitoring"}[5m])
          +
            rate(prometheus_remote_storage_succeeded_samples_total{job="prometheus-k8s",namespace="monitoring"}[5m])
          )
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusRemoteWriteBehind
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} remote write
          is {{ printf "%.1f" $value }}s behind for {{$labels.remote_name}}:{{$labels.url}}.
        summary: Prometheus remote write is behind.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="prometheus-k8s",namespace="monitoring"}[5m])
        - on(job, instance) group_right
          max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="prometheus-k8s",namespace="monitoring"}[5m])
        )
        > 120
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusRemoteWriteDesiredShards
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} remote write {{$labels.remote_name}}:{{$labels.url}}, which is more than the max of prometheus_remote_storage_shards_max{instance="{{ $labels.instance }}",job="prometheus-k8s",namespace="monitoring"}.
        summary: Prometheus remote write desired shards calculation wants to run more
          than configured max shards.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_shards_desired{job="prometheus-k8s",namespace="monitoring"}[5m])
        >
          max_over_time(prometheus_remote_storage_shards_max{job="prometheus-k8s",namespace="monitoring"}[5m])
        )
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusRuleFailures
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has failed to
          evaluate {{ printf "%.0f" $value }} rules in the last 5m.
        summary: Prometheus is failing rule evaluations.
      expr: |
        increase(prometheus_rule_evaluation_failures_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
      for: 15m
      labels:
        severity: critical
    - alert: PrometheusMissingRuleEvaluations
      annotations:
        description: Prometheus {{ $labels.namespace }}/{{ $labels.pod }} has missed {{ printf "%.0f" $value }} rule group evaluations in the last 5m.
        summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
      expr: |
        increase(prometheus_rule_group_iterations_missed_total{job="prometheus-k8s",namespace="monitoring"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
  - name: alertmanager.rules
    rules:
    - alert: AlertmanagerFailedReload
      annotations:
        message: Reloading Alertmanager's configuration has failed for {{$labels.namespace}}/{{ $labels.pod }}.
      expr: |
        alertmanager_config_last_reload_successful{job="alertmanager-main",namespace="monitoring"} == 0
      for: 10m
      labels:
        severity: warning
    - alert: AlertmanagerMembersInconsistent
      annotations:
        message: Alertmanager has not found all other members of the cluster.
      expr: |
        alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"}
          != on (service) GROUP_LEFT()
        count by (service) (alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"})
      for: 5m
      labels:
        severity: critical
  - name: node-network
    rules:
    - alert: NodeNetworkInterfaceFlapping
      annotations:
        message: Network interface "{{$labels.device}}" changing it's up status
          often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
      expr: |
        changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
      for: 2m
      labels:
        severity: warning
  - name: prometheus-operator
    rules:
    - alert: PrometheusOperatorListErrors
      annotations:
        message: Errors while performing List operations in controller {{$labels.controller}}
          in {{ $labels.namespace }} namespace.
      expr: |
        (sum by (controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job="prometheus-operator",namespace="monitoring"}[1h])) / sum by (controller,namespace) (rate(prometheus_operator_list_operations_total{job="prometheus-operator",namespace="monitoring"}[1h]))) > 0.4
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusOperatorWatchErrors
      annotations:
        message: Errors while performing Watch operations in controller {{$labels.controller}}
          in {{ $labels.namespace }} namespace.
      expr: |
        (sum by (controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job="prometheus-operator",namespace="monitoring"}[1h])) / sum by (controller,namespace) (rate(prometheus_operator_watch_operations_total{job="prometheus-operator",namespace="monitoring"}[1h]))) > 0.4
      for: 15m
      labels:
        severity: warning
    - alert: PrometheusOperatorReconcileErrors
      annotations:
        message: Errors while reconciling {{$labels.controller}} in {{ $labels.namespace }} Namespace.
      expr: |
        rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator",namespace="monitoring"}[5m]) > 0.1
      for: 10m
      labels:
        severity: warning
    - alert: PrometheusOperatorNodeLookupErrors
      annotations:
        message: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
      expr: |
        rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator",namespace="monitoring"}[5m]) > 0.1
      for: 10m
      labels:
        severity: warning
---
# Source: apica-ascent/charts/logiq-flash/templates/serviceMonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: logiq-flash
    release: "prometheus"
  name: prometheus-default-logiq-flash
  namespace: default
spec:
  endpoints:
  - interval: 2m
    path: /metrics
    scheme: https
    targetPort: 9998
    tlsConfig:
      insecureSkipVerify: true
  - interval: 2m
    path: /metrics
    scheme: https
    targetPort: 8899
    tlsConfig:
      insecureSkipVerify: true
  namespaceSelector:
    matchNames:
    - default
  selector:
    matchLabels:
      promMonitor: logiq-flash
---
# Source: apica-ascent/charts/postgres/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: postgres
  namespace: default
  labels:
    app: postgres
    chart: postgres-8.7.3
    release: "release-name"
    heritage: "Helm"
spec:
  endpoints:
    - port: http-metrics
      interval: 1m
      scrapeTimeout: 10s
  namespaceSelector:
    matchNames:
      - default
  selector:
    matchLabels:
      app: postgres
      release: release-name
---
# Source: apica-ascent/charts/prometheus/charts/kube-state-metrics/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-kube-state-metrics
  namespace: default
  labels:
    app.kubernetes.io/name: kube-state-metrics
    helm.sh/chart: kube-state-metrics-2.1.19
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: release-name
  endpoints:
    - port: http
      honorLabels: true
  namespaceSelector:
    matchNames:
    - default
---
# Source: apica-ascent/charts/prometheus/charts/node-exporter/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-node-exporter
  namespace: default
  labels:
    app.kubernetes.io/name: node-exporter
    helm.sh/chart: node-exporter-2.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  jobLabel: jobLabel
  selector:
    matchLabels:
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/instance: release-name
  endpoints:
    - port: metrics
  namespaceSelector:
    matchNames:
    - default
---
# Source: apica-ascent/charts/prometheus/templates/alertmanager/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-prometheus-alertmanager
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: alertmanager
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: alertmanager
  namespaceSelector:
    matchNames:
      - default
  endpoints:
    - port: http
      path: /metrics
---
# Source: apica-ascent/charts/prometheus/templates/exporters/kubelet/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-prometheus-kubelet
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kubelet
spec:
  jobLabel: k8s-app
  selector:
    matchLabels:
      k8s-app: kubelet
  namespaceSelector:
    matchNames:
      - kube-system
  endpoints:
    - port: https-metrics
      scheme: https
      tlsConfig:
        caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        serverName: kubernetes
        insecureSkipVerify: true
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      honorLabels: true
    - port: https-metrics
      path: /metrics/cadvisor
      scheme: https
      tlsConfig:
        caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        serverName: kubernetes
        insecureSkipVerify: true
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      honorLabels: true
---
# Source: apica-ascent/charts/prometheus/templates/prometheus-operator/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-prometheus-operator
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: operator
spec:
  endpoints:
    - port: http
      honorLabels: true
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: operator
  namespaceSelector:
    matchNames:
      - default
---
# Source: apica-ascent/charts/prometheus/templates/prometheus/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-prometheus-prometheus
  namespace: default
  labels:
    app.kubernetes.io/name: prometheus
    helm.sh/chart: prometheus-6.5.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: prometheus
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: prometheus
  namespaceSelector:
    matchNames:
      - default
  endpoints:
    - port: http
      path: /metrics
---
# Source: apica-ascent/charts/redis/templates/metrics-prometheus.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: redis
  namespace: default
  labels:
    app: redis
    chart: redis-10.6.5
    release: release-name
    heritage: Helm
    prometheus: "kube-prometheus"
    release: "prometheus"
spec:
  endpoints:
  - port: metrics
    interval: 1m
  selector:
    matchLabels:
      app: redis
      release: release-name
  namespaceSelector:
    matchNames:
    - default
---
# Source: apica-ascent/charts/thanos/templates/bucketweb/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-thanos-bucketweb
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: bucketweb
spec:
  endpoints:
    - port: http
  namespaceSelector:
    matchNames:
      - default
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: bucketweb
---
# Source: apica-ascent/charts/thanos/templates/compactor/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-thanos-compactor
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: compactor
spec:
  endpoints:
    - port: http
  namespaceSelector:
    matchNames:
      - default
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: compactor
---
# Source: apica-ascent/charts/thanos/templates/query/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-thanos-query
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
spec:
  endpoints:
    - port: http
  namespaceSelector:
    matchNames:
      - default
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: query
---
# Source: apica-ascent/charts/thanos/templates/receive/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-thanos-receive
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: receive
spec:
  endpoints:
    - port: http
  namespaceSelector:
    matchNames:
      - default
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: receive
---
# Source: apica-ascent/charts/thanos/templates/ruler/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-thanos-ruler
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ruler
spec:
  endpoints:
    - port: http
  namespaceSelector:
    matchNames:
      - default
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: ruler
---
# Source: apica-ascent/charts/thanos/templates/storegateway/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-thanos-storegateway
  namespace: "default"
  labels:
    app.kubernetes.io/name: thanos
    helm.sh/chart: thanos-8.2.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: storegateway
spec:
  endpoints:
    - port: http
  namespaceSelector:
    matchNames:
      - default
  selector:
    matchLabels:
      app.kubernetes.io/name: thanos
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: storegateway
---
# Source: apica-ascent/charts/flash-discovery/templates/license.yaml
apiVersion: v1
kind: Secret
metadata:
  name: flash-discovery-license
  namespace: default
  labels:
    app.kubernetes.io/name: flash-discovery
    helm.sh/chart: flash-discovery-1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.0.0
  annotations:
    "helm.sh/hook": "pre-install"
    "helm.sh/hook-delete-policy": "before-hook-creation"
data:
  license.jws: |-
    ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnNhV01pT25zaVkyeDFjM1JsY2kxcFpDSTZJbFZ1YTI1dmQyNGlMQ0ptYkhWbGJuUmtabTl5ZDJGeVpDMWxibUZpYkdWa0lqb2lkSEoxWlNJc0ltWnNkV1Z1ZEdSbWIzSjNZWEprTFhKaGRHVXRiR2x0YVhRaU9pSWlMQ0pzYjJkbWJHOTNJam9pZEhKMVpTSXNJbXh2WjJac2IzZFBibXg1SWpvaVptRnNjMlVpTENKdWRXMWlaWEl0YjJZdFlXeHNiM2RsWkMxa1pYTjBhVzVoYVhSdmJpMWlkV05yWlhSeklqb2lNU0lzSW01MWJXSmxjaTF2WmkxaGJHeHZkMlZrTFdac1lYTm9MWEJ2WkhNaU9pSXlJaXdpY21WaGJDMTBhVzFsTFhOMGNtVmhiV2x1WnlJNkluUnlkV1VpTENKeVpXeHdMV1Z1WVdKc1pXUWlPaUowY25WbElpd2ljbVZzY0MxeVlYUmxMV3hwYldsMElqb2lJaXdpY3pNdFkzVnpkRzl0TFhCaGNuUnBkR2x2Ym1sdVp5STZJblJ5ZFdVaUxDSnplWE5zYjJjdFpXNWhZbXhsWkNJNkluUnlkV1VpTENKemVYTnNiMmN0Y21GMFpTMXNhVzFwZENJNklpSjlMQ0psZUhBaU9qRTVPVGsxTURReE5EVXNJbXAwYVNJNkltdGxlVWxrSWl3aWFXRjBJam94TmpnME1UUTBNVFExTENKcGMzTWlPaUpzYjJkcGNTNWpiRzkxWkNJc0luTjFZaUk2SWxCeWJ5MU1hV05sYm5ObEluMC5HTWJGMWVicDJLczJ1VkVhRlRLTHFVVHVYeDZiWE1IQXQzeTVvNlNLSXdwOFFqVWE2czVvYmpEbjVPN3JLbEJOd0xXTkZzckhlbUtCWDg1R2RiN21uODN3OGhyTFRKY0ZkVW9UZFhJeDdRcDUtbFdTTVVvZFJLeC1URE5kRGZYTTlfMVFHM1FfVHFXdlIzTDc1cTlaN2hzUXJYd3ozVDdGT0hYLU16b0RGTDlsVVpfN081T3FtOUZyZWtZZVJCdENSb3dDcnJ3R1JjVURnSUJMUkZsNGtOcVBBTHF5ZlplSDhzU3haTGtKNUtDazZRNG4xR1RZSVVQaHBSOW81RmFaUjR1dTlBRXlBdW90X1pzckxBYTZiMWdUb3VtMEZhMml6bVItY1hXcFV0cHlNc2xscS1sMG1nakRUVHVkNVdEZTBvOVhKSnhhcm5yM1pRWVh1MFRXcmJaMGpBdk5fS0VHYWlTUWxhOE1meFBtUE9JeERVMk5aM1h4MUxtam5pcHBQQ2habklHWXZyOFJ0YWR3dzhQNmZ0ZjBIbWJXSkxUcE44QjI5eVZfUEFYZk44VkZQeWMzOC0tQWlteUhLS24tWFR1TVdmYlBuM2luMHlUM1FjY0w0ODNmaXdTWFlSQ3ZLU0VGSEVhMVlDQko3SkZsb3BDQ19IN3pLVUlKQ0pNMU9YMHc4NURON25vRW5vRmJGdXdMNzlVN2tnRTRXMlBkY1RIbkxJb3YyT2VFMVhCb3BuVXJaQ21UNWYwazZZdlVORmJ4UzRJWWREeDdjampQdWtuWVRYc1ZSSFctSlQ0S0M1cThSNDRONFI2OWdBU2FKeFEwWG5NZnVjY1IxWjV0MUNzQkpFSWw4aEh6RHpNWWdwQ25YdnVnWW9TeDBBLU1oUDFMYUZSWXdTRQ==
---
# Source: apica-ascent/charts/kubernetes-ingress/templates/controller-defaultcertsecret.yaml
apiVersion: v1
kind: Secret
type: kubernetes.io/tls
metadata:
  name: release-name-kubernetes-ingress-default-cert
  namespace: default
  labels:
    app.kubernetes.io/name: kubernetes-ingress
    helm.sh/chart: kubernetes-ingress-1.37.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 0.32.0
  annotations:
    "helm.sh/hook": "pre-install"
    "helm.sh/hook-delete-policy": "before-hook-creation"
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURKVENDQWcyZ0F3SUJBZ0lSQU5XTGVOM1BNSzZ3Qk9GaUt4QUhaNWt3RFFZSktvWklodmNOQVFFTEJRQXcKR3pFWk1CY0dBMVVFQXhNUWJtZHBibmd0YVc1bmNtVnpjeTFqWVRBZUZ3MHlOREExTVRReU16TXhNVFZhRncweQpOVEExTVRReU16TXhNVFZhTUI4eEhUQWJCZ05WQkFNVEZISmxiR1ZoYzJVdGJtRnRaUzVrWldaaGRXeDBNSUlCCklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEzdTRKc05ISjVLa3Y4SnhiUk83MURFK1QKbDBRRTVaTDVGbk5mY3dMSENlT2M0R08vUzAwSUpYRU8zOEpQcFl3MHlwR1dESEhacWY5Y0kxWTUxVWlmcnpBZApLWkZVV2hCTWViS3RQYW9YQTkySlpOZTFrZnZCb0gxS1FZc28rY3lqbXFrRjRCWlJzcWNXT0lhcUgyeTF0elVUCmk2RlF5MUt3M2dHOVUvVUh6OFprWTdDT3AydjRzcDh0dFhFY1pxQzFwajdCbnBjMWl5MGJUUGpONXg5UnFqNksKeGhSM05EOGZaVGx5dWw5QjNHMkhGTWIyUU1oT3p0YS9yMGxwOTdseTVYdWlEcTNEMVVSNDNNNmJMY1NRU1FBWQpMK1pFQmpuOWRlVVpEMGtnaHVLVzNFNml6aldvZHRzNEh0VEtZdVlLSzd5K0dMRFkvaEVkVDhyUlZvRGRwd0lECkFRQUJvMkF3WGpBT0JnTlZIUThCQWY4RUJBTUNCYUF3SFFZRFZSMGxCQll3RkFZSUt3WUJCUVVIQXdFR0NDc0cKQVFVRkJ3TUNNQXdHQTFVZEV3RUIvd1FDTUFBd0h3WURWUjBqQkJnd0ZvQVVCVy8yYnFrMEQ1RCtkdGZoMmhXSgpocXlwby9rd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFDMGVRYjRJK0dSSG4yRmhUL01sY2FjdE9kcTRsbEZhCkpEdTRWbGtiMkxYd3BzOThadTV6ZW9qWFlIejVjZGhqbXBpMGxMMGhNRU5YSmFEZ2tIeUdNMVo5WFlNZVk5TUgKNUtPOGpYTnV0M2NiU3VRak1ETXp2cnRUaWdmR01FYW4yZEhzVWRHMXcrcEZWSjY0VytxdzVGeDYrdmowMzdqTgpJT2FDN1ZCb2t0MlBPMExvZCtMNjdjWjhJNUxyNC9OU2UxckFUSGlxMFpMMmxMQ1BFbFJXLzBlWjk2YmxnQXI2CmNucHRMN3NWNmlqKzNleWV5T3NicitkZXdRU1ZuYytDSjVUcWx2dHdlZ21GeVc4VVI4RnpwNGRiTXhld2sxR2EKZWExaVpxeExBMGh0WDV1MVNYc2NQeGRKYWNack9yTEtEMWpVa3QvT3plV25NRHVwdVpsQjRtZz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
  tls.key: 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
---
# Source: apica-ascent/templates/shared-secret.yaml
apiVersion: v1
kind: Secret
type: kubernetes.io/tls
metadata:
  name: logiq-shared-secret
  namespace: default
  labels:
    app.kubernetes.io/name: apica-ascent
    helm.sh/chart: apica-ascent-2.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: v3.7.1
  annotations:
    "helm.sh/hook": "pre-install"
    "helm.sh/hook-delete-policy": "before-hook-creation"
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURIVENDQWdXZ0F3SUJBZ0lSQVB6MC9ZM2hxbGQ0V0FQODJPMkhHOGt3RFFZSktvWklodmNOQVFFTEJRQXcKRXpFUk1BOEdBMVVFQXhNSWJHOW5hWEV0WTJFd0hoY05NalF3TlRFME1qTXpNVEUxV2hjTk16RXdOVEV6TWpNegpNVEUxV2pBZk1SMHdHd1lEVlFRREV4UnlaV3hsWVhObExXNWhiV1V1WkdWbVlYVnNkRENDQVNJd0RRWUpLb1pJCmh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTWQ0R0RpS2pvTDRsWXhDbmd0SXpCVEwwdHIvNFVYMDRFcnEKQ2dkMW9DazJxdW9xcUlVcWR3RXlmNkgwZGlDTlhxc2ovTWVGRFovbmgzZHdNWFdSVkgwbXdiQUdUcWhtN1ZvRAppaEh2VEswQ1EwOVA3WEUxWHQ1NU5NOUZGMkdQb2VFUkpPUVlsYkRYYTh2RktSdHA5c2RuTU1XVk9PWi83ckF0CmRZbzhpZW10VUlZWjFNblBMZWYrZ0lhUFFCQ1ZiWHZEc3NRei9YRUlwWWwydUpyZUZWTkRqSCtWenljK1lOcEoKcHhNZG5uZFEraDRXQWRqa2FHa0xFbXdKNTcxeHJkd0Q4aXBBYlRxNDdDZXkyeGtZNWNucWNDcVNHN3NVcXFqNwppQ3gxN3UwUkdGRDY1bWtLNlVpN3NpNWJlRXMxbmRWU2JYbmdkcHhEYWY2SjRSV01BeDhDQXdFQUFhTmdNRjR3CkRnWURWUjBQQVFIL0JBUURBZ1dnTUIwR0ExVWRKUVFXTUJRR0NDc0dBUVVGQndNQkJnZ3JCZ0VGQlFjREFqQU0KQmdOVkhSTUJBZjhFQWpBQU1COEdBMVVkSXdRWU1CYUFGRkh5bGM2eGFVSzRNYTNzNndJVkswVzlTRlFwTUEwRwpDU3FHU0liM0RRRUJDd1VBQTRJQkFRQlVVYW1qME4xNldoYys0Z1NjalVxWlNycEFUVGg0em96WlFNa3NqbWFkCnJOUWlVQ3JETXArZ3lvd1QxcEp1NlBmRkJDN1p0VDAzWkw3VFZvMHpMSGxnK0RhVlhRUTM3cUgvTEVMaURGclUKd2JBZUk3aGtxYm9JQmg1ai9qYnF4dHFMWDZWN2lXVEVld0JISUlxRTRsRDE5TVAxYUMzNGszU3I0TE1vcFcwQgpPbGFwZUdQOWFjR29zY3E1UmRCdDVRN1luVnlIT0xacWpkYVZUZWwxdDNKTVQ3WFFSM3N4LzJvV0w1cFlINkhZCnJ0aUl3WXdENXVBS3F2eGZTbGQ5ZGVMckkwSkRzNkh1UGhSU1NMYlYyOCtCTFpkbm8yS0lpS0kzNUxHRGRTSGQKclA3T3hhbEtPcGNuQ2VMUVYraHlIKzBlZ29DdCtNREFsTHIxY0NRVU9vQjEKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
  tls.key: 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
  awsaccesskey: "bG9naXFfYWNjZXNz"
  awssecretkey: "bG9naXFfc2VjcmV0"
  s3_access: "bG9naXFfYWNjZXNz" 
  s3_secret: "bG9naXFfc2VjcmV0" 
  s3_bucket: "bG9naXE="
---
# Source: apica-ascent/templates/thanos-secret.yaml
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: thanos-objectstorage-config
  namespace: default
  annotations:
    "helm.sh/hook": "pre-install"
    "helm.sh/hook-delete-policy": "before-hook-creation"
  labels:
    app.kubernetes.io/name: apica-ascent
    helm.sh/chart: apica-ascent-2.0.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  thanos.yaml: dHlwZTogczMKCmNvbmZpZzogCiAgCiAgYWNjZXNzX2tleTogbG9naXFfYWNjZXNzCiAgc2VjcmV0X2tleTogbG9naXFfc2VjcmV0CiAgCiAgYnVja2V0OiBsb2dpcSAKCiAgZW5kcG9pbnQ6IHMzLWdhdGV3YXk6OTAwMCAKICBpbnNlY3VyZTogdHJ1ZQoKICByZWdpb246IHVzLWVhc3QtMSAK
---
# Source: apica-ascent/charts/logiqctl/templates/post-install-upload-dashboard-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-logiqctl-upload-dashboard-job
  namespace: default
  labels:
    app: logiqctl-upload-dashboard-job
    chart: logiqctl-1.0.0
    release: release-name
    heritage: Helm
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-weight": "10"
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
spec:
  template:
    metadata:
      labels:
        app: logiqctl-job
        release: release-name
    spec:
      serviceAccountName: logiqctl
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
        runAsGroup: 1000
      restartPolicy: OnFailure


      volumes:
        - name: logiqctl-configuration
          projected:
            sources:
            - configMap:
                name: release-name-logiqctl
            - secret:
                name: release-name-logiqctl
      containers:
      - name: logiqctl
        image: "logiqai/logiqctl:2.0.4"
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "-c"]
        args:
        - echo uploading;
          until $(curl --output /dev/null --silent --fail http://coffee); do echo "waiting for coffee"; sleep 10; done;
          /flash/bin/logiqctl config set-cluster coffee;
          export ACCESS=$(cat /flash/config/export/admin_email) ; export SECRET=$(cat /flash/config/export/admin_password) ;
          /flash/bin/logiqctl config set-credential $ACCESS $SECRET ; sleep 5;
          /flash/bin/logiqctl create dashboard -f /flash/config/export/logiq.json;
          while [ $? -ne 0 ]; do sleep 5; echo "Waiting for dashboard to create";/flash/bin/logiqctl create dashboard -f /flash/config/export/logiq.json; done;
          echo done;
        env:
          - name: LOGIQ_NS
            value: default
        volumeMounts:
          - name: logiqctl-configuration
            mountPath: /flash/config/export
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
---
# Source: apica-ascent/charts/s3gateway/templates/post-install-create-bucket-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: s3-gateway-make-bucket-job
  namespace: default
  labels:
    app: s3gateway-make-bucket-job
    chart: s3gateway-5.0.20
    release: release-name
    heritage: Helm
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-weight": "1"
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
spec:
  template:
    metadata:
      labels:
        app: s3gateway-job
        release: release-name
    spec:
      restartPolicy: OnFailure      


      volumes:
        - name: minio-configuration
          projected:
            sources:
            - configMap:
                name: s3-gateway
            - secret:
                name: s3-gateway
      serviceAccountName: "s3-gateway"
      containers:
      - name: minio-mc
        image: "minio/mc:RELEASE.2020-03-14T01-23-37Z"
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "/config/initialize"]
        env:
          - name: MINIO_ENDPOINT
            value: s3-gateway
          - name: MINIO_PORT
            value: "9000"
        volumeMounts:
          - name: minio-configuration
            mountPath: /config
        resources:
          {}

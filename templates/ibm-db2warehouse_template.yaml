---
# Source: ibm-db2warehouse/templates/db2u-ldap-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-na-ibm-db2warehouse-db2u-ldap
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
type: Opaque
data:
  password: "bU9kZVN3TFdzc1NaejY1Q1VxcVY1TklDbA=="
---
# Source: ibm-db2warehouse/templates/db2u-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-na-ibm-db2warehouse-db2u-config"
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    run: "release-na-ibm-db2warehouse-db2u-config"
data:
  db2u-var: |
    SERVICE_NAME ""
    RUNTIME_ENV LOCAL
    SSH_PORT 50022
    BLUADMPASS ""
    BLUDR_ENABLED NO
    BLUDR_EXTMNT_CFG_FILE ""
    REBALANCE_TABLESPACE NO
    SSL_CERT_FILE ""
    SSL_CERT_KEY_FILE ""
    SSL_CERT_DB_TYPE p12
    OBJECT_STORE_ENDPOINT ""
    STAGING_PATH /local/db2inst1/staging
    WV_HACLASS UDB
    FCM_IPS ""
    DB2TYPE "db2wh"
    DBNAME "BLUDB"
    DBWORKLOAD ""
    MLN_TOTAL "1"
    MLN_DISTRIBUTION "0:0"
    DB_PATH /mnt/blumeta0/db2/databases
    TABLE_ORG "COLUMN"
    DB_PAGE_SIZE "32768"
    ENCRYPT_DB "YES"
    INSTANCE_MEMORY_PERCENT "80"
    DB2_COMPATIBILITY_VECTOR "NULL"
    DB_CODESET "UTF-8"
    DB_TERRITORY "US"
    DB_COLLATION_SEQUENCE "IDENTITY"
    DB2INSTANCE "db2inst1"
    INST_GROUP "db2iadm1"
    FENCED_USER "db2fenc1"
    DB_USER_UID 500
    DB_ADM_GROUP_GID 1003
    DB_FENCED_USER_UID 501
    DB_FENCED_GROUP_GID 1002
    DBM_CFG_FILE /mnt/blumeta0/db2_config/custom_dbm.cfg
    DB_CFG_FILE /mnt/blumeta0/db2_config/custom_db.cfg
    DB2_REGVAR_FILE /mnt/blumeta0/db2_config/custom_registry.cfg
    LDAP_ENABLED true
    LDAP_SERVER ""
    LDAP_DOMAIN "blustratus"
    LDAP_ADMIN "bluldap"
    LDAP_USER_GROUP "bluusers"
    LDAP_ADMIN_GROUP "bluadmin"
    DB2_4K_DEV_SUPPORT "false"
    LDAP_PORT 50389

  db2u-future-var: |
    EXTERNAL_SSH_ENABLED NO
    DISABLE_SPARK YES
    SPARK_MEMORY_SHARE 10
    TIMEZONE UTC
    ENABLE_ANALYTICS_ACCELERATOR NO
    STIG_HARDENING NO
    DB2W_UNIFIED_CONSOLE NO
    KUBERSMP NO
    IAM_ENABLED NO
    IAM_STASH_FILE ""
    DATA_ON_MLN0 NO
    SEGMENT_CRED_FILE ""
    ICP4D NO
    NAMESPACE ""
    DNS_SUBDOMAIN ""
    ASPERA_HOST ""
    GUARDIUM_INFO ""
    PRUNE_LOGS_SCHEDULE 2
    USAGE_TRACKING_ENABLED NO
---
# Source: ibm-db2warehouse/templates/db2u-console-config-map.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-na-ibm-db2warehouse-db2u-uc-config
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
data:
  METADB_USER: db2inst1
  METADB_HOST: release-na-ibm-db2warehouse-db2u-engn-svc
  METADB_PORT: "50000"
  METADB_SSL: "false"
  METADB_SECURITY_MECHANISM: "3"
  METADB_DB_NAME: "BLUDB"
  METADB_DS_EXT_TYPE: "DB2LUW"
  METADB_DB2IADM1_GROUP_ID: "1000"
  INSTANCE_ID: ""
  DISABLE_HOST_CHECK: "true"
  SHARED_PV_MOUNTED: "true"
  POD_NAMESPACE: "default"
  DSSERVER_USER_HOME: "/mnt/blumeta0/home"
  LDAP_HOST: "release-na-ibm-db2warehouse-db2u-ldap"
  LDAP_PORT: "50389"
  LDAP_BASE_DN: "dc=blustratus,dc=com"
  LDAP_ROOT_DN: "cn=bluldap,dc=blustratus,dc=com"
  LDAP_USER_GROUP: "bluusers"
  LDAP_ADMIN_GROUP: "bluadmin"
  LDAP_SSL_METHOD: "starttls"
  CREATE_CUSTOMER_DB_PROFILE: "true"
  PLATFORM_CODE: "PLATFORM_K8S_db2wh_smp"
  CP_NAMESPACE: default
---
# Source: ibm-db2warehouse/templates/db2u-hadr-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-na-ibm-db2warehouse-db2u-hadr-config"
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    run: "release-na-ibm-db2warehouse-db2u-hadr-config"
data:
  db2u-hadr-var: |
    HADR_ENABLED "false"
    HADR_INIT_ROLE "STANDARD"
    HADR_PRIMARY_PORT "60006"
    HADR_STANDBY_PORT "60007"
    HADR_REMOTE_HOST "NULL"
    HADR_TARGET_LIST "NULL"
    HADR_REMOTE_INST "db2inst1"
    HADR_TIMEOUT "120"
    HADR_SYNCMODE "NEARSYNC"
    HADR_PEER_WINDOW "120"
---
# Source: ibm-db2warehouse/templates/db2u-lic-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-na-ibm-db2warehouse-db2u-lic"
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    run: "release-na-ibm-db2warehouse-db2u-lic"
data:
  db2u-lic: |
    [LicenseCertificate]
    CheckSum=C37CDFAE6E5CE335500B4876C6E73444
    TimeStamp=1496664869
    PasswordVersion=4
    VendorName=IBM Toronto Lab
    VendorPassword=7v8p4fq2dtfpc
    VendorID=5fbee0ee6feb.02.09.15.0f.48.00.00.00
    ProductName=dashDB
    ProductID=2057
    ProductVersion=1.0.0
    ProductPassword=5cn2tur9wj7hnhk4agessatp
    ProductAnnotation=6; (_uw)
    LicenseStyle=nodelocked
    LicenseStartDate=06/05/2017
    LicenseDuration=7515
    LicenseEndDate=12/31/2037
    LicenseCount=1
    MultiUseRules=
    RegistrationLevel=3
    TryAndBuy=No
    SoftStop=No
    TargetType=ANY
    TargetTypeName=Open Target
    TargetID=ANY
    ExtendedTargetType=
    ExtendedTargetID=
    DerivedLicenseStyle=
    DerivedLicenseStartDate=
    DerivedLicenseEndDate=
    DerivedLicenseAggregateDuration=
    SerialNumber=
    Upgrade=No
    CapacityType=
    Bundle=No
    InstallProgram=
    AdditionalLicenseData=
    CustomAttribute1=No
    CustomAttribute2=No
    CustomAttribute3=No
    SubCapacityEligibleProduct=No
    MaxOfflinePeriod=
---
# Source: ibm-db2warehouse/templates/db2u-wv-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-na-ibm-db2warehouse-db2u-wv-config"
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    run: "release-na-ibm-db2warehouse-db2u-wv-config"
data:
  db2u-wolverine-var: |
    disabled_dms: '[]'
    ha_enabled: "true"
    settings.cluster_quorum_timeout: "300"
    settings.config_file: '{metadir}/wolverine/config.json'
    settings.dm_status_ttl: "30"
    settings.etcd_client_port: "2379"
    settings.etcd_data_dir: '{metadir}/wolverine/{pod}/etcd'
    settings.etcd_server_port: "2380"
    settings.ha_loop_frequency: "10"
    settings.heartbeat_timeout: "30"
    settings.leader_election_timeout: "90"
    settings.local_filesystems: '{"scratch": "/scratch", "data": "/data", "local": "/local"}'
    settings.log_file_path: '{metadir}/wolverine/{pod}/logs/ha.log'
    settings.node_failover_timeout: "30"
    settings.rest-server-port: "8443"
    settings.shared_filesystems: '{"head": "/head", "ldaphome": "/ldaphome"}'
    settings.startup_cluster_quorum_timeout: "120"
    settings.startup_heartbeat_timeout: "120"
    settings.startup_leader_timeout: "120"
    settings.startup_recovery_ready_timeout: "30"
---
# Source: ibm-db2warehouse/templates/db2u-shared-sqllib-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: release-na-ibm-db2warehouse-db2u-sqllib-shared
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
spec:
  storageClassName: ""
  accessModes:
    - "ReadWriteMany"
  resources:
    requests:
      storage: "40Gi"
---
# Source: ibm-db2warehouse/templates/db2u-engn-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-db2u-engn-svc
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    component: "db2wh"
    release: "release-name"
    heritage: "Helm"
spec:
  type: NodePort
  ports:
  - port: 50000
    targetPort: 50000
    protocol: TCP
    name: legacy-server
  - port: 50001
    targetPort: 50001
    protocol: TCP
    name: ssl-server
  selector:
    app: release-na-ibm-db2warehouse
    component: "db2wh"
    type: "engine"
---
# Source: ibm-db2warehouse/templates/db2u-etcd-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-etcd
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    component: "etcd"
    release: "release-name"
    heritage: "Helm"
    icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
    icpdsupport/app: release-na-ibm-db2warehouse
spec:
  ports:
  - port: 2380
    name: etcd-server
  - port: 2379
    name: etcd-client
    protocol: TCP
  clusterIP: None
  selector:
    app: release-na-ibm-db2warehouse
    component: "etcd"
---
# Source: ibm-db2warehouse/templates/db2u-ldap-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-db2u-ldap
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    component: "ldap"
    release: "release-name"
    heritage: "Helm"
spec:
  ports:
  - port: 50389
    targetPort: 50389
    protocol: TCP
    name: release-na-ibm-db2warehouse-db2u-ldap
  selector:
    app: release-na-ibm-db2warehouse
    component: "ldap"
---
# Source: ibm-db2warehouse/templates/db2u-rest-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-db2u-rest-svc
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    component: "db2wh"
    release: "release-name"
    heritage: "Helm"
spec:
  type: NodePort
  ports:
  - port: 50050
    targetPort: 50050
    protocol: TCP
    name: rest-server
  selector:
    app: release-na-ibm-db2warehouse
    component: "db2wh"
    type: "rest"
---
# Source: ibm-db2warehouse/templates/db2u-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-na-ibm-db2warehouse-db2u
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    component: "db2wh"
    release: "release-name"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
  - port: 50000
    targetPort: 50000
    protocol: TCP
    name: db2-server
  - port: 50001
    targetPort: 50001
    protocol: TCP
    name: db2-ssl-server
  - port: 25000
    targetPort: 25000
    protocol: TCP
    name: spark-p25000
  - port: 25001
    targetPort: 25001
    protocol: TCP
    name: spark-p25001
  - port: 25002
    targetPort: 25002
    protocol: TCP
    name: spark-p25002
  - port: 25003
    targetPort: 25003
    protocol: TCP
    name: spark-p25003
  - port: 25004
    targetPort: 25004
    protocol: TCP
    name: spark-p25004
  - port: 25005
    targetPort: 25005
    protocol: TCP
    name: spark-p25005
  selector:
    app: release-na-ibm-db2warehouse
    component: "db2wh"
    type: "engine"
---
# Source: ibm-db2warehouse/templates/db2u-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: "release-na-ibm-db2warehouse-db2u-internal"
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    component: "db2wh"
    type: "engine"
    release: "release-name"
    heritage: "Helm"
spec:
  ports:
  - port: 50000
    name: main
    targetPort: 50000
    protocol: TCP
  - port: 9443
    name: wvha-rest
    targetPort: 9443
    protocol: TCP
  clusterIP: None
  selector:
    app: release-na-ibm-db2warehouse
    component: "db2wh"
    type: "engine"
---
# Source: ibm-db2warehouse/templates/db2u-ldap-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-na-ibm-db2warehouse-db2u-ldap
  labels:
    app: release-na-ibm-db2warehouse
    release: "release-name"
    component: "ldap"
    heritage: "Helm"
    chart: "ibm-db2warehouse"
    icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
    icpdsupport/app: release-na-ibm-db2warehouse
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: release-na-ibm-db2warehouse
      chart: "ibm-db2warehouse"
      release: "release-name"
      component: "ldap"
      heritage: "Helm"
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: "ibm-db2warehouse"
        release: "release-name"
        component: "ldap"
        heritage: "Helm"
        icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:        
        productName: "Db2 Warehouse"
        productID: "Community"
        productVersion: "11.5.4.0"
    spec:
      volumes:
      - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        persistentVolumeClaim:
          claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
      - name: ldap-secret
        secret:
          secretName: release-na-ibm-db2warehouse-db2u-ldap
          defaultMode: 292
      - name: bluadmin-secret
        secret:
          secretName: release-na-ibm-db2warehouse-db2u-ldap-bluadmin
          defaultMode: 292      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
          #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
          #
          #If you specify multiple matchExpressions associated with nodeSelectorTerms,
          #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
          #
          #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                  - amd64              
            
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      serviceAccount: db2u
      initContainers:
      - name: init-ldapdb-dir
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        command:
          - "/bin/sh"
          - "-ecx"
          - |
            /tools/post-install/db2u_ready.sh \
                --replicas 1 \
                --template release-na-ibm-db2warehouse \
                --namespace default \
                --dbType db2wh
            DETERMINATION_FILE=/mnt/blumeta0/nodeslist
            CAT_NODE=$(head -1 $DETERMINATION_FILE)
            kubectl exec -it -n default ${CAT_NODE?} -- bash -c \
                "sudo mkdir -p /mnt/blumeta0/ldap_db /mnt/blumeta0/ldap_rootCA \
                && sudo chown -c -R 55:55 /mnt/blumeta0/ldap_db /mnt/blumeta0/ldap_rootCA \
                && sudo chmod -c -R 640 /mnt/blumeta0/ldap_db \
                && sudo chmod -c 700 /mnt/blumeta0/ldap_db"
            exit $?
        volumeMounts:
          - mountPath: /mnt/blumeta0
            name: release-na-ibm-db2warehouse-db2u-sqllib-shared
      containers:
      - name: ldap
        image: "icr.io/obs/hdm/db2u/db2u.auxiliary.auth:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 55
          capabilities:
            add:
            - CHOWN
            - NET_BIND_SERVICE
            - DAC_OVERRIDE
            - SETGID
            - SETUID
            - KILL
            drop:
            - ALL
        readinessProbe:
          timeoutSeconds: 3
          initialDelaySeconds: 10
          exec:
            command:
            - "/bin/sh"
            - "-c"
            - "test -f /opt/ibm/scripts/.ldap_initialized"
          periodSeconds: 30
          successThreshold: 1
          failureThreshold: 5
        livenessProbe:
          timeoutSeconds: 10
          initialDelaySeconds: 20
          exec:
            command:
            - "/bin/sh"
            - "-c"
            - "test -f /opt/ibm/scripts/.ldap_initialized"
          periodSeconds: 30
          successThreshold: 1
          failureThreshold: 5
        resources:
          requests:
            cpu: "100m"
            memory: "512Mi"
          limits:
            cpu: "200m"
            memory: "2Gi"
        env:
        - name: INSTANCE_NAME
          value: "db2inst1"
        - name: SERVICE_NAME
          value: release-na-ibm-db2warehouse-db2u-ldap
        - name: LDAP_PWD
          valueFrom:
            secretKeyRef:
              name: release-na-ibm-db2warehouse-db2u-ldap
              key: password
        volumeMounts:
        - mountPath: /mnt/blumeta0
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - name: ldap-secret
          mountPath: "/secrets/bluldap.pwd"
          readOnly: true
        - name: bluadmin-secret
          mountPath: "/secrets/bluadmin.pwd"
          readOnly: true
---
# Source: ibm-db2warehouse/templates/db2u-tools-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-na-ibm-db2warehouse-db2u-tools
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    component: "db2wh"
    icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
    icpdsupport/app: release-na-ibm-db2warehouse
spec:
  replicas: 1
  selector:
    matchLabels:
      type: tools
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: "ibm-db2warehouse"
        release: "release-name"
        heritage: "Helm"
        component: "db2wh"
        api-database-status: "db2wh-api"
        type: tools
        icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:        
        productName: "Db2 Warehouse"
        productID: "Community"
        productVersion: "11.5.4.0"
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
          #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
          #
          #If you specify multiple matchExpressions associated with nodeSelectorTerms,
          #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
          #
          #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                  - amd64              
            
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      serviceAccount: db2u
      containers:
      - name: release-na-ibm-db2warehouse-db2u-tools
        image: 'icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64'
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        env:
        - name: SERVICE_NAME
          value: release-na-ibm-db2warehouse
        - name: DB2U_SERVICE_NAME
          value: release-na-ibm-db2warehouse-db2u
        volumeMounts:
        - mountPath: /mnt/blumeta0
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - mountPath: /mnt/blumeta0/configmap/hadr
          name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
        - mountPath: "/secrets/db2instancepwd"
          name: db2instance-secret
          readOnly: true
        - name: ldap-secret
          mountPath: "/secrets/bluldap.pwd"
          readOnly: true
      volumes:
      - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        persistentVolumeClaim:
          claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
      - name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
        configMap:
          name: release-na-ibm-db2warehouse-db2u-hadr-config
      - name: db2instance-secret
        secret:
          secretName: release-na-ibm-db2warehouse-db2u-instance
          defaultMode: 256
      - name: ldap-secret
        secret:
          secretName: release-na-ibm-db2warehouse-db2u-ldap
          defaultMode: 292
---
# Source: ibm-db2warehouse/templates/db2u-etcd-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-na-ibm-db2warehouse-etcd
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    component: "etcd"
    release: "release-name"
    heritage: "Helm"
    icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
    icpdsupport/app: release-na-ibm-db2warehouse 
spec:
  selector:
    matchLabels:
      app: release-na-ibm-db2warehouse
      release: release-name
      heritage: Helm
      component: "etcd"
  serviceName: release-na-ibm-db2warehouse-etcd
  replicas: 3
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: "ibm-db2warehouse"
        component: "etcd"
        release: "release-name"
        heritage: "Helm"
        icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:        
        productName: "Db2 Warehouse"
        productID: "Community"
        productVersion: "11.5.4.0"
    spec:      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
          #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
          #
          #If you specify multiple matchExpressions associated with nodeSelectorTerms,
          #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
          #
          #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                  - amd64                      
            
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      serviceAccount: db2u
      terminationGracePeriodSeconds: 0
      volumes:
      - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        persistentVolumeClaim:
          claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
      initContainers:
      - name: init-etcd-dir
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        command:
          - "/bin/sh"
          - "-ec"
          - |
            set -x
            /tools/post-install/db2u_ready.sh \
                --replicas 1 \
                --template release-na-ibm-db2warehouse \
                --namespace default \
                --dbType db2wh
            DETERMINATION_FILE=/persistence/nodeslist
            CAT_NODE=$(head -1 $DETERMINATION_FILE)
            kubectl exec -it -n default ${CAT_NODE?} -- bash -c \
                "sudo mkdir -p /mnt/blumeta0/etcd \
                && sudo chmod -c -R 777 /mnt/blumeta0/etcd"
            exit $?
        volumeMounts:
        - mountPath: /persistence
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
      containers:
      - name: release-na-ibm-db2warehouse-etcd
        image: 'icr.io/obs/hdm/db2u/etcd:3.3.10-56-x86_64'
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 1001
          capabilities:
            drop:
            - ALL
        livenessProbe:
          tcpSocket:
            port: 2379
          timeoutSeconds: 10
          initialDelaySeconds: 20
          periodSeconds: 30
          successThreshold: 1
          failureThreshold: 5
        ports:
        - containerPort: 2380
          name: peer
        - containerPort: 2379
          name: client
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
        env:
        - name: INITIAL_CLUSTER_SIZE
          value: "3"
        - name: SET_NAME
          value: release-na-ibm-db2warehouse-etcd
        volumeMounts:
        - mountPath: /persistence
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        lifecycle:
          preStop:
            exec:
              command:
                - "/bin/sh"
                - "-ec"
                - |
                  EPS=""
                  for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                      EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
                  done
                  HOSTNAME=$(hostname)
                  member_hash() {
                      etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1
                  }
                  SET_ID=${HOSTNAME##*[^0-9]}
                  if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
                      echo "Removing ${HOSTNAME} from etcd cluster"
                      ETCDCTL_ENDPOINT=${EPS} etcdctl member remove $(member_hash)
                      if [ $? -eq 0 ]; then
                          # Remove everything otherwise the cluster will no longer scale-up
                          rm -rf /var/run/etcd/*
                      fi
                  fi
        command:
          - "/bin/sh"
          - "-ec"
          - |
            set -x
            HOSTNAME=$(hostname)
            if [ ! -d "/persistence/etcd/${HOSTNAME}" ]; then
                mkdir -p /persistence/etcd/${HOSTNAME}
            fi
            ln -sf /persistence/etcd/${HOSTNAME} /var/run/etcd
            # store member id into PVC for later member replacement
            collect_member() {
                while ! etcdctl member list &>/dev/null; do sleep 1; done
                etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1 > /var/run/etcd/member_id
                exit 0
            }
            eps() {
                EPS=""
                for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                    EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
                done
                echo ${EPS}
            }
            member_hash() {
                etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1
            }
            # re-joining after failure?
            if [ -e /var/run/etcd/default.etcd ]; then
                echo "Re-joining etcd member"
                if [ ! -f /var/run/etcd/member_id ]; then
                    # prestop cleanup
                    EPS=""
                    for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                        EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
                    done
                    HOSTNAME=$(hostname)
                    member_hash() {
                        etcdctl member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d':' -f1 | cut -d'[' -f1
                    }

                    #Add in member
                    export ETCDCTL_ENDPOINT=$(eps)
                    # member already added?
                    MEMBER_HASH=$(member_hash)
                    if [ -n "${MEMBER_HASH}" ]; then
                        # the member hash exists but for some reason etcd failed
                        # as the datadir has not be created, we can remove the member
                        # and retrieve new hash
                        etcdctl member remove ${MEMBER_HASH}
                        if [ $? -eq 0 ]; then
                          # Remove everything otherwise the cluster will no longer scale-up
                          rm -rf /var/run/etcd/*
                        fi
                    fi
                    echo "Adding new member"
                    etcdctl member add ${HOSTNAME} http://${HOSTNAME}.${SET_NAME}:2380 | grep "^ETCD_" > /var/run/etcd/new_member_envs
                    if [ $? -ne 0 ]; then
                        echo "Exiting"
                        rm -f /var/run/etcd/new_member_envs
                        exit 1
                    fi
                    cat /var/run/etcd/new_member_envs
                    source /var/run/etcd/new_member_envs
                    collect_member &
                    exec etcd --name ${HOSTNAME} \
                        --listen-peer-urls http://0.0.0.0:2380 \
                        --listen-client-urls http://0.0.0.0:2379 \
                        --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                        --data-dir /var/run/etcd/default.etcd \
                        --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                        --initial-cluster ${ETCD_INITIAL_CLUSTER} \
                        --initial-cluster-state ${ETCD_INITIAL_CLUSTER_STATE}
                else
                #same logic as before for when using a pvc.. rejoin member
                member_id=$(cat /var/run/etcd/member_id)
                # re-join member
                ETCDCTL_ENDPOINT=$(eps) etcdctl member update ${member_id} http://${HOSTNAME}.${SET_NAME}:2380 | true
                exec etcd --name ${HOSTNAME} \
                    --listen-peer-urls http://0.0.0.0:2380 \
                    --listen-client-urls http://0.0.0.0:2379\
                    --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                    --data-dir /var/run/etcd/default.etcd
                fi
            fi
            # etcd-SET_ID
            SET_ID=${HOSTNAME##*[^0-9]}
            # adding a new member to existing cluster (assuming all initial pods are available)
            if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
                export ETCDCTL_ENDPOINT=$(eps)
                # member already added?
                MEMBER_HASH=$(member_hash)
                if [ -n "${MEMBER_HASH}" ]; then
                    # the member hash exists but for some reason etcd failed
                    # as the datadir has not be created, we can remove the member
                    # and retrieve new hash
                    etcdctl member remove ${MEMBER_HASH}
                fi
                echo "Adding new member"
                etcdctl member add ${HOSTNAME} http://${HOSTNAME}.${SET_NAME}:2380 | grep "^ETCD_" > /var/run/etcd/new_member_envs
                if [ $? -ne 0 ]; then
                    echo "Exiting"
                    rm -f /var/run/etcd/new_member_envs
                    exit 1
                fi
                cat /var/run/etcd/new_member_envs
                source /var/run/etcd/new_member_envs
                collect_member &
                exec etcd --name ${HOSTNAME} \
                    --listen-peer-urls http://0.0.0.0:2380 \
                    --listen-client-urls http://0.0.0.0:2379 \
                    --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                    --data-dir /var/run/etcd/default.etcd \
                    --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                    --initial-cluster ${ETCD_INITIAL_CLUSTER} \
                    --initial-cluster-state ${ETCD_INITIAL_CLUSTER_STATE}
            fi
            PEERS=""
            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                PEERS="${PEERS}${PEERS:+,}${SET_NAME}-${i}=http://${SET_NAME}-${i}.${SET_NAME}:2380"
            done
            collect_member &
            # join member
            exec etcd --name ${HOSTNAME} \
                --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                --listen-peer-urls http://0.0.0.0:2380 \
                --listen-client-urls http://0.0.0.0:2379 \
                --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                --initial-cluster-token etcd-cluster-1 \
                --initial-cluster ${PEERS} \
                --initial-cluster-state new \
                --data-dir /var/run/etcd/default.etcd
---
# Source: ibm-db2warehouse/templates/db2u-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-na-ibm-db2warehouse-db2u
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    component: "db2wh"
    type: "engine"
    icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
    icpdsupport/app: release-na-ibm-db2warehouse
spec:
  serviceName: "release-na-ibm-db2warehouse-db2u-internal"
  podManagementPolicy: "Parallel"
  replicas: 1
  selector:
    matchLabels:
      app: release-na-ibm-db2warehouse
      chart: "ibm-db2warehouse"
      release: "release-name"
      heritage: "Helm"
      component: "db2wh"
      type: "engine"
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: "ibm-db2warehouse"
        release: "release-name"
        heritage: "Helm"
        component: "db2wh"
        type: "engine"
        api-progress: "db2wh-api"
        icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:        
        productName: "Db2 Warehouse"
        productID: "Community"
        productVersion: "11.5.4.0"
    spec:      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
          #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
          #
          #If you specify multiple matchExpressions associated with nodeSelectorTerms,
          #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
          #
          #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                  - amd64                      
            
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      serviceAccount: db2u
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: init-db2
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        env:
          - name: MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: release-na-ibm-db2warehouse-db2u
                resource: limits.memory
        command: ['/bin/sh']        
        securityContext:
          privileged: true
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: true
          runAsNonRoot: false
          runAsUser: 0
          capabilities:
            add:
            - "SYS_RESOURCE"
            - "IPC_OWNER"
            - "SYS_NICE"
            drop:
            - "ALL"
        args: ["-cx", '/tools/pre-install/set_kernel_params.sh']
        volumeMounts:
        - mountPath: /host/proc
          name: proc
          readOnly: false
        - mountPath: /host/proc/sys
          name: sys
          readOnly: false
      - name: init-db2-node-cfg        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        command:
          - "/bin/sh"
          - "-ec"
          - |
            /tools/pre-install/update_nodeslist.sh
            lineNum="$(grep -n $POD_NAME /mnt/blumeta0/nodeslist | head -n 1 | cut -d: -f1)"
            if [ $lineNum -eq 1 ]; then
                kubectl label pod $POD_NAME name=dashmpp-head-0 -n $POD_NAMESPACE --overwrite
            else
                lineNum=$[$lineNum-1]
                kubectl label pod $POD_NAME name=dashmpp-data${lineNum}-0 -n $POD_NAMESPACE --overwrite
            fi
            kubectl wait --for=condition=complete job/release-na-ibm-db2warehouse-db2u-nodes-cfg-job -n default
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        volumeMounts:
        - mountPath: /mnt/blumeta0
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
      containers:
      - name: release-na-ibm-db2warehouse-db2u
        image: 'icr.io/obs/hdm/db2u/db2u:11.5.4.0-56-x86_64'
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: true
          runAsNonRoot: true
          runAsUser: 600
          capabilities:
            add:
            - "SYS_RESOURCE"
            - "IPC_OWNER"
            - "SYS_NICE"
            #Default capabilities, re-add them as it will be drop
            - "CHOWN"
            - "DAC_OVERRIDE"
            - "FSETID"
            - "FOWNER"
            - "SETGID"
            - "SETUID"
            - "SETFCAP"
            - "SETPCAP"
            - "NET_BIND_SERVICE"
            - "SYS_CHROOT"
            - "KILL"
            - "AUDIT_WRITE"
            drop:
            - "ALL"
        resources:
          limits:
            memory: 4.3Gi
            cpu: 2
          requests:
            memory: 4.3Gi
            cpu: 2
        tty: true
        resources:
          limits:
            memory: 4.3Gi
            cpu: 2
          requests:
            memory: 4.3Gi
            cpu: 2
        env:
        - name: MEMORY_LIMIT
          valueFrom:
            resourceFieldRef:
              resource: limits.memory
        - name: etcdoperator
          value: "true"
        - name: POD
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['name']
        - name: ETCD_ENDPOINTS
          value: "http://release-na-ibm-db2warehouse-etcd-0.release-na-ibm-db2warehouse-etcd:2379,http://release-na-ibm-db2warehouse-etcd-1.release-na-ibm-db2warehouse-etcd:2379,http://release-na-ibm-db2warehouse-etcd-2.release-na-ibm-db2warehouse-etcd:2379"
        - name: WV_RECOVERY
          value: partial
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - mountPath: /mnt/blumeta0
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - mountPath: /mnt/bludata0
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - mountPath: /mnt/blumeta0/configmap/db2u
          name: release-na-ibm-db2warehouse-db2u-config-volume
        - mountPath: /mnt/blumeta0/configmap/wv
          name: release-na-ibm-db2warehouse-db2u-wv-config-volume
        - mountPath: /mnt/blumeta0/configmap/hadr
          name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
        - mountPath: /run
          name: runvol
        - name: db2instance-secret
          mountPath: "/secrets/db2instancepwd"
          readOnly: true
        - mountPath: /db2u/license
          name: release-na-ibm-db2warehouse-db2u-lic-volume
          readOnly: false
        lifecycle:
          preStop:
            exec:
              command:
              - "/bin/bash"
              - "-cl"
              - |
                /usr/bin/stop
                sudo kill -CONT 1
        ports:
          - containerPort: 50000
            protocol: TCP
            name: db2-server
          - containerPort: 50001
            protocol: TCP
            name: db2-ssl-server
          # SSH port, Wolverine ports and Db2 FCM ports are only applicable to MPP deployments
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      - name: sys
        hostPath:
          path: /proc/sys
      - name: proc
        hostPath:
          path: /proc
      - name: runvol
        emptyDir:
          medium: Memory
      - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        persistentVolumeClaim:
          claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
      - name: release-na-ibm-db2warehouse-db2u-config-volume
        configMap:
          name: release-na-ibm-db2warehouse-db2u-config
          items:
          - key: db2u-var
            path: db2u-var
      - name: release-na-ibm-db2warehouse-db2u-lic-volume
        configMap:
          name: release-na-ibm-db2warehouse-db2u-lic
          items:
            - key: db2u-lic
              path: db2u-lic
      - name: release-na-ibm-db2warehouse-db2u-wv-config-volume
        configMap:
          name: release-na-ibm-db2warehouse-db2u-wv-config
      - name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
        configMap:
          name: release-na-ibm-db2warehouse-db2u-hadr-config
      - name: db2instance-secret
        secret:
          secretName: release-na-ibm-db2warehouse-db2u-instance
          defaultMode: 256
---
# Source: ibm-db2warehouse/templates/db2u-engn-update-job.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: "release-na-ibm-db2warehouse-db2u-engn-update-job"
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
    icpdsupport/app: release-na-ibm-db2warehouse
  annotations:    
    productName: "Db2 Warehouse"
    productID: "Community"
    productVersion: "11.5.4.0"
spec:
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: "ibm-db2warehouse"
        release: "release-name"
        heritage: "Helm"
        icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:        
        productName: "Db2 Warehouse"
        productID: "Community"
        productVersion: "11.5.4.0"
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
          #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
          #
          #If you specify multiple matchExpressions associated with nodeSelectorTerms,
          #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
          #
          #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                  - amd64      
            
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      initContainers:
# -- 1st init container to excute condition ready probe such that we proceed only after all PODs
# -- have reached the end of ENTRYPOINT successfully
      - name: condition-ready
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        command: ['/bin/sh']
        args: ['-cx', '/tools/post-install/db2u_ready.sh --replicas 1 --template release-na-ibm-db2warehouse --namespace default --dbType db2wh']
# -- 2nd init container to check Db2 engn VRMF changes
      - name: detect-vrmf-change
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        command:
          - "/bin/sh"
          - "-ec"
          - |
            DETERMINATION_FILE=/mnt/blumeta0/nodeslist
            CAT_NODE=$(head -1 $DETERMINATION_FILE)
            # After INSTDB job completes, Db2 instance home is persisted on disk. Which is a
            # prerequisite for the VRMF detection code, since it depends on ~/sqllib/.instuse file.
            kubectl wait --for=condition=complete job/release-na-ibm-db2warehouse-db2u-sqllib-shared-job -n default
            kubectl exec -it -n default ${CAT_NODE?} -- bash -c "sudo /db2u/scripts/detect_db2_vrmf_change.sh -file"
        volumeMounts:
        - mountPath: /mnt/blumeta0
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
      containers:
# -- main container for triggering Db2 engine update/upgrade logic
      - name: engn-update
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL
        command:
          - "/bin/sh"
          - "-c"
          - |
            DETERMINATION_FILE=/mnt/blumeta0/nodeslist
            CAT_NODE=$(head -1 $DETERMINATION_FILE)
            cmd=""
            updt_upgrd_opt="-all"
            hadr_enabled="false"
            RC=0

            kubectl exec -it -n default ${CAT_NODE?} -- bash -c '[[ -f /mnt/blumeta0/vrmf/change.out ]] || exit 0; exit $(cat /mnt/blumeta0/vrmf/change.out)' 2>/dev/null
            vrmf_chk=$?
            echo "VRMF check status bit: ${vrmf_chk}"

            # If HADR is enabled dont run the DB update/upgrade scripts. This will be handled
            # by external mechanics to work around rolling updates.
            kubectl exec -it -n default ${CAT_NODE?} -- bash -c 'grep -qE "^HADR_ENABLED.*true" /mnt/blumeta0/configmap/hadr/*' 2>/dev/null
            [[ $? -eq 0 ]] && hadr_enabled="true"
            [[ "${hadr_enabled}" == "true" ]] && updt_upgrd_opt="-inst"

            # Check VRMF change bit and execute Db2 update or upgrade process
            if [[ $vrmf_chk -ne 0 ]]; then
                if [[ $vrmf_chk -eq 1 ]]; then
                    echo "Running the Db2 engine update script ..."
                    cmd="su - db2inst1 -c '/db2u/scripts/db2u_update.sh ${updt_upgrd_opt}'"
                elif [[ $vrmf_chk -eq 2 ]]; then
                    echo "Running the Db2 engine upgrade script ..."
                    cmd="su - db2inst1 -c '/db2u/scripts/db2u_upgrade.sh ${updt_upgrd_opt}'"
                fi
                [[ -n "$cmd" ]] && kubectl exec -it -n default ${CAT_NODE?} -- bash -c "$cmd"
                RC=$?
                [[ $RC -ne 0 ]] && exit $RC

                # If HADR is enabled, dont start Woliverine HA
                [[ "${hadr_enabled}" == "true" ]] && exit $RC

                # For all other Db2 engine update/upgrade scenarios, start Woliverine HA on all Db2U PODs now
                echo "Starting Wolverine HA ..."
                cmd="source /db2u/scripts/include/common_functions.sh && start_wvha_allnodes"
                kubectl exec -it -n default ${CAT_NODE?} -- bash -c "$cmd"
                RC=$?
            fi
            exit $RC        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        volumeMounts:
        - mountPath: /mnt/blumeta0
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - mountPath: /mnt/blumeta0/configmap/hadr
          name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
      restartPolicy: Never
      serviceAccount: db2u
      volumes:
      - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        persistentVolumeClaim:
          claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
      - name: release-na-ibm-db2warehouse-db2u-hadr-config-volume
        configMap:
          name: release-na-ibm-db2warehouse-db2u-hadr-config
---
# Source: ibm-db2warehouse/templates/db2u-nodes-cfg-job.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: "release-na-ibm-db2warehouse-db2u-nodes-cfg-job"
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
    icpdsupport/app: release-na-ibm-db2warehouse
  annotations:    
    productName: "Db2 Warehouse"
    productID: "Community"
    productVersion: "11.5.4.0"
spec:
  backoffLimit: 30
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: "ibm-db2warehouse"
        release: "release-name"
        heritage: "Helm"
        icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:        
        productName: "Db2 Warehouse"
        productID: "Community"
        productVersion: "11.5.4.0"
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false      
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
          #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
          #
          #If you specify multiple matchExpressions associated with nodeSelectorTerms,
          #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
          #
          #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                  - amd64      
      
      initContainers:
      - name: db2u-sqllib-shared-job
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        command: ['/bin/sh']
        args: ["-cx", 'kubectl wait --for=condition=complete job/release-na-ibm-db2warehouse-db2u-sqllib-shared-job -n default']

      containers:
      - name: db2u-db2nodes-cfg-init
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL
        command: ['/bin/sh']
        args: ['-cx', '/tools/pre-install/generate_mln_distribution.sh -mlns 1 -replicas 1']        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        volumeMounts:
        - mountPath: /mnt/blumeta0
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - mountPath: /mnt/blumeta0/configmap/db2u
          name: release-na-ibm-db2warehouse-db2u-config-volume
      restartPolicy: Never
      serviceAccount: db2u
      volumes:
      - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        persistentVolumeClaim:
          claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
      - name: release-na-ibm-db2warehouse-db2u-config-volume
        configMap:
          name: release-na-ibm-db2warehouse-db2u-config
          items:
          - key: db2u-var
            path: db2u-var
---
# Source: ibm-db2warehouse/templates/db2u-restore-morph-job.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: "release-na-ibm-db2warehouse-db2u-restore-morph-job"
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
    icpdsupport/app: release-na-ibm-db2warehouse
  annotations:    
    productName: "Db2 Warehouse"
    productID: "Community"
    productVersion: "11.5.4.0"
spec:
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: "ibm-db2warehouse"
        release: "release-name"
        heritage: "Helm"
        icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:        
        productName: "Db2 Warehouse"
        productID: "Community"
        productVersion: "11.5.4.0"
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
          #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
          #
          #If you specify multiple matchExpressions associated with nodeSelectorTerms,
          #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
          #
          #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                  - amd64      
            
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      initContainers:
      - name: condition-ready
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        command: ['/bin/sh']
        args: ['-cx', '/tools/post-install/db2u_ready.sh --replicas 1 --template release-na-ibm-db2warehouse --namespace default --dbType db2wh']
      containers:
      - name: restore-morph
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL
        command:
          - "/bin/sh"
          - "-ec"
          - |
            DETERMINATION_FILE=/mnt/blumeta0/nodeslist
            CAT_NODE=$(head -1 $DETERMINATION_FILE)
            kubectl exec -it -n default ${CAT_NODE?} -- bash -c "su - db2inst1 -c \"/db2u/db2u_restore_morph.sh\""
            exit $?        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        volumeMounts:
        - mountPath: /mnt/blumeta0
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - mountPath: /mnt/blumeta0/configmap/db2u
          name: release-na-ibm-db2warehouse-db2u-config-volume
      restartPolicy: Never
      serviceAccount: db2u
      volumes:
      - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        persistentVolumeClaim:
          claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
      - name: release-na-ibm-db2warehouse-db2u-config-volume
        configMap:
          name: release-na-ibm-db2warehouse-db2u-config
          items:
          - key: db2u-var
            path: db2u-var
---
# Source: ibm-db2warehouse/templates/db2u-shared-sqllib-job.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: "release-na-ibm-db2warehouse-db2u-sqllib-shared-job"
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
    icpdsupport/app: release-na-ibm-db2warehouse
  annotations:    
    productName: "Db2 Warehouse"
    productID: "Community"
    productVersion: "11.5.4.0"
spec:
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: "ibm-db2warehouse"
        release: "release-name"
        heritage: "Helm"
        icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:        
        productName: "Db2 Warehouse"
        productID: "Community"
        productVersion: "11.5.4.0"
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
          #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
          #
          #If you specify multiple matchExpressions associated with nodeSelectorTerms,
          #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
          #
          #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                  - amd64      
            
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      containers:
      - name: release-na-ibm-db2warehouse-db2-init
        image: "icr.io/obs/hdm/db2u/db2u.instdb:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: true
          runAsNonRoot: true
          runAsUser: 700
          capabilities:
            add:
            #Default capabilities, re-add some back
            - "FOWNER"
            # Glusterfs support
            - "SETGID"
            - "SETUID"
            - "CHOWN"
            - "DAC_OVERRIDE"
            drop:
            - "ALL"
        command: ['/bin/sh']
        args: ['-cx', '/Db2wh_preinit/instdb_entrypoint.sh']        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "200Mi"
        env:
        - name: SERVICENAME
          value: ""
        volumeMounts:
        - mountPath: /mnt/blumeta0
          name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        - mountPath: /mnt/blumeta0/configmap/db2u
          name: release-na-ibm-db2warehouse-db2u-config-volume
      restartPolicy: Never
      serviceAccount: db2u
      volumes:
      - name: release-na-ibm-db2warehouse-db2u-sqllib-shared
        persistentVolumeClaim:
          claimName: release-na-ibm-db2warehouse-db2u-sqllib-shared
      - name: release-na-ibm-db2warehouse-db2u-config-volume
        configMap:
          name: release-na-ibm-db2warehouse-db2u-config
          items:
          - key: db2u-var
            path: db2u-var
---
# Source: ibm-db2warehouse/templates/tests/connect-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "test-connection"
  annotations:
    "helm.sh/hook": test-success
  labels:
    app: release-na-ibm-db2warehouse-test
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    component: "db2wh"
spec:
  restartPolicy: Never  
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
      #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
      #
      #If you specify multiple matchExpressions associated with nodeSelectorTerms,
      #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
      #
      #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
        nodeSelectorTerms:
        - matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
              - amd64  
    
  hostNetwork: false
  hostPID: false
  hostIPC: false
  securityContext:
    runAsNonRoot: true
  containers:
  - name: release-na-ibm-db2warehouse-db2u-test
    image: 'icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64'    
    securityContext:
      privileged: false
      readOnlyRootFilesystem: false
      allowPrivilegeEscalation: false
      runAsNonRoot: true
      runAsUser: 500
      capabilities:
        drop:
        - ALL
    resources:
      requests:
        cpu: "100m"
        memory: "100Mi"
      limits:
        cpu: "200m"
        memory: "500Mi"
    command: ['/bin/sh']
    args:
      - -cx
      - |
        pods=$(kubectl get pods --selector=app=release-na-ibm-db2warehouse --selector=type=engine --no-headers | awk '{print($1)}')
        for i in ${pods//|/ }; do
              kubectl exec -ti ${i} -- su - db2inst1 -c "db2 connect to bludb"
              rc=$?
              if [[ ${rc} -ne 0 ]]; then
                  exit ${rc}
              fi
        done
        exit $?
---
# Source: ibm-db2warehouse/templates/db2u-preupgrade-hook-v3.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: "release-na-ibm-db2warehouse-db2u-preupgrade-hook-v3"
  labels:
    app: release-na-ibm-db2warehouse
    chart: "ibm-db2warehouse"
    release: "release-name"
    heritage: "Helm"
    icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
    icpdsupport/app: release-na-ibm-db2warehouse
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded    
    productName: "Db2 Warehouse"
    productID: "Community"
    productVersion: "11.5.4.0"
spec:
  template:
    metadata:
      labels:
        app: release-na-ibm-db2warehouse
        chart: "ibm-db2warehouse"
        release: "release-name"
        heritage: "Helm"
        icpdsupport/serviceInstanceId: "release-na-ibm-db2warehouse"
        icpdsupport/app: release-na-ibm-db2warehouse
      annotations:        
        productName: "Db2 Warehouse"
        productID: "Community"
        productVersion: "11.5.4.0"
    spec:
      hostNetwork: false
      hostPID: false
      hostIPC: false      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          #If you specify multiple nodeSelectorTerms associated with nodeAffinity types,
          #then the pod can be scheduled onto a node if one of the nodeSelectorTerms is satisfied.
          #
          #If you specify multiple matchExpressions associated with nodeSelectorTerms,
          #then the pod can be scheduled onto a node only if all matchExpressions can be satisfied.
          #
          #valid operators: In, NotIn, Exists, DoesNotExist, Gt, Lt
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                  - amd64      
            
      hostNetwork: false
      hostPID: false
      hostIPC: false
      securityContext:
        runAsNonRoot: true
      containers:
      - name: stop-slapd
        image: "icr.io/obs/hdm/db2u/db2u.tools:11.5.4.0-56-x86_64"
        imagePullPolicy: IfNotPresent        
        securityContext:
          privileged: false
          readOnlyRootFilesystem: false
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 500
          capabilities:
            drop:
            - ALL        
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "200m"
            memory: "100Mi"
        command:
          - "/bin/sh"
          - "-ecx"
          - |
            ldap_pods=$(kubectl get -n default po -l component=ldap,release=release-name -o jsonpath='{.items[*].metadata.name}')
            for pod in ${ldap_pods}; do
                  kubectl exec -it -n default ${pod} -- bash -c "which supervisorctl && supervisorctl stop slapd || exit 0"
            done
      restartPolicy: Never
      serviceAccount: db2u

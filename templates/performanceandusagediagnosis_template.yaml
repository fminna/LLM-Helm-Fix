---
# Source: performanceandusagediagnosis/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
  name: release-name-grafana
  namespace: default
---
# Source: performanceandusagediagnosis/charts/grafana/templates/tests/test-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
  name: release-name-grafana-test
  namespace: default
---
# Source: performanceandusagediagnosis/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-4.18.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.6.0"
  name: release-name-kube-state-metrics
  namespace: default
imagePullSecrets:
  []
---
# Source: performanceandusagediagnosis/charts/prometheus-node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.23.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.6.1"
---
# Source: performanceandusagediagnosis/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  admin-user: "YWRtaW4="
  admin-password: "ZW9DazI3TE13MFB4bGQwWk5XZ01FQjVNaHhLanJ5Ykg4d3hzc1Z2bQ=="
  ldap-toml: ""
---
# Source: performanceandusagediagnosis/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
data:
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    [server]
    domain =
---
# Source: performanceandusagediagnosis/charts/grafana/templates/tests/test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-grafana-test
  namespace: default
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
data:
  run.sh: |-
    @test "Test Health" {
      url="http://release-name-grafana/api/health"

      code=$(wget --server-response --spider --timeout 90 --tries 10 ${url} 2>&1 | awk '/^  HTTP/{print $2}')
      [ "$code" == "200" ]
    }
---
# Source: performanceandusagediagnosis/templates/server/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-performanceandusagediagnosis-server
  namespace: "default"
  labels:
    helm.sh/chart: performanceandusagediagnosis-1.0.0
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: server
    isMainInterface: "yes"
    tier: external
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
data:
  alerting_rules.yml: |
    groups:
    - name: Instances
      rules:
      - alert: InstanceDown
        annotations:
          description: '{{ $labels.instance }} of job {{ $labels.job }} has been down
            for more than 5 minutes.'
          summary: Instance {{ $labels.instance }} down
        expr: up == 0
        for: 5m
        labels:
          severity: page
      - alert: PrometheusAllTargetsMissing
        annotations:
          description: |-
            A Prometheus job does not have living target anymore.
              VALUE = {{ $value }}
              LABELS = {{ $labels }}
          summary: Prometheus all targets missing (instance {{ $labels.instance }})
        expr: sum by (job) (up) == 0
        for: 0m
        labels:
          severity: critical
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 1m
      scrape_timeout: 10s
    remote_write:
    - url: http://release-name-performanceandusagediagnosis-prometheusesadapter:8000/write
    remote_read:
    - url: http://release-name-performanceandusagediagnosis-prometheusesadapter:8000/read
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts#

    scrape_configs:
    - job_name: prometheus
      static_configs:
        - targets:
          - localhost:9090

    - job_name: Httpsd-Targets
      http_sd_configs:
        - url: http://release-name-performanceandusagediagnosis-targetapi.default.svc.cluster.local:5000
 
    - job_name: Node_exporter
      static_configs:
        - targets:
          - release-name-prometheus-node-exporter.default.svc.cluster.local:9100

    - job_name: kube-state-metrics
      static_configs:
        - targets:
          - release-name-kube-state-metrics.default.svc.cluster.local:8080
  recording_rules.yml: |
    {}
  rules: |
    {}
---
# Source: performanceandusagediagnosis/templates/targetapi/pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
#  name: targetapi-persistent-volume
  name: release-name-performanceandusagediagnosis-targetapi-volume
  labels:
#    type: local
#    app: targetapi
    helm.sh/chart: performanceandusagediagnosis-1.0.0
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: targetapi
    isMainInterface: "no"
    tier: internal
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  volumeMode: Filesystem
  capacity:
    storage: 256Mi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: "/targets"
  persistentVolumeReclaimPolicy: Retain
---
# Source: performanceandusagediagnosis/templates/server/pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: release-name-performanceandusagediagnosis-server
  namespace: "default"
  labels:
    helm.sh/chart: performanceandusagediagnosis-1.0.0
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: server
    isMainInterface: "yes"
    tier: external
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
  - ReadWriteMany
  storageClassName: ""
  volumeName: storage-volume
  resources:
    requests:
      storage: 3Gi
---
# Source: performanceandusagediagnosis/templates/targetapi/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
#  name: targetapi-persistent-volume-claim
  name: release-name-performanceandusagediagnosis-targetapi-volume-claim
  labels:
#    app: targetapi
    helm.sh/chart: performanceandusagediagnosis-1.0.0
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: targetapi
    isMainInterface: "no"
    tier: internal
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 256Mi
  selector:
    matchLabels:
      pv: targetapi-persistent-volume
---
# Source: performanceandusagediagnosis/charts/grafana/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
  name: release-name-grafana-clusterrole
rules: []
---
# Source: performanceandusagediagnosis/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-4.18.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.6.0"
  name: release-name-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: performanceandusagediagnosis/charts/grafana/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-grafana-clusterrolebinding
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: release-name-grafana
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: performanceandusagediagnosis/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-4.18.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.6.0"
  name: release-name-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: release-name-kube-state-metrics
  namespace: default
---
# Source: performanceandusagediagnosis/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:      ['extensions']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [release-name-grafana]
---
# Source: performanceandusagediagnosis/charts/grafana/templates/tests/test-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-grafana-test
  namespace: default
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:      ['policy']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [release-name-grafana-test]
---
# Source: performanceandusagediagnosis/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-grafana
subjects:
- kind: ServiceAccount
  name: release-name-grafana
  namespace: default
---
# Source: performanceandusagediagnosis/charts/grafana/templates/tests/test-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-grafana-test
  namespace: default
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-grafana-test
subjects:
- kind: ServiceAccount
  name: release-name-grafana-test
  namespace: default
---
# Source: performanceandusagediagnosis/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
---
# Source: performanceandusagediagnosis/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-4.18.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.6.0"
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
---
# Source: performanceandusagediagnosis/charts/prometheus-node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.23.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.6.1"
  annotations:
    prometheus.io/scrape: "true"
spec:
  type: NodePort
  ports:
    - port: 9100
      nodePort: 31012
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: release-name
---
# Source: performanceandusagediagnosis/templates/prometheusesadapter/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-performanceandusagediagnosis-prometheusesadapter
  namespace: "default"
  labels:
    helm.sh/chart: performanceandusagediagnosis-1.0.0
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: prometheusesadapter
    isMainInterface: "no"
    tier: internal
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: pea8000
      port: 8000
      targetPort: 8000
      protocol: TCP
    - name: pea8001
      port: 8001
      targetPort: 8001
      protocol: TCP
  selector:
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: prometheusesadapter
    isMainInterface: "no"
    tier: internal
---
# Source: performanceandusagediagnosis/templates/server/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-performanceandusagediagnosis-server
  namespace: "default"
  labels:
    helm.sh/chart: performanceandusagediagnosis-1.0.0
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: server
    isMainInterface: "yes"
    tier: external
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: NodePort
  ports:
    - name: server
      port: 9090
      targetPort: 9090
      protocol: TCP
  selector:
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: server
    isMainInterface: "yes"
    tier: external
---
# Source: performanceandusagediagnosis/templates/targetapi/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-performanceandusagediagnosis-targetapi
  namespace: "default"
  labels:
    helm.sh/chart: performanceandusagediagnosis-1.0.0
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: targetapi
    isMainInterface: "no"
    tier: internal
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - name: targetapi
      port: 5000
      targetPort: 5000
      protocol: TCP
  selector:
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: targetapi
    isMainInterface: "no"
    tier: internal
---
# Source: performanceandusagediagnosis/charts/prometheus-node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-prometheus-node-exporter
  namespace: default
  labels:
    helm.sh/chart: prometheus-node-exporter-4.23.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.6.1"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/instance: release-name
  revisionHistoryLimit: 10
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        helm.sh/chart: prometheus-node-exporter-4.23.2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: prometheus-node-exporter
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "1.6.1"
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: release-name-prometheus-node-exporter
      containers:
        - name: node-exporter
          image: quay.io/prometheus/node-exporter:v1.6.1
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --web.listen-address=[$(HOST_IP)]:9100
          securityContext:
            readOnlyRootFilesystem: true
          env:
            - name: HOST_IP
              value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: 9100
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
      hostNetwork: true
      hostPID: true
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
---
# Source: performanceandusagediagnosis/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: release-name
      annotations:
        checksum/config: fdc8eb03b7d2abd2c9aadb556c739caa299639cff22fc634cedff97b7b3357cd
        checksum/dashboards-json-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6
        checksum/sc-dashboard-provider-config: 7eb70257593da06f682a3ddda54a9d260d4fc514f645237f5ca74b08f8da61a6
        checksum/secret: 563d05904eb238b42beb124555aabb2d488e60fd4596067d751d67386f70747c
    spec:
      
      serviceAccountName: release-name-grafana
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsUser: 472
      enableServiceLinks: true
      containers:
        - name: grafana
          image: "grafana/grafana:9.1.1"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
          ports:
            - name: grafana
              containerPort: 3000
              protocol: TCP
          env:
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: release-name-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-grafana
                  key: admin-password
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: release-name-grafana
        - name: storage
          emptyDir: {}
---
# Source: performanceandusagediagnosis/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kube-state-metrics
  namespace: default
  labels:    
    helm.sh/chart: kube-state-metrics-4.18.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.6.0"
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: release-name
  replicas: 1
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-4.18.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.6.0"
    spec:
      hostNetwork: false
      serviceAccountName: release-name-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsUser: 65534
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        imagePullPolicy: IfNotPresent
        image: "registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.6.0"
        ports:
        - containerPort: 8080
          name: "http"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
---
# Source: performanceandusagediagnosis/templates/prometheusesadapter/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-performanceandusagediagnosis-prometheusesadapter
  namespace: "default"
  labels:
    helm.sh/chart: performanceandusagediagnosis-1.0.0
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: prometheusesadapter
    isMainInterface: "no"
    tier: internal
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: performanceandusagediagnosis
      app.kubernetes.io/instance: release-name
      enabler: performanceandusagediagnosis
      app.kubernetes.io/component: prometheusesadapter
      isMainInterface: "no"
      tier: internal
  template:
    metadata:
      labels:
        helm.sh/chart: performanceandusagediagnosis-1.0.0
        app.kubernetes.io/name: performanceandusagediagnosis
        app.kubernetes.io/instance: release-name
        enabler: performanceandusagediagnosis
        app.kubernetes.io/component: prometheusesadapter
        isMainInterface: "no"
        tier: internal
        app.kubernetes.io/version: "1.0.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext:
        {}
      containers:
        - name: prometheusesadapter
          securityContext:
            {}
          image: "gitlab.assist-iot.eu:5050/enablers-registry/public/pud/prometheus-es-adapter:latest"
          imagePullPolicy: IfNotPresent
          ports:
            - name: pea8000
              containerPort: 8000
              protocol: TCP
            - name: pea8001
              containerPort: 8001
              protocol: TCP
          resources:
            {}
          env:
            - name: ES_URL
              value: "http://10.97.108.30:9200"
            - name: DEBUG
              value: "true"
            - name: ES_INDEX_DAILY
              value: "true"
            - name: ES_INDEX_MAX_DOCS
              value: "2000"
            - name: ES_INDEX_REPLICAS
              value: "0"
            - name: ES_INDEX_SHARDS
              value: "1"
            #- name: ES_URL
            #  value: http://elasticsearch:9200
---
# Source: performanceandusagediagnosis/templates/server/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-performanceandusagediagnosis-server
  namespace: "default"
  labels:
    helm.sh/chart: performanceandusagediagnosis-1.0.0
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: server
    isMainInterface: "yes"
    tier: external
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: performanceandusagediagnosis
      app.kubernetes.io/instance: release-name
      enabler: performanceandusagediagnosis
      app.kubernetes.io/component: server
      isMainInterface: "yes"
      tier: external
  template:
    metadata:
      labels:
        helm.sh/chart: performanceandusagediagnosis-1.0.0
        app.kubernetes.io/name: performanceandusagediagnosis
        app.kubernetes.io/instance: release-name
        enabler: performanceandusagediagnosis
        app.kubernetes.io/component: server
        isMainInterface: "yes"
        tier: external
        app.kubernetes.io/version: "1.0.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext:
        {}
      volumes:
        - name: config-volume
          configMap:
            name: release-name-performanceandusagediagnosis-server
        ####################################
        - name: storage-volume
          emptyDir: {}
      containers:
        #- name: configmap-reload
        - name: performanceandusagediagnosis-server-configmap-reload
          image: "jimmidyson/configmap-reload:v0.5.0"
          imagePullPolicy: IfNotPresent
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://127.0.0.1:9090/-/reload
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true
        #- name: server
        - name: performanceandusagediagnosis-server
          securityContext:
            {}
          image: "quay.io/prometheus/prometheus:v2.36.2"
          imagePullPolicy: IfNotPresent
          args:
            - --storage.tsdb.retention.time=15d
            - --storage.tsdb.path=/data
            - --config.file=/etc/config/prometheus.yml
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
          ports:
            - name: server
              containerPort: 9090
              protocol: TCP
          resources:
            {}
          env:
            - name: EXAMPLE_ENV_VAR
              value: "exampleValue"
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
---
# Source: performanceandusagediagnosis/templates/targetapi/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-performanceandusagediagnosis-targetapi
  namespace: "default"
  labels:
    helm.sh/chart: performanceandusagediagnosis-1.0.0
    app.kubernetes.io/name: performanceandusagediagnosis
    app.kubernetes.io/instance: release-name
    enabler: performanceandusagediagnosis
    app.kubernetes.io/component: targetapi
    isMainInterface: "no"
    tier: internal
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: performanceandusagediagnosis
      app.kubernetes.io/instance: release-name
      enabler: performanceandusagediagnosis
      app.kubernetes.io/component: targetapi
      isMainInterface: "no"
      tier: internal
  template:
    metadata:
      labels:
        helm.sh/chart: performanceandusagediagnosis-1.0.0
        app.kubernetes.io/name: performanceandusagediagnosis
        app.kubernetes.io/instance: release-name
        enabler: performanceandusagediagnosis
        app.kubernetes.io/component: targetapi
        isMainInterface: "no"
        tier: internal
        app.kubernetes.io/version: "1.0.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext:
        {}
      containers:
        - name: targetapi
          securityContext:
            {}
          image: "gitlab.assist-iot.eu:5050/enablers-registry/public/pud/prom-target-api:latest"
          imagePullPolicy: IfNotPresent
          ports:
            - name: targetapi
              containerPort: 5000
              protocol: TCP
          resources:
            {}
          env:
            - name: TARGET_API_PORT
              value: "5000"
          volumeMounts:
          - mountPath: /targets
            name: targetapi-volume
      volumes:
      - name: targetapi-volume
        persistentVolumeClaim:
#            claimName: targetapi-persistent-volume-claim
            claimName: release-name-performanceandusagediagnosis-targetapi-volume-claim
---
# Source: performanceandusagediagnosis/charts/grafana/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: release-name-grafana-test
  labels:
    helm.sh/chart: grafana-6.36.2
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "9.1.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  namespace: default
spec:
  serviceAccountName: release-name-grafana-test
  containers:
    - name: release-name-test
      image: "bats/bats:v1.4.1"
      imagePullPolicy: "IfNotPresent"
      command: ["/opt/bats/bin/bats", "-t", "/tests/run.sh"]
      volumeMounts:
        - mountPath: /tests
          name: tests
          readOnly: true
  volumes:
  - name: tests
    configMap:
      name: release-name-grafana-test
  restartPolicy: Never

---
# Source: ct-single/charts/nats/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: release-name-nats
  namespace: default
  labels:
    helm.sh/chart: nats-0.17.5
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.8.4"
    app.kubernetes.io/managed-by: Helm
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nats
      app.kubernetes.io/instance: release-name
---
# Source: ct-single/charts/nats/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-nats
  namespace: default
  labels:
    helm.sh/chart: nats-0.17.5
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.8.4"
    app.kubernetes.io/managed-by: Helm
---
# Source: ct-single/charts/nats/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-nats-config
  namespace: default
  labels:
    helm.sh/chart: nats-0.17.5
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.8.4"
    app.kubernetes.io/managed-by: Helm
data:
  nats.conf: |
    # NATS Clients Port
    port: 4222

    # PID file shared with configuration reloader.
    pid_file: "/var/run/nats/nats.pid"

    ###############
    #             #
    # Monitoring  #
    #             #
    ###############
    http: 8222
    server_name:$POD_NAME
    lame_duck_grace_period: 10s
    lame_duck_duration: 30s
---
# Source: ct-single/charts/nats/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-nats
  namespace: default
  labels:
    helm.sh/chart: nats-0.17.5
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.8.4"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
  - name: client
    port: 4222
    appProtocol: tcp
  - name: cluster
    port: 6222
    appProtocol: tcp
  - name: monitor
    port: 8222
    appProtocol: http
  - name: metrics
    port: 7777
    appProtocol: http
  - name: leafnodes
    port: 7422
    appProtocol: tcp
  - name: gateways
    port: 7522
    appProtocol: tcp
---
# Source: ct-single/templates/admin-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-admin-service-lb
  labels:
    app: release-name-admin-service
  # annotations:
    # metallb.universe.tf/address-pool: default
    # metallb.universe.tf/allow-shared-ip: "ok"
spec:
  type: LoadBalancer
#   type: ClusterIP
  externalTrafficPolicy: Cluster
  selector:
    app: release-name-admin-service
  ports:
    - port: 443
      targetPort: 4001
      protocol: TCP
      name: web
    - port: 22
      targetPort: 3022
      protocol: TCP
      name: sftp
---
# Source: ct-single/templates/api-serivice.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-api
  labels:
    app: release-name-api
spec:
  type: ClusterIP
  selector:
    app: release-name-api
  ports:
    - port: 80
      targetPort: 4080
      protocol: TCP
      name: web
    - port: 22
      targetPort: 3022
      protocol: TCP
      name: sftp
---
# Source: ct-single/templates/loadbalancer.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-web-lb
  labels:
    app: release-name-web-lb
spec:
  type: LoadBalancer
  selector:
    app: release-name-web
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
      name: http
# ---
# apiVersion: v1
# kind: Service
# metadata:
#   name: ct-single-ftp-lb
#   labels:
#     app: calltelemetry-ftp
# spec:
#   type: LoadBalancer
#   selector:
#     app: calltelemetry-ftp
#   ports:
#     - port: 21
#       targetPort: 3021
#       protocol: TCP
#       name: ftp
---
# Source: ct-single/templates/sftp-LB-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-sftp-lb
  labels:
    app: release-name-sftp-lb
spec:
  type: LoadBalancer
  selector:
    app: release-name-admin-service
  ports:
    - port: 22
      targetPort: 3022
      protocol: TCP
      name: sftp
---
# Source: ct-single/templates/traceroute-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: traceroute
  labels:
    app: release-name-traceroute-service
  annotations:
    metallb.universe.tf/address-pool: default
    metallb.universe.tf/allow-shared-ip: "ok"
spec:
  type: ClusterIP
  selector:
    app: traceroute
  ports:
    - port: 4100
      targetPort: 4100
      protocol: TCP
      name: api
---
# Source: ct-single/charts/nats/templates/nats-box.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-nats-box
  namespace: default
  labels:
    app: release-name-nats-box
    chart: nats-0.17.5
spec:
  replicas: 1
  selector:
    matchLabels:
      app: release-name-nats-box
  template:
    metadata:
      labels:
        app: release-name-nats-box
    spec:
      volumes:
      containers:
      - name: nats-box
        image: natsio/nats-box:0.11.0
        imagePullPolicy: IfNotPresent
        resources:
          null
        env:
        - name: NATS_URL
          value: release-name-nats
        command:
        - "tail"
        - "-f"
        - "/dev/null"
        volumeMounts:
---
# Source: ct-single/templates/admin-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-admin-service
  labels:
    app: release-name-admin-service
    deployment: release-name-admin-service
spec:
  selector:
    matchLabels:
      app: release-name-admin-service
      deployment: release-name-admin-service
  replicas: 1
  strategy:
    # type: Recreate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: release-name-admin-service
        deployment: release-name-admin-service
    spec:
      tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 2
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 2
      terminationGracePeriodSeconds: 5
      # affinity:
      #     podAntiAffinity:
      #       preferredDuringSchedulingIgnoredDuringExecution:
      #       - weight: 100
      #         podAffinityTerm:
      #           labelSelector:
      #             matchExpressions:
      #             - key: app
      #               operator: In
      #               values:
      #               - release-name-primary-web
      #               - release-name-secondary-web
      #           topologyKey: "kubernetes.io/hostname"
      containers:
        - name: admin-service
          image: calltelemetry/web:0.8.1-rc51
          imagePullPolicy: 
          ports:
            - containerPort: 4000
          resources:
            requests:
              # memory: "512Mi"
              cpu: "100m"
            limits:
              # memory: "1024Mi"
              cpu: 4
          # livenessProbe:
          #   httpGet:
          #     path: /
          #     port: 8080
          #   initialDelaySeconds: 120
          env:
            - name: EXTERNAL_IP
              value: 192.168.123.205
            - name: DB_USER
              value: postgres
            - name: DB_PASSWORD
              value: calltelemetry
            - name: DB_NAME
              value: calltelemetry_db
            - name: DB_HOSTNAME
              value: pg-postgresql.default.svc.cluster.local
            - name: DB_PORT
              value: "5432"
            - name: CDR_ROOT_PATH
              value: /tmp
            - name: LOGGING_LEVEL
              value: debug
---
# Source: ct-single/templates/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-api
  labels:
    app: release-name-api
spec:
  selector:
    matchLabels:
      app: release-name-api
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 70%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: release-name-api
    spec:
      securityContext:
        runAsUser: 1001
      tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 2
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 2
      terminationGracePeriodSeconds: 5
      affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - release-name-api
                topologyKey: "kubernetes.io/hostname"
      containers:
        - name: primary-api
          image: calltelemetry/web:0.8.1-rc51
          imagePullPolicy: 
          ports:
            - containerPort: 4080
            - containerPort: 3022
          resources:
            requests:
              memory: "512Mi"
              cpu: "100m"
            limits:
              memory: "1024Mi"
              cpu: 4
          # livenessProbe:
          #   httpGet:
          #     path: /
          #     port: 8080
          #   initialDelaySeconds: 120
          env:
            - name: EXTERNAL_IP
              value: 192.168.123.205
            - name: DB_USER
              value: postgres
            - name: DB_PASSWORD
              value: calltelemetry
            - name: DB_NAME
              value: calltelemetry_db
            - name: DB_HOSTNAME
              value: pg-postgresql.default.svc.cluster.local
            - name: DB_PORT
              value: "5432"
            - name: CDR_ROOT_PATH
              value: /tmp
            - name: LOGGING_LEVEL
              value: debug
---
# Source: ct-single/templates/sftp-server.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-sftp-service
  labels:
    app: release-name-sftp-service
    deployment: release-name-sftp-service
spec:
  selector:
    matchLabels:
      app: release-name-sftp-service
      deployment: release-name-sftp-service
  replicas: 1
  strategy:
    # type: Recreate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: release-name-sftp-service
        deployment: release-name-sftp-service
    spec:
      tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 2
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 2
      terminationGracePeriodSeconds: 5
      # affinity:
      #     podAntiAffinity:
      #       preferredDuringSchedulingIgnoredDuringExecution:
      #       - weight: 100
      #         podAffinityTerm:
      #           labelSelector:
      #             matchExpressions:
      #             - key: app
      #               operator: In
      #               values:
      #               - release-name-primary-web
      #               - release-name-secondary-web
      #           topologyKey: "kubernetes.io/hostname"
      containers:
        - name: sftp-service
          image: calltelemetry/sftp:0.6.8
          imagePullPolicy: 
          ports:
            - containerPort: 4000
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
            limits:
            #   # memory: "1024Mi"
              cpu: 1
          # livenessProbe:
          #   httpGet:
          #     path: /
          #     port: 8080
          #   initialDelaySeconds: 120
          env:
            - name: NATS_HOSTNAME
              value: release-name-nats
---
# Source: ct-single/templates/traceroute-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-traceroute-service
  labels:
    app: release-name-traceroute-service
    deployment: release-name-traceroute-service
spec:
  selector:
    matchLabels:
      app: release-name-traceroute-service
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 70%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: release-name-traceroute-service
    spec:
      securityContext:
      # ICMP needs to run as root for security reasons.
        runAsUser: 0
        # runAsUser: 1001
      tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 2
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 2
      terminationGracePeriodSeconds: 5
      # affinity:
      #     podAntiAffinity:
      #       preferredDuringSchedulingIgnoredDuringExecution:
      #       - weight: 100
      #         podAffinityTerm:
      #           labelSelector:
      #             matchExpressions:
      #             - key: app
      #               operator: In
      #               values:
      #               - release-name-primary-web
      #               - release-name-secondary-web
      #           topologyKey: "kubernetes.io/hostname"
      containers:
        - name: traceroute-service
          image: calltelemetry/traceroute:0.7.1
          imagePullPolicy: 
          ports:
            - containerPort: 4100
          resources:
            requests:
              # memory: "512Mi"
              cpu: "128m"
            limits:
              # memory: "1024Mi"
              cpu: 1
          env:
            - name: NATS_HOSTNAME
              value: release-name-nats
---
# Source: ct-single/charts/nats/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-nats
  namespace: default
  labels:
    helm.sh/chart: nats-0.17.5
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.8.4"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: nats
      app.kubernetes.io/instance: release-name
  replicas: 1
  serviceName: release-name-nats

  podManagementPolicy: Parallel

  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "7777"
        prometheus.io/scrape: "true"
        checksum/config: 4947bf3ca3be94ae6ea03d1f6dc26e603b7e4eea36502b8fb5306b6a2f60419f
      labels:
        app.kubernetes.io/name: nats
        app.kubernetes.io/instance: release-name
    spec:
      # Common volumes for the containers.
      volumes:
      - name: config-volume
        configMap:
          name: release-name-nats-config

      # Local volume shared with the reloader.
      - name: pid
        emptyDir: {}

      #################
      #               #
      #  TLS Volumes  #
      #               #
      #################

      serviceAccountName: release-name-nats

      # Required to be able to HUP signal and apply config
      # reload to the server without restarting the pod.
      shareProcessNamespace: true

      #################
      #               #
      #  NATS Server  #
      #               #
      #################
      terminationGracePeriodSeconds: 60
      containers:
      - name: nats
        image: nats:2.8.4-alpine
        imagePullPolicy: IfNotPresent
        resources:
          {}
        ports:
        - containerPort: 4222
          name: client
        - containerPort: 6222
          name: cluster
        - containerPort: 8222
          name: monitor
        - containerPort: 7777
          name: metrics

        command:
        - "nats-server"
        - "--config"
        - "/etc/nats-config/nats.conf"

        # Required to be able to define an environment variable
        # that refers to other environment variables.  This env var
        # is later used as part of the configuration file.
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_NAME
          value: $(POD_NAME)
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CLUSTER_ADVERTISE
          value: $(POD_NAME).release-name-nats.$(POD_NAMESPACE).svc.cluster.local
        volumeMounts:
        - name: config-volume
          mountPath: /etc/nats-config
        - name: pid
          mountPath: /var/run/nats
        

        #######################
        #                     #
        # Healthcheck Probes  #
        #                     #
        #######################
        livenessProbe:
          httpGet:
            path: /
            port: 8222
          initialDelaySeconds: 10
          timeoutSeconds: 5
          periodSeconds: 30
          successThreshold: 1
          failureThreshold: 3
        startupProbe:
          httpGet:
            # for NATS server versions >=2.7.1, healthz will be enabled to allow for a grace period
            # in case of JetStream enabled deployments to form quorum and streams to catch up.
            path: /healthz
            port: 8222
          initialDelaySeconds: 10
          timeoutSeconds: 5
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 30

        # Gracefully stop NATS Server on pod deletion or image upgrade.
        #
        lifecycle:
          preStop:
            exec:
              # send the lame duck shutdown signal to trigger a graceful shutdown
              # nats-server will ignore the TERM signal it receives after this
              #
              command:
              - "nats-server"
              - "-sl=ldm=/var/run/nats/nats.pid"

      #################################
      #                               #
      #  NATS Configuration Reloader  #
      #                               #
      #################################
      - name: reloader
        image: natsio/nats-server-config-reloader:0.7.2
        imagePullPolicy: IfNotPresent
        resources:
          null
        command:
        - "nats-server-config-reloader"
        - "-pid"
        - "/var/run/nats/nats.pid"
        - "-config"
        - "/etc/nats-config/nats.conf"
        volumeMounts:
        - name: config-volume
          mountPath: /etc/nats-config
        - name: pid
          mountPath: /var/run/nats
        

      ##############################
      #                            #
      #  NATS Prometheus Exporter  #
      #                            #
      ##############################
      - name: metrics
        image: natsio/prometheus-nats-exporter:0.10.0
        imagePullPolicy: IfNotPresent
        resources:
          {}
        args:
        - -connz
        - -routez
        - -subz
        - -varz
        - -prefix=nats
        - -use_internal_server_id
        - http://localhost:8222/
        ports:
        - containerPort: 7777
          name: metrics

  volumeClaimTemplates:
---
# Source: ct-single/charts/nats/templates/tests/test-request-reply.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-nats-test-request-reply"
  labels:
    chart: nats-0.17.5
    app: release-name-nats-test-request-reply
  annotations:
    "helm.sh/hook": test
spec:
  containers:
  - name: nats-box
    image: synadia/nats-box
    env:
    - name: NATS_HOST
      value: release-name-nats
    command:
    - /bin/sh
    - -ec
    - |
      nats reply -s nats://$NATS_HOST:4222 'name.>' --command "echo 1" &
    - |
      "&&"
    - |
      name=$(nats request -s nats://$NATS_HOST:4222 name.test '' 2>/dev/null)
    - |
      "&&"
    - |
      [ $name = test ]

  restartPolicy: Never

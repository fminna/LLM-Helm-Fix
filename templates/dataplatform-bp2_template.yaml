---
# Source: dataplatform-bp2/charts/elasticsearch/charts/kibana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-kibana
  namespace: "default"
  labels:
    app.kubernetes.io/name: kibana
    helm.sh/chart: kibana-9.3.13
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: dataplatform-bp2/charts/kafka/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-16.2.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
  annotations:
automountServiceAccountToken: true
---
# Source: dataplatform-bp2/charts/logstash/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-logstash
  namespace: "default"
  labels:
    app.kubernetes.io/name: logstash
    helm.sh/chart: logstash-3.8.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: dataplatform-bp2/charts/spark/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-spark
  namespace: default
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
automountServiceAccountToken: true
---
# Source: dataplatform-bp2/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-dataplatform-bp2
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform
automountServiceAccountToken: true
---
# Source: dataplatform-bp2/charts/spark/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-spark-secret
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
---
# Source: dataplatform-bp2/charts/elasticsearch/charts/kibana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-kibana-conf
  labels:
    app.kubernetes.io/name: kibana
    helm.sh/chart: kibana-9.3.13
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  kibana.yml: |
    pid.file: /opt/bitnami/kibana/tmp/kibana.pid
    server.host: "::"
    server.port: 5601
    elasticsearch.hosts: [http://release-name-coordinating-only:9200]
    server.rewriteBasePath: false
---
# Source: dataplatform-bp2/charts/kafka/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-zookeeper-scripts
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOST"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: dataplatform-bp2/charts/kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-kafka-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-16.2.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  setup.sh: |-
    #!/bin/bash

    ID="${MY_POD_NAME#"release-name-kafka-"}"
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        export KAFKA_CFG_BROKER_ID="$(grep "broker.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
    else
        export KAFKA_CFG_BROKER_ID="$((ID + 0))"
    fi

    exec /entrypoint.sh /run.sh
---
# Source: dataplatform-bp2/charts/logstash/templates/configuration-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-logstash
  labels:
    app.kubernetes.io/name: logstash
    helm.sh/chart: logstash-3.8.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  logstash.conf: |-
    input {
      # udp {
      #   port => 1514
      #   type => syslog
      # }
      # tcp {
      #   port => 1514
      #   type => syslog
      # }
      http { port => 8080 }
    }
    output {
      # elasticsearch {
      #   hosts => ["${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}"]
      #   manage_template => false
      #   index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
      # }
      # gelf {
      #   host => "${GRAYLOG_HOST}"
      #   port => ${GRAYLOG_PORT}
      # }
      stdout {}
    }
---
# Source: dataplatform-bp2/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-dataplatform-bp2-exporter-configuration
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  bp.json: |-
    {
      "blueprintName": "bp2",
      "metrics": [
        {
          "name": "zookeeper_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of zookeeper nodes in the data platform",
          "key": "zookeeper",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "zookeeper_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of zookeeper nodes in the data platform",
          "key": "zookeeper",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "kafka_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of kafka nodes in the data platform",
          "key": "kafka",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "kafka_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of kafka nodes in the data platform",
          "key": "kafka",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "elasticsearch_master_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of elasticsearch master nodes in the data platform",
          "key": "elasticsearch-master",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "elasticsearch_master_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of elasticsearch master nodes in the data platform",
          "key": "elasticsearch-master",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "elasticsearch_data_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of elasticsearch data nodes in the data platform",
          "key": "elasticsearch-data",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "elasticsearch_data_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of elasticsearch data nodes in the data platform",
          "key": "elasticsearch-data",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "spark_master_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of spark master nodes in the data platform",
          "key": "spark-master",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "spark_master_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of spark master nodes in the data platform",
          "key": "spark-master",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "spark_worker_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of spark worker nodes in the data platform",
          "key": "spark-worker",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "spark_worker_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of spark worker nodes in the data platform",
          "key": "spark-worker",
          "dataComponent": "AvailableNodes"
        },
        {
          "name": "logstash_desired_nodes",
          "type": "gauge",
          "helpMessage": "Desired number of logstash nodes in the data platform",
          "key": "logstash",
          "dataComponent": "DesiredNodes"
        },
        {
          "name": "logstash_available_nodes",
          "type": "gauge",
          "helpMessage": "Available number of logstash nodes in the data platform",
          "key": "logstash",
          "dataComponent": "AvailableNodes"
        }
      ]
    }
---
# Source: dataplatform-bp2/charts/elasticsearch/charts/kibana/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: release-name-kibana
  labels:
    app.kubernetes.io/name: kibana
    helm.sh/chart: kibana-9.3.13
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "10Gi"
---
# Source: dataplatform-bp2/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-dataplatform-bp2
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform
rules:
  - apiGroups:
      - ""
    resources:
      - statefulsets
      - pods
      - services
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - namespaces
      - namespaces/status
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - apps
    resources:
      - controllerrevisions
      - daemonsets
      - daemonsets/status
      - deployments
      - deployments/scale
      - deployments/status
      - replicasets
      - replicasets/scale
      - replicasets/status
      - statefulsets
      - statefulsets/scale
      - statefulsets/status
    verbs:
      - get
      - list
      - watch
---
# Source: dataplatform-bp2/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-dataplatform-bp2
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform
roleRef:
  kind: Role
  name: release-name-dataplatform-bp2
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: release-name-dataplatform-bp2
    namespace: default
---
# Source: dataplatform-bp2/charts/elasticsearch/charts/kibana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kibana
  labels:
    app.kubernetes.io/name: kibana
    helm.sh/chart: kibana-9.3.13
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm

spec:
  type: ClusterIP
  ports:
    - name: http
      port: 5601
      targetPort: http
      nodePort: null
  selector:
    app.kubernetes.io/name: kibana
    app.kubernetes.io/instance: release-name
---
# Source: dataplatform-bp2/charts/elasticsearch/templates/coordinating-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-coordinating-only
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.22
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: coordinating-only
  annotations: 
    {}
spec:
  type: "ClusterIP"
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 9200
      targetPort: http
      nodePort: null
    - name: tcp-transport
      port: 9300
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: coordinating-only
---
# Source: dataplatform-bp2/charts/elasticsearch/templates/data-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-elasticsearch-data
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.22
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: data
  annotations: 
    {}
spec:
  type: ClusterIP
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 9200
      targetPort: http
    - name: tcp-transport
      port: 9300
      targetPort: transport
      nodePort: null
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: data
---
# Source: dataplatform-bp2/charts/elasticsearch/templates/master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-elasticsearch-master
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.22
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
  annotations: 
    {}
spec:
  type: "ClusterIP"
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 9200
      targetPort: http
    - name: tcp-transport
      port: 9300
      targetPort: transport
      nodePort: null
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: master
---
# Source: dataplatform-bp2/charts/kafka/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper-headless
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: zookeeper
---
# Source: dataplatform-bp2/charts/kafka/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: zookeeper
---
# Source: dataplatform-bp2/charts/kafka/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kafka-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-16.2.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
    - name: tcp-internal
      port: 9093
      protocol: TCP
      targetPort: kafka-internal
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: kafka
---
# Source: dataplatform-bp2/charts/kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-16.2.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
      nodePort: null
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: kafka
---
# Source: dataplatform-bp2/charts/logstash/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-logstash-headless
  labels:
    app.kubernetes.io/name: logstash
    helm.sh/chart: logstash-3.8.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
  selector: 
    app.kubernetes.io/name: logstash
    app.kubernetes.io/instance: release-name
---
# Source: dataplatform-bp2/charts/logstash/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-logstash
  labels:
    app.kubernetes.io/name: logstash
    helm.sh/chart: logstash-3.8.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
  selector: 
    app.kubernetes.io/name: logstash
    app.kubernetes.io/instance: release-name
---
# Source: dataplatform-bp2/charts/spark/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-spark-headless
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  selector:
    app.kubernetes.io/name: spark
    app.kubernetes.io/instance: release-name
---
# Source: dataplatform-bp2/charts/spark/templates/svc-master.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-spark-master-svc
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 7077
      targetPort: cluster
      name: cluster
      nodePort: null
    - port: 80
      targetPort: http
      name: http
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/name: spark
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: master
---
# Source: dataplatform-bp2/templates/emitter-svc.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform-emitter
  name: "release-name-dataplatform-bp2-emitter"
  namespace: "default"
spec:
  type: ClusterIP
  
  ports:
    - name: tcp-client
      port: 8091
      protocol: TCP
      targetPort: emitter-port
      nodePort: null
  selector:
    app.kubernetes.io/name: dataplatform-bp2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: dataplatform-emitter
---
# Source: dataplatform-bp2/templates/exporter-svc.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform-exporter
  name: "release-name-dataplatform-bp2-exporter"
  annotations:
    
    prometheus.io/path: /metrics
    prometheus.io/port: "9090"
    prometheus.io/scrape: "true"
  namespace: "default"
spec:
  type: ClusterIP
  
  ports:
    - name: tcp-client
      port: 9090
      protocol: TCP
      targetPort: exporter-port
      nodePort: null
  selector:
    app.kubernetes.io/name: dataplatform-bp2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: dataplatform-exporter
---
# Source: dataplatform-bp2/charts/elasticsearch/charts/kibana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kibana
  labels:
    app.kubernetes.io/name: kibana
    helm.sh/chart: kibana-9.3.13
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: kibana
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kibana
        helm.sh/chart: kibana-9.3.13
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app: kibana
    spec:
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: kibana
                    app.kubernetes.io/instance: release-name
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      serviceAccountName: release-name-kibana
      securityContext:
        fsGroup: 1001
      containers:
        - name: kibana
          image: marketplace.azurecr.io/bitnami/kibana:7.17.1-debian-10-r20
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 1001
          env:
            - name: KIBANA_PORT_NUMBER
              value: "5601"
            - name: KIBANA_ELASTICSEARCH_URL
              value: "http://release-name-coordinating-only:9200"
            - name: KIBANA_ELASTICSEARCH_PORT_NUMBER
              value: "9200"
            - name: KIBANA_FORCE_INITSCRIPTS
              value: "false"
            - name: KIBANA_SERVER_ENABLE_TLS
              value: "false"
            - name: KIBANA_ELASTICSEARCH_ENABLE_TLS
              value: "false"
            - name: KIBANA_ELASTICSEARCH_TLS_USE_PEM
              value: "false"
            - name: KIBANA_ELASTICSEARCH_TLS_VERIFICATION_MODE
              value: "full"
          ports:
            - name: http
              containerPort: 5601
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /login
              port: http
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /status
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: kibana-data
              mountPath: /bitnami/kibana
            - name: kibana-config
              mountPath: /bitnami/kibana/conf
      volumes:
        - name: kibana-data
          persistentVolumeClaim:
            claimName: release-name-kibana
        - name: kibana-config
          configMap:
            name: release-name-kibana-conf
---
# Source: dataplatform-bp2/templates/emitter-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform-emitter
  name: release-name-dataplatform-bp2-emitter
  namespace: "default"
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: dataplatform-bp2
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: dataplatform-emitter
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: dataplatform-bp2
        helm.sh/chart: dataplatform-bp2-12.0.2
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: dataplatform-emitter
    spec:
      serviceAccountName: release-name-dataplatform-bp2
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: dataplatform-bp2
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: dataplatform-emitter
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: dataplatform-emitter
          image: marketplace.azurecr.io/bitnami/dataplatform-emitter:1.0.1-scratch-r26
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BP_NAME
              value: release-name-dataplatform-bp2
            - name: BP_RELEASE_NAME
              value: release-name
            - name: BP_NAMESPACE
              value: default
          ports:
            - name: emitter-port
              containerPort: 8091
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            httpGet:
              path: "/v1/health"
              port: 8091
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 15
            failureThreshold: 15
            successThreshold: 1
          readinessProbe:
            httpGet:
              path: "/v1/health"
              port: 8091
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 15
            failureThreshold: 15
            successThreshold: 15
          volumeMounts:
      volumes:
---
# Source: dataplatform-bp2/templates/exporter-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: dataplatform-bp2
    helm.sh/chart: dataplatform-bp2-12.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: dataplatform-exporter
  name: release-name-dataplatform-bp2-exporter
  namespace: "default"
spec:
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: dataplatform-bp2
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: dataplatform-exporter
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: dataplatform-bp2
        helm.sh/chart: dataplatform-bp2-12.0.2
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: dataplatform-exporter
    spec:
      serviceAccountName: release-name-dataplatform-bp2
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: dataplatform-bp2
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: dataplatform-exporter
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: dataplatform-exporter
          image: marketplace.azurecr.io/bitnami/dataplatform-exporter:1.0.1-scratch-r22
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: METRIC_CONFIG_PATH
              value: "/data/bp.json"
            - name: DP_URI
              value: http://release-name-dataplatform-bp2-emitter:8091
          ports:
            - name: exporter-port
              containerPort: 9090
          resources:
            limits: {}
            requests: {}
          livenessProbe:
            httpGet:
              path: "/metrics"
              port: 9090
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 15
            failureThreshold: 15
            successThreshold: 1
          readinessProbe:
            httpGet:
              path: "/metrics"
              port: 9090
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 15
            failureThreshold: 15
            successThreshold: 15
          volumeMounts:
            - name: exporter-config
              mountPath: /data/bp.json
              subPath: bp.json
      volumes:
        - name: exporter-config
          configMap:
            name: release-name-dataplatform-bp2-exporter-configuration
---
# Source: dataplatform-bp2/charts/elasticsearch/templates/coordinating-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-coordinating-only
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.22
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: coordinating-only
    ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
    app: coordinating-only
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: coordinating-only
  podManagementPolicy: Parallel
  replicas: 2
  serviceName: release-name-coordinating-only
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-17.9.22
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: coordinating-only
        ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
        app: coordinating-only
      annotations:
    spec:
      
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - coordinating-only
              - key: app.kubernetes.io/name
                operator: In
                values:
                - elasticsearch
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - 'release-name'
            topologyKey: kubernetes.io/hostname
      serviceAccountName: default
      securityContext:
        fsGroup: 1001
      initContainers:
        ## Image that performs the sysctl operation to modify Kernel settings (needed sometimes to avoid boot errors)
        - name: sysctl
          image: marketplace.azurecr.io/bitnami/bitnami-shell:10-debian-10-r384
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: marketplace.azurecr.io/bitnami/elasticsearch:7.17.2-debian-10-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: "elastic"
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: "release-name-elasticsearch-master.default.svc.cluster.local,release-name-coordinating-only.default.svc.cluster.local,release-name-elasticsearch-data.default.svc.cluster.local,"
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "5"
            - name: ELASTICSEARCH_CLUSTER_MASTER_HOSTS
              value: release-name-elasticsearch-master-0 release-name-elasticsearch-master-1 release-name-elasticsearch-master-2 
            - name: ELASTICSEARCH_MINIMUM_MASTER_NODES
              value: "2"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).release-name-coordinating-only.default.svc.cluster.local"
            - name: ELASTICSEARCH_HEAP_SIZE
              value: "768m"
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "yes"
            - name: ELASTICSEARCH_NODE_TYPE
              value: "coordinating"
          ports:
            - name: http
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 1Gi
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes:
        - name: "data"
          emptyDir: {}
---
# Source: dataplatform-bp2/charts/elasticsearch/templates/data-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-elasticsearch-data
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.22
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: data
    ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
    app: data
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: data
  podManagementPolicy: Parallel
  replicas: 2
  serviceName: release-name-elasticsearch-data
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-17.9.22
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: data
        ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
        app: data
      annotations:
    spec:
      
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - data
              - key: app.kubernetes.io/name
                operator: In
                values:
                - elasticsearch
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - 'release-name'
            topologyKey: kubernetes.io/hostname
      serviceAccountName: default
      securityContext:
        fsGroup: 1001
      initContainers:
        ## Image that performs the sysctl operation to modify Kernel settings (needed sometimes to avoid boot errors)
        - name: sysctl
          image: marketplace.azurecr.io/bitnami/bitnami-shell:10-debian-10-r384
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: marketplace.azurecr.io/bitnami/elasticsearch:7.17.2-debian-10-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: "elastic"
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: "release-name-elasticsearch-master.default.svc.cluster.local,release-name-coordinating-only.default.svc.cluster.local,release-name-elasticsearch-data.default.svc.cluster.local,"
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "5"
            - name: ELASTICSEARCH_HEAP_SIZE
              value: "4096m"
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "yes"
            - name: ELASTICSEARCH_NODE_TYPE
              value: "data"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).release-name-elasticsearch-data.default.svc.cluster.local"
          ports:
            - name: http
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 5Gi
          volumeMounts:
            - name: "data"
              mountPath: "/bitnami/elasticsearch/data"
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: "data"
      spec:
        accessModes:
          - ReadWriteOnce
        
        
        resources:
          requests:
            storage: "8Gi"
---
# Source: dataplatform-bp2/charts/elasticsearch/templates/master-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-elasticsearch-master
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-17.9.22
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
    ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
    app: master
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: master
  podManagementPolicy: Parallel
  replicas: 3
  serviceName: release-name-elasticsearch-master
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-17.9.22
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
        ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
        app: master
      annotations:
    spec:
      
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - master
              - key: app.kubernetes.io/name
                operator: In
                values:
                - elasticsearch
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - 'release-name'
            topologyKey: kubernetes.io/hostname
      serviceAccountName: default
      securityContext:
        fsGroup: 1001
      initContainers:
        ## Image that performs the sysctl operation to modify Kernel settings (needed sometimes to avoid boot errors)
        - name: sysctl
          image: marketplace.azurecr.io/bitnami/bitnami-shell:10-debian-10-r384
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: marketplace.azurecr.io/bitnami/elasticsearch:7.17.2-debian-10-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: "elastic"
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: "release-name-elasticsearch-master.default.svc.cluster.local,release-name-coordinating-only.default.svc.cluster.local,release-name-elasticsearch-data.default.svc.cluster.local,"
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "5"
            - name: ELASTICSEARCH_CLUSTER_MASTER_HOSTS
              value: release-name-elasticsearch-master-0 release-name-elasticsearch-master-1 release-name-elasticsearch-master-2 
            - name: ELASTICSEARCH_MINIMUM_MASTER_NODES
              value: "2"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).release-name-elasticsearch-master.default.svc.cluster.local"
            - name: ELASTICSEARCH_HEAP_SIZE
              value: "768m"
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "yes"
            - name: ELASTICSEARCH_NODE_TYPE
              value: "master"
          ports:
            - name: http
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 1Gi
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: "data"
      spec:
        accessModes:
          - ReadWriteOnce
        
        
        resources:
          requests:
            storage: "8Gi"
---
# Source: dataplatform-bp2/charts/kafka/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-9.0.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 3
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: zookeeper
  serviceName: release-name-zookeeper-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-9.0.2
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - zookeeper
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - 'release-name'
            topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: marketplace.azurecr.io/bitnami/zookeeper:3.8.0-debian-10-r11
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 5Gi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: release-name-zookeeper-0.release-name-zookeeper-headless.default.svc.cluster.local:2888:3888::1 release-name-zookeeper-1.release-name-zookeeper-headless.default.svc.cluster.local:2888:3888::2 release-name-zookeeper-2.release-name-zookeeper-headless.default.svc.cluster.local:2888:3888::3 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "4096"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: release-name-zookeeper-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: data
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: dataplatform-bp2/charts/kafka/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-kafka
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-16.2.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: kafka
  serviceName: release-name-kafka-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-16.2.0
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka
      annotations:
    spec:
      
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - zookeeper
                - key: app.kubernetes.io/instance
                  operator: In
                  values:
                  - 'release-name'
              topologyKey: kubernetes.io/hostname
            weight: 50
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - kafka
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - 'release-name'
            topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1001
      serviceAccountName: release-name-kafka
      containers:
        - name: kafka
          image: marketplace.azurecr.io/bitnami/kafka:3.1.0-debian-10-r60
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_CFG_ZOOKEEPER_CONNECT
              value: "release-name-zookeeper"
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: "INTERNAL"
            - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
              value: "INTERNAL:PLAINTEXT,CLIENT:PLAINTEXT"
            - name: KAFKA_CFG_LISTENERS
              value: "INTERNAL://:9093,CLIENT://:9092"
            - name: KAFKA_CFG_ADVERTISED_LISTENERS
              value: "INTERNAL://$(MY_POD_NAME).release-name-kafka-headless.default.svc.cluster.local:9093,CLIENT://$(MY_POD_NAME).release-name-kafka-headless.default.svc.cluster.local:9092"
            - name: ALLOW_PLAINTEXT_LISTENER
              value: "yes"
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_LOG_DIR
              value: "/opt/bitnami/kafka/logs"
            - name: KAFKA_CFG_DELETE_TOPIC_ENABLE
              value: "false"
            - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
              value: "true"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx4096m -Xms4096m"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES
              value: "10000"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MS
              value: "1000"
            - name: KAFKA_CFG_LOG_RETENTION_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_RETENTION_CHECK_INTERVALS_MS
              value: "300000"
            - name: KAFKA_CFG_LOG_RETENTION_HOURS
              value: "168"
            - name: KAFKA_CFG_MESSAGE_MAX_BYTES
              value: "1000012"
            - name: KAFKA_CFG_LOG_SEGMENT_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_DIRS
              value: "/bitnami/kafka/data"
            - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
              value: "1"
            - name: KAFKA_CFG_NUM_IO_THREADS
              value: "8"
            - name: KAFKA_CFG_NUM_NETWORK_THREADS
              value: "3"
            - name: KAFKA_CFG_NUM_PARTITIONS
              value: "1"
            - name: KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR
              value: "1"
            - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
              value: "104857600"
            - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS
              value: "6000"
            - name: KAFKA_CFG_AUTHORIZER_CLASS_NAME
              value: ""
            - name: KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND
              value: "true"
            - name: KAFKA_CFG_SUPER_USERS
              value: "User:admin"
          ports:
            - name: kafka-client
              containerPort: 9092
            - name: kafka-internal
              containerPort: 9093
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 5120Mi
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
      volumes:
        - name: scripts
          configMap:
            name: release-name-kafka-scripts
            defaultMode: 0755
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: dataplatform-bp2/charts/logstash/templates/sts.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-logstash
  labels:
    app.kubernetes.io/name: logstash
    helm.sh/chart: logstash-3.8.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: release-name-logstash-headless
  replicas: 2
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: logstash
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: logstash
        helm.sh/chart: logstash-3.8.7
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/configuration: 614823d654965b3a00cf2e70213a4d8f2d268de67f73e483039bffcc521e64ff
    spec:
      
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - logstash
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - 'release-name'
            topologyKey: kubernetes.io/hostname
      securityContext:
        runAsUser: 1001
        fsGroup: 1001
      serviceAccountName: release-name-logstash
      containers:
        - name: logstash
          image: marketplace.azurecr.io/bitnami/logstash:7.17.2-debian-10-r2
          imagePullPolicy: "IfNotPresent"
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: LOGSTASH_CONF_FILENAME
              value: "logstash.conf"
            - name: LOGSTASH_ENABLE_MULTIPLE_PIPELINES
              value: "false"
            - name: LOGSTASH_EXPOSE_API
              value: "yes"
            - name: LOGSTASH_API_PORT_NUMBER
              value: "9600"
            - name: LS_JAVA_OPTS
              value: -Xmx1g -Xms1g
          envFrom:
          ports: 
            - containerPort: 8080
              name: http
              protocol: TCP
            - containerPort: 9600
              name: monitoring
              protocol: TCP
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /
              port: monitoring
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /
              port: monitoring
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 1500Mi
          volumeMounts:
            - name: configurations
              mountPath: /bitnami/logstash/config
      volumes:
        - name: configurations
          configMap:
            name: release-name-logstash
---
# Source: dataplatform-bp2/charts/spark/templates/statefulset-master.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-spark-master
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  serviceName: release-name-spark-headless
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: spark
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: master
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spark
        helm.sh/chart: spark-5.9.7
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
    spec:
      
      serviceAccountName: release-name-spark
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - worker
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - 'release-name'
            topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
        runAsGroup: 0
      containers:
        - name: spark-master
          image: marketplace.azurecr.io/bitnami/spark:3.2.1-debian-10-r60
          imagePullPolicy: "IfNotPresent"
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: cluster
              containerPort: 7077
          volumeMounts:
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: SPARK_MODE
              value: "master"
            - name: SPARK_DAEMON_MEMORY
              value: ""
            - name: SPARK_MASTER_PORT
              value: "7077"
            - name: SPARK_MASTER_WEBUI_PORT
              value: "8080"
          livenessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 180
            periodSeconds: 20
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 5Gi
      volumes:
---
# Source: dataplatform-bp2/charts/spark/templates/statefulset-worker.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-spark-worker
  labels:
    app.kubernetes.io/name: spark
    helm.sh/chart: spark-5.9.7
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: worker
spec:
  serviceName: release-name-spark-headless
  replicas: 2
  podManagementPolicy: OrderedReady
  selector:
    matchLabels:
      app.kubernetes.io/name: spark
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spark
        helm.sh/chart: spark-5.9.7
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: worker
    spec:
      
      serviceAccountName: release-name-spark
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - worker
                - master
              - key: app.kubernetes.io/name
                operator: In
                values:
                - spark
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - 'release-name'
            topologyKey: kubernetes.io/hostname
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
        runAsGroup: 0
      containers:
        - name: spark-worker
          image: marketplace.azurecr.io/bitnami/spark:3.2.1-debian-10-r60
          imagePullPolicy: "IfNotPresent"
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
          volumeMounts:
          env:
            - name: SPARK_MODE
              value: "worker"
            - name: BITNAMI_DEBUG
              value: "false"
            - name: SPARK_DAEMON_MEMORY
              value: ""
            ## There are some environment variables whose existence needs
            ## to be checked because Spark checks if they are null instead of an
            ## empty string
            - name: SPARK_WORKER_WEBUI_PORT
              value: "8081"
            - name: SPARK_DAEMON_JAVA_OPTS
              value: ""
            - name: SPARK_MASTER_URL
              value: spark://release-name-spark-master-svc:7077
            # If you use a custom properties file, it must be loaded using a ConfigMap
            - name: SPARK_WORKER_OPTS
              value: 
          livenessProbe:
            httpGet:
              path: /
              port: 8081
            initialDelaySeconds: 180
            periodSeconds: 20
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            httpGet:
              path: /
              port: 8081
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 5Gi
      volumes:

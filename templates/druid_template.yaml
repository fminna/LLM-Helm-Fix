---
# Source: druid/charts/postgresql/templates/primary/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: release-name-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-15.2.8
    app.kubernetes.io/component: primary
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/component: primary
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 5432
---
# Source: druid/charts/zookeeper/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: release-name-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/version: 3.9.2
    helm.sh/chart: zookeeper-12.12.1
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: zookeeper
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections to ZooKeeper
    - ports:
        - port: 2181
    # Allow internal communications between nodes
    - ports:
        - port: 2888
        - port: 3888
      from:
        - podSelector:
            matchLabels:
              app.kubernetes.io/instance: release-name
              app.kubernetes.io/name: zookeeper
---
# Source: druid/charts/postgresql/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-15.2.8
automountServiceAccountToken: false
---
# Source: druid/charts/zookeeper/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/version: 3.9.2
    helm.sh/chart: zookeeper-12.12.1
    app.kubernetes.io/component: zookeeper
    role: zookeeper
automountServiceAccountToken: false
---
# Source: druid/templates/broker/serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: release-name-druid-broker
  labels:
    app: druid
    chart: druid-29.1.6
    component: broker
    release: release-name
    heritage: Helm
---
# Source: druid/templates/coordinator/serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: release-name-druid-coordinator
  labels:
    app: druid
    chart: druid-29.1.6
    component: coordinator
    release: release-name
    heritage: Helm
---
# Source: druid/templates/historical/serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: release-name-druid-historical
  labels:
    app: druid
    chart: druid-29.1.6
    component: historical
    release: release-name
    heritage: Helm
---
# Source: druid/templates/middleManager/serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: release-name-druid-middle-manager
  labels:
    app: druid
    chart: druid-29.1.6
    component: middle-manager
    release: release-name
    heritage: Helm
---
# Source: druid/templates/overlord/serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: release-name-druid-overlord
  labels:
    app: druid
    chart: druid-29.1.6
    component: overlord
    release: release-name
    heritage: Helm
---
# Source: druid/templates/router/serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: release-name-druid-router
  labels:
    app: druid
    chart: druid-29.1.6
    component: router
    release: release-name
    heritage: Helm
---
# Source: druid/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-15.2.8
type: Opaque
data:
  postgres-password: "OFRRSzZxcjhlaA=="
  password: "ZHJ1aWQ="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: druid/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-zookeeper-scripts
  namespace: default
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/version: 3.9.2
    helm.sh/chart: zookeeper-12.12.1
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOSTNAME"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: druid/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: druid
  labels:
    app: druid
    chart: druid-29.1.6
    release: release-name
    heritage: Helm
data:
  DRUID_USE_CONTAINER_IP: "true"
  druid_emitter: noop
  druid_emitter_http_recipientBaseUrl: http://druid_exporter_url:druid_exporter_port/druid
  druid_emitter_logging_logLevel: debug
  druid_extensions_loadList: '["druid-histogram", "druid-datasketches", "druid-lookups-cached-global",
    "postgresql-metadata-storage"]'
  druid_indexer_logs_directory: /opt/data/indexing-logs
  druid_indexer_logs_type: file
  druid_metadata_storage_connector_connectURI: jdbc:postgresql://postgres:5432/druid
  druid_metadata_storage_connector_password: druid
  druid_metadata_storage_connector_user: druid
  druid_metadata_storage_type: postgresql
  druid_storage_type: local
  druid_zk_service_host: release-name-zookeeper-headless:2181
  druid_metadata_storage_type: postgresql
  druid_metadata_storage_connector_connectURI: jdbc:postgresql://release-name-postgresql:5432/druid
  druid_metadata_storage_connector_user: druid
  druid_metadata_storage_connector_password: druid
---
# Source: druid/templates/broker/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-druid-broker
  labels:
    app: druid
    chart: druid-29.1.6
    component: broker
    release: release-name
    heritage: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - configmaps
    verbs:
      - '*'
---
# Source: druid/templates/coordinator/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-druid-coordinator
  labels:
    app: druid
    chart: druid-29.1.6
    component: coordinator
    release: release-name
    heritage: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - configmaps
    verbs:
      - '*'
---
# Source: druid/templates/historical/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-druid-historical
  labels:
    app: druid
    chart: druid-29.1.6
    component: historical
    release: release-name
    heritage: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - configmaps
    verbs:
      - '*'
---
# Source: druid/templates/middleManager/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-druid-middle-manager
  labels:
    app: druid
    chart: druid-29.1.6
    component: middle-manager
    release: release-name
    heritage: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - configmaps
    verbs:
      - '*'
---
# Source: druid/templates/overlord/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-druid-overlord
  labels:
    app: druid
    chart: druid-29.1.6
    component: overlord
    release: release-name
    heritage: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - configmaps
    verbs:
      - '*'
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - '*'
---
# Source: druid/templates/router/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-druid-router
  labels:
    app: druid
    chart: druid-29.1.6
    component: router
    release: release-name
    heritage: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - pods
      - configmaps
    verbs:
      - '*'
---
# Source: druid/templates/broker/roleBinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-druid-broker
  labels:
    app: druid
    chart: druid-29.1.6
    component: broker
    release: release-name
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-druid-broker
subjects:
  - kind: ServiceAccount
    name: release-name-druid-broker
    namespace: default
---
# Source: druid/templates/coordinator/roleBinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-druid-coordinator
  labels:
    app: druid
    chart: druid-29.1.6
    component: coordinator
    release: release-name
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-druid-coordinator
subjects:
  - kind: ServiceAccount
    name: release-name-druid-coordinator
    namespace: default
---
# Source: druid/templates/historical/roleBinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-druid-historical
  labels:
    app: druid
    chart: druid-29.1.6
    component: historical
    release: release-name
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-druid-historical
subjects:
  - kind: ServiceAccount
    name: release-name-druid-historical
    namespace: default
---
# Source: druid/templates/middleManager/roleBinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-druid-middle-manager
  labels:
    app: druid
    chart: druid-29.1.6
    component: middle-manager
    release: release-name
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-druid-middle-manager
subjects:
  - kind: ServiceAccount
    name: release-name-druid-middle-manager
    namespace: default
---
# Source: druid/templates/overlord/roleBinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-druid-overlord
  labels:
    app: druid
    chart: druid-29.1.6
    component: overlord
    release: release-name
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-druid-overlord
subjects:
  - kind: ServiceAccount
    name: release-name-druid-overlord
    namespace: default
---
# Source: druid/templates/router/roleBinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-druid-router
  labels:
    app: druid
    chart: druid-29.1.6
    component: router
    release: release-name
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-druid-router
subjects:
  - kind: ServiceAccount
    name: release-name-druid-router
    namespace: default
---
# Source: druid/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql-hl
  namespace: "default"
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-15.2.8
    app.kubernetes.io/component: primary
  annotations:
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: druid/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-15.2.8
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/component: primary
---
# Source: druid/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper-headless
  namespace: default
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/version: 3.9.2
    helm.sh/chart: zookeeper-12.12.1
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/component: zookeeper
---
# Source: druid/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/version: 3.9.2
    helm.sh/chart: zookeeper-12.12.1
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/component: zookeeper
---
# Source: druid/templates/broker/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-druid-broker
  labels:
    app: druid
    chart: druid-29.1.6
    component: broker
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8082
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: druid
    release: release-name
    component: broker
---
# Source: druid/templates/coordinator/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-druid-coordinator
  labels:
    app: druid
    chart: druid-29.1.6
    component: coordinator
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8081
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: druid
    release: release-name
    component: coordinator
---
# Source: druid/templates/historical/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-druid-historical
  labels:
    app: druid
    chart: druid-29.1.6
    component: historical
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8083
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: druid
    release: release-name
    component: historical
---
# Source: druid/templates/middleManager/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-druid-middle-manager
  labels:
    app: druid
    chart: druid-29.1.6
    component: middle-manager
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8091
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: druid
    release: release-name
    component: middle-manager
---
# Source: druid/templates/overlord/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-druid-overlord
  labels:
    app: druid
    chart: druid-29.1.6
    component: overlord
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8081
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: druid
    release: release-name
    component: overlord
---
# Source: druid/templates/router/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-druid-router
  labels:
    app: druid
    chart: druid-29.1.6
    component: router
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8888
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: druid
    release: release-name
    component: router
---
# Source: druid/templates/broker/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-druid-broker
  labels:
    app: druid
    chart: druid-29.1.6
    component: broker
    release: release-name
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: druid
      release: release-name
      component: broker
  template:
    metadata:
      labels:
        app: druid
        release: release-name
        component: broker
      annotations:
        druid.k8s.enablePatching: "true"
    spec:
      serviceAccountName: release-name-druid-broker
      containers:
        - name: druid
          image: "apache/druid:29.0.1"
          imagePullPolicy: IfNotPresent
          args: [ "broker" ]
          env:
          - name: POD_NAME
            valueFrom:  {fieldRef: {fieldPath: metadata.name}}
          - name: POD_NAMESPACE
            valueFrom: {fieldRef: {fieldPath: metadata.namespace}}
          - name: DRUID_MAXDIRECTMEMORYSIZE
            value: "400m"
          - name: DRUID_XMS
            value: "512m"
          - name: DRUID_XMX
            value: "512m"
          - name: druid_processing_buffer_sizeBytes
            value: "50000000"
          - name: druid_processing_numMergeBuffers
            value: "2"
          - name: druid_processing_numThreads
            value: "1"
          envFrom:
            - configMapRef:
                name: druid
          ports:
            - name: http
              containerPort: 8082
              protocol: TCP
          livenessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /status/health
              port: 8082
          readinessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /status/health
              port: 8082
          resources:
            {}
---
# Source: druid/templates/coordinator/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-druid-coordinator
  labels:
    app: druid
    chart: druid-29.1.6
    component: coordinator
    release: release-name
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: druid
      release: release-name
      component: coordinator
  template:
    metadata:
      labels:
        app: druid
        release: release-name
        component: coordinator
      annotations:
        druid.k8s.enablePatching: "true"
    spec:
      serviceAccountName: release-name-druid-coordinator
      containers:
        - name: druid
          image: "apache/druid:29.0.1"
          imagePullPolicy: IfNotPresent
          args: [ "coordinator" ]
          env:
          - name: POD_NAME
            valueFrom:  {fieldRef: {fieldPath: metadata.name}}
          - name: POD_NAMESPACE
            valueFrom: {fieldRef: {fieldPath: metadata.namespace}}
          - name: DRUID_XMS
            value: "256m"
          - name: DRUID_XMX
            value: "256m"
          envFrom:
            - configMapRef:
                name: druid
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
          livenessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /status/health
              port: 8081
          readinessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /status/health
              port: 8081
          resources:
            {}
          volumeMounts:
      volumes:
---
# Source: druid/templates/overlord/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-druid-overlord
  labels:
    app: druid
    chart: druid-29.1.6
    component: overlord
    release: release-name
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: druid
      release: release-name
      component: overlord
  template:
    metadata:
      labels:
        app: druid
        release: release-name
        component: overlord
      annotations:
        druid.k8s.enablePatching: "true"
    spec:
      serviceAccountName: release-name-druid-overlord
      containers:
        - name: druid
          image: "apache/druid:29.0.1"
          imagePullPolicy: IfNotPresent
          args: [ "overlord" ]
          env:
          - name: POD_NAME
            valueFrom:  {fieldRef: {fieldPath: metadata.name}}
          - name: POD_NAMESPACE
            valueFrom: {fieldRef: {fieldPath: metadata.namespace}}
          envFrom:
            - configMapRef:
                name: druid 
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
          livenessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /status/health
              port: 8081
          readinessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /status/health
              port: 8081
          resources:
            {}
          volumeMounts:
      volumes:
---
# Source: druid/templates/router/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-druid-router
  labels:
    app: druid
    chart: druid-29.1.6
    component: router
    release: release-name
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: druid
      release: release-name
      component: router
  template:
    metadata:
      labels:
        app: druid
        release: release-name
        component: router
      annotations:
        druid.k8s.enablePatching: "true"
    spec:
      serviceAccountName: release-name-druid-router
      containers:
        - name: druid
          image: "apache/druid:29.0.1"
          imagePullPolicy: IfNotPresent
          args: [ "router" ]
          env:
          - name: POD_NAME
            valueFrom:  {fieldRef: {fieldPath: metadata.name}}
          - name: POD_NAMESPACE
            valueFrom: {fieldRef: {fieldPath: metadata.namespace}}
          - name: DRUID_MAXDIRECTMEMORYSIZE
            value: "128m"
          - name: DRUID_XMS
            value: "128m"
          - name: DRUID_XMX
            value: "128m"
          envFrom:
            - configMapRef:
                name: druid 
          ports:
            - name: http
              containerPort: 8888
              protocol: TCP
          livenessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /status/health
              port: 8888
          readinessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /status/health
              port: 8888
          resources:
            {}
---
# Source: druid/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/version: 16.2.0
    helm.sh/chart: postgresql-15.2.8
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: release-name-postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: release-name-postgresql
      labels:
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: postgresql
        app.kubernetes.io/version: 16.2.0
        helm.sh/chart: postgresql-15.2.8
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: release-name-postgresql
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:16.2.0-debian-12-r18
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_USER
              value: "druid"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-postgresql
                  key: password
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-postgresql
                  key: postgres-password
            - name: POSTGRES_DATABASE
              value: "druid"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "druid" -d "dbname=druid" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "druid" -d "dbname=druid" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 1024Mi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/postgresql/conf
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /opt/bitnami/postgresql/tmp
              subPath: app-tmp-dir
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: druid/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-zookeeper
  namespace: default
  labels:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/version: 3.9.2
    helm.sh/chart: zookeeper-12.12.1
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/component: zookeeper
  serviceName: release-name-zookeeper-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: zookeeper
        app.kubernetes.io/version: 3.9.2
        helm.sh/chart: zookeeper-12.12.1
        app.kubernetes.io/component: zookeeper
    spec:
      enableServiceLinks: true
      serviceAccountName: release-name-zookeeper
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/component: zookeeper
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.9.2-debian-12-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "1"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "10"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: release-name-zookeeper-0.release-name-zookeeper-headless.default.svc.cluster.local:2888:3888::1 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_ENABLE_QUORUM_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/bash
                - -ec
                - ZOO_HC_TIMEOUT=2 /opt/bitnami/scripts/zookeeper/healthcheck.sh
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/bash
                - -ec
                - ZOO_HC_TIMEOUT=2 /opt/bitnami/scripts/zookeeper/healthcheck.sh
          volumeMounts:
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/zookeeper/conf
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /opt/bitnami/zookeeper/logs
              subPath: app-logs-dir
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: scripts
          configMap:
            name: release-name-zookeeper-scripts
            defaultMode: 493
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: druid/templates/historical/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: druid
    chart: druid-29.1.6
    component: historical
    heritage: Helm
    release: release-name
  name: release-name-druid-historical
spec:
  serviceName: release-name-druid-historical
  replicas: 1
  selector:
    matchLabels:
      app: druid
      release: release-name
      component: historical
  template:
    metadata:
      labels:
        app: druid
        component: historical
        release: release-name
      annotations:
        druid.k8s.enablePatching: "true"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: "druid"
                  release: "release-name"
                  component: "historical"
      securityContext:
        fsGroup: 1000
      serviceAccountName: release-name-druid-historical
      containers:
      - name: druid
        args: [ "historical" ]
        env:
        - name: POD_NAME
          valueFrom:  {fieldRef: {fieldPath: metadata.name}}
        - name: POD_NAMESPACE
          valueFrom: {fieldRef: {fieldPath: metadata.namespace}}
        - name: DRUID_MAXDIRECTMEMORYSIZE
          value: "400m"
        - name: DRUID_XMS
          value: "512m"
        - name: DRUID_XMX
          value: "512m"
        - name: druid_processing_buffer_sizeBytes
          value: "50000000"
        - name: druid_processing_numMergeBuffers
          value: "2"
        - name: druid_processing_numThreads
          value: "1"
        envFrom:
          - configMapRef:
              name: druid
        resources:
            {}
        livenessProbe:
          initialDelaySeconds: 60
          httpGet:
            path: /status/health
            port: 8083
        readinessProbe:
          initialDelaySeconds: 60
          httpGet:
            path: /status/health
            port: 8083
        image: "apache/druid:29.0.1"
        imagePullPolicy: "IfNotPresent"
        ports:
        - containerPort: 8083
          name: http
        volumeMounts:
        - mountPath: /opt/druid/var/druid/
          name: data
      volumes:
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: "4Gi"
---
# Source: druid/templates/middleManager/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: druid
    chart: druid-29.1.6
    component: middle-manager
    heritage: Helm
    release: release-name
  name: release-name-druid-middle-manager
spec:
  serviceName: release-name-druid-middle-manager
  replicas: 1
  selector:
    matchLabels:
      app: druid
      release: release-name
      component: middle-manager
  template:
    metadata:
      labels:
        app: druid
        component: middle-manager
        release: release-name
      annotations:
        druid.k8s.enablePatching: "true"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: "druid"
                  release: "release-name"
                  component: "middle-manager"
      securityContext:
        fsGroup: 1000
      serviceAccountName: release-name-druid-middle-manager
      containers:
      - name: druid
        args: [ "middleManager" ]
        env:
        - name: POD_NAME
          valueFrom:  {fieldRef: {fieldPath: metadata.name}}
        - name: POD_NAMESPACE
          valueFrom: {fieldRef: {fieldPath: metadata.namespace}}
        - name: DRUID_XMS
          value: "64m"
        - name: DRUID_XMX
          value: "64m"
        - name: druid_indexer_fork_property_druid_processing_buffer_sizeBytes
          value: "25000000"
        - name: druid_indexer_runner_javaOptsArray
          value: "[\"-server\", \"-Xms256m\", \"-Xmx256m\", \"-XX:MaxDirectMemorySize=300m\", \"-Duser.timezone=UTC\", \"-Dfile.encoding=UTF-8\", \"-XX:+ExitOnOutOfMemoryError\", \"-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager\"]"
        envFrom:
          - configMapRef:
              name: druid 
        resources:
            {}
        livenessProbe:
          initialDelaySeconds: 60
          httpGet:
            path: /status/health
            port: 8091
        readinessProbe:
          initialDelaySeconds: 60
          httpGet:
            path: /status/health
            port: 8091
        image: "apache/druid:29.0.1"
        imagePullPolicy: "IfNotPresent"
        ports:
        - containerPort: 8091
          name: http
        volumeMounts:
        - mountPath: /opt/druid/var/druid/
          name: data
      volumes:
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: "4Gi"

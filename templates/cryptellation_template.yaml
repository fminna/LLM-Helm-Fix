---
# Source: cryptellation/charts/cockroachdb/templates/poddisruptionbudget.yaml
kind: PodDisruptionBudget
apiVersion: policy/v1
metadata:
  name: release-name-cockroachdb-budget
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: cockroachdb
      app.kubernetes.io/instance: "release-name"
      app.kubernetes.io/component: cockroachdb
  maxUnavailable: 1
---
# Source: cryptellation/charts/nats/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: release-name-nats
  namespace: default
  labels:
    helm.sh/chart: nats-0.19.13
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.9.16"
    app.kubernetes.io/managed-by: Helm
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nats
      app.kubernetes.io/instance: release-name
---
# Source: cryptellation/charts/cockroachdb/templates/serviceaccount-certRotateSelfSigner.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: release-name-cockroachdb-rotate-self-signer
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
---
# Source: cryptellation/charts/cockroachdb/templates/serviceaccount.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: release-name-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
---
# Source: cryptellation/charts/nats/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-nats
  namespace: default
  labels:
    helm.sh/chart: nats-0.19.13
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.9.16"
    app.kubernetes.io/managed-by: Helm
---
# Source: cryptellation/charts/nats/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-nats-config
  namespace: default
  labels:
    helm.sh/chart: nats-0.19.13
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.9.16"
    app.kubernetes.io/managed-by: Helm
data:
  nats.conf: |
    # NATS Clients Port
    port: 4222

    # PID file shared with configuration reloader.
    pid_file: "/var/run/nats/nats.pid"

    ###############
    #             #
    # Monitoring  #
    #             #
    ###############
    http: 8222
    server_name:$POD_NAME
    lame_duck_grace_period: 10s
    lame_duck_duration: 30s
---
# Source: cryptellation/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
    name: cryptellation-config
data:
    # Backtests
    backtests.sqldb.database: "backtests"
    backtests.health.port: "9000"
    
    # Candlesticks
    candlesticks.sqldb.database: "candlesticks"
    candlesticks.health.port: "9000"

    # Exchanges
    exchanges.sqldb.database: "exchanges"
    exchanges.health.port: "9000"

    # Exchanges
    ticks.sqldb.database: "ticks"
    ticks.health.port: "9000"
---
# Source: cryptellation/templates/credentials.yaml
apiVersion: v1
kind: ConfigMap
metadata:
    name: cryptellation-credentials
data:
    # Exchanges
    binance.api_key: ""
    binance.secret_key: ""

    # Event brokers
    
    nats.host: "release-name-nats.default.svc.cluster.local"
    nats.port: "4222"

    # Datastores
    
    sqldb.host: "release-name-cockroachdb-public.default.svc.cluster.local"
    sqldb.port: "26257"
    sqldb.user: "root"
    sqldb.password: ""
---
# Source: cryptellation/charts/cockroachdb/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-cockroachdb-default
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
rules:
  - apiGroups: ["certificates.k8s.io"]
    resources: ["certificatesigningrequests"]
    verbs: ["create", "get", "watch"]
---
# Source: cryptellation/charts/cockroachdb/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-cockroachdb-default
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-cockroachdb-default
subjects:
  - kind: ServiceAccount
    name: release-name-cockroachdb
    namespace: "default"
---
# Source: cryptellation/charts/cockroachdb/templates/role-certRotateSelfSigner.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-cockroachdb-rotate-self-signer
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["create", "get", "update", "delete"]
  - apiGroups: ["apps"]
    resources: ["statefulsets"]
    verbs: ["get"]
    resourceNames:
      - release-name-cockroachdb
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["delete", "get"]
---
# Source: cryptellation/charts/cockroachdb/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["create", "get"]
---
# Source: cryptellation/charts/cockroachdb/templates/rolebinding-certRotateSelfSigner.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-cockroachdb-rotate-self-signer
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-cockroachdb-rotate-self-signer
subjects:
  - kind: ServiceAccount
    name: release-name-cockroachdb-rotate-self-signer
    namespace: "default"
---
# Source: cryptellation/charts/cockroachdb/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-cockroachdb
subjects:
  - kind: ServiceAccount
    name: release-name-cockroachdb
    namespace: "default"
---
# Source: cryptellation/charts/cockroachdb/templates/service.discovery.yaml
# This service only exists to create DNS entries for each pod in
# the StatefulSet such that they can resolve each other's IP addresses.
# It does not create a load-balanced ClusterIP and should not be used directly
# by clients in most circumstances.
kind: Service
apiVersion: v1
metadata:
  name: release-name-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: cockroachdb
  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon, but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running
    # in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: _status/vars
    prometheus.io/port: "8080"
spec:
  clusterIP: None
  # We want all Pods in the StatefulSet to have their addresses published for
  # the sake of the other CockroachDB Pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    # The main port, served by gRPC, serves Postgres-flavor SQL, inter-node
    # traffic and the CLI.
    - name: "grpc"
      port: 26257
      targetPort: grpc
    # The secondary port serves the UI as well as health and debug endpoints.
    - name: "http"
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/component: cockroachdb
---
# Source: cryptellation/charts/cockroachdb/templates/service.public.yaml
# This Service is meant to be used by clients of the database.
# It exposes a ClusterIP that will automatically load balance connections
# to the different database Pods.
kind: Service
apiVersion: v1
metadata:
  name: release-name-cockroachdb-public
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: cockroachdb
  annotations:
    service.alpha.kubernetes.io/app-protocols: '{"http":"HTTPS"}'
spec:
  type: "ClusterIP"
  ports:
    # The main port, served by gRPC, serves Postgres-flavor SQL, inter-node
    # traffic and the CLI.
    - name: "grpc"
      port: 26257
      targetPort: grpc
    # The secondary port serves the UI as well as health and debug endpoints.
    - name: "http"
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/component: cockroachdb
---
# Source: cryptellation/charts/nats/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-nats
  namespace: default
  labels:
    helm.sh/chart: nats-0.19.13
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.9.16"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
  - name: client
    port: 4222
    appProtocol: tcp
  - name: cluster
    port: 6222
    appProtocol: tcp
  - name: monitor
    port: 8222
    appProtocol: http
  - name: metrics
    port: 7777
    appProtocol: http
  - name: leafnodes
    port: 7422
    appProtocol: tcp
  - name: gateways
    port: 7522
    appProtocol: tcp
---
# Source: cryptellation/charts/nats/templates/nats-box.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-nats-box
  namespace: default
  labels:
    app: release-name-nats-box
    chart: nats-0.19.13
spec:
  replicas: 1
  selector:
    matchLabels:
      app: release-name-nats-box
  template:
    metadata:
      labels:
        app: release-name-nats-box
    spec:
      volumes:
      containers:
      - name: nats-box
        image: natsio/nats-box:0.13.8
        imagePullPolicy: IfNotPresent
        resources:
          {}
        env:
        - name: NATS_URL
          value: release-name-nats
        command:
        - "tail"
        - "-f"
        - "/dev/null"
        volumeMounts:
---
# Source: cryptellation/templates/backtests.deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: backtests
  name: backtests
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backtests
  strategy: {}
  template:
    metadata:
      labels:
        app: backtests
    spec:
      initContainers:      
      - name: init-sql-database
        image: lerenn/cryptellation:latest
        imagePullPolicy: Always
        command: [ "cryptellation-backtests", "migrations", "migrate" ]
        env:        
        - name: "HEALTH_PORT"
          valueFrom:
            configMapKeyRef:
              key:  backtests.health.port
              name: cryptellation-config
        - name: "NATS_HOST"
          valueFrom:
            configMapKeyRef:
              key:  nats.host
              name: cryptellation-credentials
        - name: "NATS_PORT"
          valueFrom:
            configMapKeyRef:
              key:  nats.port
              name: cryptellation-credentials
        - name: "SQLDB_USER"
          valueFrom:
            configMapKeyRef:
              key: sqldb.user
              name: cryptellation-credentials
        - name: "SQLDB_PASSWORD"
          valueFrom:
            configMapKeyRef:
              key: sqldb.password
              name: cryptellation-credentials
        - name: "SQLDB_HOST"
          valueFrom:
            configMapKeyRef:
              key: sqldb.host
              name: cryptellation-credentials
        - name: "SQLDB_PORT"
          valueFrom:
            configMapKeyRef:
              key: sqldb.port
              name: cryptellation-credentials
        - name: "SQLDB_DATABASE"
          valueFrom:
            configMapKeyRef:
              key: backtests.sqldb.database
              name: cryptellation-config
      containers:
      - name: backtests
        image: lerenn/cryptellation:latest
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /liveness
            port: 9000
        readinessProbe:
          httpGet:
            path: /readiness
            port: 9000
        command: [ "cryptellation-backtests", "serve" ]
        env:        
        - name: "HEALTH_PORT"
          valueFrom:
            configMapKeyRef:
              key:  backtests.health.port
              name: cryptellation-config
        - name: "NATS_HOST"
          valueFrom:
            configMapKeyRef:
              key:  nats.host
              name: cryptellation-credentials
        - name: "NATS_PORT"
          valueFrom:
            configMapKeyRef:
              key:  nats.port
              name: cryptellation-credentials
        - name: "SQLDB_USER"
          valueFrom:
            configMapKeyRef:
              key: sqldb.user
              name: cryptellation-credentials
        - name: "SQLDB_PASSWORD"
          valueFrom:
            configMapKeyRef:
              key: sqldb.password
              name: cryptellation-credentials
        - name: "SQLDB_HOST"
          valueFrom:
            configMapKeyRef:
              key: sqldb.host
              name: cryptellation-credentials
        - name: "SQLDB_PORT"
          valueFrom:
            configMapKeyRef:
              key: sqldb.port
              name: cryptellation-credentials
        - name: "SQLDB_DATABASE"
          valueFrom:
            configMapKeyRef:
              key: backtests.sqldb.database
              name: cryptellation-config
        resources: {}
      volumes:        
status: {}
---
# Source: cryptellation/templates/candlesticks.deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: candlesticks
  name: candlesticks
spec:
  replicas: 1
  selector:
    matchLabels:
      app: candlesticks
  strategy: {}
  template:
    metadata:
      labels:
        app: candlesticks
    spec:
      initContainers:      
      - name: init-sql-database
        image: lerenn/cryptellation:latest
        imagePullPolicy: Always
        command: [ "cryptellation-candlesticks", "migrations", "migrate" ]
        env:        
        - name: "HEALTH_PORT"
          valueFrom:
            configMapKeyRef:
              key:  candlesticks.health.port
              name: cryptellation-config
        - name: "NATS_HOST"
          valueFrom:
            configMapKeyRef:
              key:  nats.host
              name: cryptellation-credentials
        - name: "NATS_PORT"
          valueFrom:
            configMapKeyRef:
              key:  nats.port
              name: cryptellation-credentials
        - name: "SQLDB_USER"
          valueFrom:
            configMapKeyRef:
              key: sqldb.user
              name: cryptellation-credentials
        - name: "SQLDB_PASSWORD"
          valueFrom:
            configMapKeyRef:
              key: sqldb.password
              name: cryptellation-credentials
        - name: "SQLDB_HOST"
          valueFrom:
            configMapKeyRef:
              key: sqldb.host
              name: cryptellation-credentials
        - name: "SQLDB_PORT"
          valueFrom:
            configMapKeyRef:
              key: sqldb.port
              name: cryptellation-credentials
        - name: "SQLDB_DATABASE"
          valueFrom:
            configMapKeyRef:
              key: candlesticks.sqldb.database
              name: cryptellation-config
      containers:
      - name: candlesticks
        image: lerenn/cryptellation:latest
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /liveness
            port: 9000
        readinessProbe:
          httpGet:
            path: /readiness
            port: 9000
        command: [ "cryptellation-candlesticks", "serve" ]
        env:
        - name: "BINANCE_API_KEY"
          valueFrom:
            configMapKeyRef:
              key: binance.api_key
              name: cryptellation-credentials
        - name: "BINANCE_SECRET_KEY"
          valueFrom:
            configMapKeyRef:
              key: binance.secret_key
              name: cryptellation-credentials        
        - name: "HEALTH_PORT"
          valueFrom:
            configMapKeyRef:
              key:  candlesticks.health.port
              name: cryptellation-config
        - name: "NATS_HOST"
          valueFrom:
            configMapKeyRef:
              key:  nats.host
              name: cryptellation-credentials
        - name: "NATS_PORT"
          valueFrom:
            configMapKeyRef:
              key:  nats.port
              name: cryptellation-credentials
        - name: "SQLDB_USER"
          valueFrom:
            configMapKeyRef:
              key: sqldb.user
              name: cryptellation-credentials
        - name: "SQLDB_PASSWORD"
          valueFrom:
            configMapKeyRef:
              key: sqldb.password
              name: cryptellation-credentials
        - name: "SQLDB_HOST"
          valueFrom:
            configMapKeyRef:
              key: sqldb.host
              name: cryptellation-credentials
        - name: "SQLDB_PORT"
          valueFrom:
            configMapKeyRef:
              key: sqldb.port
              name: cryptellation-credentials
        - name: "SQLDB_DATABASE"
          valueFrom:
            configMapKeyRef:
              key: candlesticks.sqldb.database
              name: cryptellation-config
        resources: {}
      volumes:        
status: {}
---
# Source: cryptellation/templates/exchanges.deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: exchanges
  name: exchanges
spec:
  replicas: 1
  selector:
    matchLabels:
      app: exchanges
  strategy: {}
  template:
    metadata:
      labels:
        app: exchanges
    spec:
      initContainers:      
      - name: init-sql-database
        image: lerenn/cryptellation:latest
        imagePullPolicy: Always
        command: [ "cryptellation-exchanges", "migrations", "migrate" ]
        env:        
        - name: "HEALTH_PORT"
          valueFrom:
            configMapKeyRef:
              key:  exchanges.health.port
              name: cryptellation-config
        - name: "NATS_HOST"
          valueFrom:
            configMapKeyRef:
              key:  nats.host
              name: cryptellation-credentials
        - name: "NATS_PORT"
          valueFrom:
            configMapKeyRef:
              key:  nats.port
              name: cryptellation-credentials
        - name: "SQLDB_USER"
          valueFrom:
            configMapKeyRef:
              key: sqldb.user
              name: cryptellation-credentials
        - name: "SQLDB_PASSWORD"
          valueFrom:
            configMapKeyRef:
              key: sqldb.password
              name: cryptellation-credentials
        - name: "SQLDB_HOST"
          valueFrom:
            configMapKeyRef:
              key: sqldb.host
              name: cryptellation-credentials
        - name: "SQLDB_PORT"
          valueFrom:
            configMapKeyRef:
              key: sqldb.port
              name: cryptellation-credentials
        - name: "SQLDB_DATABASE"
          valueFrom:
            configMapKeyRef:
              key: exchanges.sqldb.database
              name: cryptellation-config
      containers:
      - name: exchanges
        image: lerenn/cryptellation:latest
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /liveness
            port: 9000
        readinessProbe:
          httpGet:
            path: /readiness
            port: 9000
        command: [ "cryptellation-exchanges", "serve" ]
        env:
        - name: "BINANCE_API_KEY"
          valueFrom:
            configMapKeyRef:
              key: binance.api_key
              name: cryptellation-credentials
        - name: "BINANCE_SECRET_KEY"
          valueFrom:
            configMapKeyRef:
              key: binance.secret_key
              name: cryptellation-credentials        
        - name: "HEALTH_PORT"
          valueFrom:
            configMapKeyRef:
              key:  exchanges.health.port
              name: cryptellation-config
        - name: "NATS_HOST"
          valueFrom:
            configMapKeyRef:
              key:  nats.host
              name: cryptellation-credentials
        - name: "NATS_PORT"
          valueFrom:
            configMapKeyRef:
              key:  nats.port
              name: cryptellation-credentials
        - name: "SQLDB_USER"
          valueFrom:
            configMapKeyRef:
              key: sqldb.user
              name: cryptellation-credentials
        - name: "SQLDB_PASSWORD"
          valueFrom:
            configMapKeyRef:
              key: sqldb.password
              name: cryptellation-credentials
        - name: "SQLDB_HOST"
          valueFrom:
            configMapKeyRef:
              key: sqldb.host
              name: cryptellation-credentials
        - name: "SQLDB_PORT"
          valueFrom:
            configMapKeyRef:
              key: sqldb.port
              name: cryptellation-credentials
        - name: "SQLDB_DATABASE"
          valueFrom:
            configMapKeyRef:
              key: exchanges.sqldb.database
              name: cryptellation-config
        resources: {}
      volumes:        
status: {}
---
# Source: cryptellation/templates/ticks.deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ticks
  name: ticks
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ticks
  strategy: {}
  template:
    metadata:
      labels:
        app: ticks
    spec:
      initContainers:      
      - name: init-sql-database
        image: lerenn/cryptellation:latest
        imagePullPolicy: Always
        command: [ "cryptellation-ticks", "migrations", "migrate" ]
        env:        
        - name: "HEALTH_PORT"
          valueFrom:
            configMapKeyRef:
              key:  ticks.health.port
              name: cryptellation-config
        - name: "NATS_HOST"
          valueFrom:
            configMapKeyRef:
              key:  nats.host
              name: cryptellation-credentials
        - name: "NATS_PORT"
          valueFrom:
            configMapKeyRef:
              key:  nats.port
              name: cryptellation-credentials
        - name: "SQLDB_USER"
          valueFrom:
            configMapKeyRef:
              key: sqldb.user
              name: cryptellation-credentials
        - name: "SQLDB_PASSWORD"
          valueFrom:
            configMapKeyRef:
              key: sqldb.password
              name: cryptellation-credentials
        - name: "SQLDB_HOST"
          valueFrom:
            configMapKeyRef:
              key: sqldb.host
              name: cryptellation-credentials
        - name: "SQLDB_PORT"
          valueFrom:
            configMapKeyRef:
              key: sqldb.port
              name: cryptellation-credentials
        - name: "SQLDB_DATABASE"
          valueFrom:
            configMapKeyRef:
              key: ticks.sqldb.database
              name: cryptellation-config
      containers:
      - name: ticks
        image: lerenn/cryptellation:latest
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /liveness
            port: 9000
        readinessProbe:
          httpGet:
            path: /readiness
            port: 9000
        command: [ "cryptellation-ticks", "serve" ]
        env:        
        - name: "HEALTH_PORT"
          valueFrom:
            configMapKeyRef:
              key:  ticks.health.port
              name: cryptellation-config
        - name: "NATS_HOST"
          valueFrom:
            configMapKeyRef:
              key:  nats.host
              name: cryptellation-credentials
        - name: "NATS_PORT"
          valueFrom:
            configMapKeyRef:
              key:  nats.port
              name: cryptellation-credentials
        - name: "SQLDB_USER"
          valueFrom:
            configMapKeyRef:
              key: sqldb.user
              name: cryptellation-credentials
        - name: "SQLDB_PASSWORD"
          valueFrom:
            configMapKeyRef:
              key: sqldb.password
              name: cryptellation-credentials
        - name: "SQLDB_HOST"
          valueFrom:
            configMapKeyRef:
              key: sqldb.host
              name: cryptellation-credentials
        - name: "SQLDB_PORT"
          valueFrom:
            configMapKeyRef:
              key: sqldb.port
              name: cryptellation-credentials
        - name: "SQLDB_DATABASE"
          valueFrom:
            configMapKeyRef:
              key: ticks.sqldb.database
              name: cryptellation-config
        resources: {}
      volumes:        
status: {}
---
# Source: cryptellation/charts/cockroachdb/templates/statefulset.yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: release-name-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: cockroachdb
spec:
  serviceName: release-name-cockroachdb
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: "Parallel"
  selector:
    matchLabels:
      app.kubernetes.io/name: cockroachdb
      app.kubernetes.io/instance: "release-name"
      app.kubernetes.io/component: cockroachdb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cockroachdb
        app.kubernetes.io/instance: "release-name"
        app.kubernetes.io/component: cockroachdb
    spec:
      serviceAccountName: release-name-cockroachdb
      initContainers:
        - name: copy-certs
          image: "busybox"
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/sh
            - -c
            - "cp -f /certs/* /cockroach-certs/; chmod 0400 /cockroach-certs/*.key"
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - name: certs
              mountPath: /cockroach-certs/
            - name: certs-secret
              mountPath: /certs/
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: cockroachdb
                    app.kubernetes.io/instance: "release-name"
                    app.kubernetes.io/component: cockroachdb
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: cockroachdb
            app.kubernetes.io/instance: "release-name"
            app.kubernetes.io/component: cockroachdb
        maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
      # No pre-stop hook is required, a SIGTERM plus some time is all that's
      # needed for graceful shutdown of a node.
      terminationGracePeriodSeconds: 60
      containers:
        - name: db
          image: "cockroachdb/cockroach:v22.2.8"
          imagePullPolicy: "IfNotPresent"
          args:
            - shell
            - -ecx
            # The use of qualified `hostname -f` is crucial:
            # Other nodes aren't able to look up the unqualified hostname.
            #
            # `--join` CLI flag is hardcoded to exactly 3 Pods, because:
            # 1. Having `--join` value depending on `statefulset.replicas`
            #    will trigger undesired restart of existing Pods when
            #    StatefulSet is scaled up/down. We want to scale without
            #    restarting existing Pods.
            # 2. At least one Pod in `--join` is enough to successfully
            #    join CockroachDB cluster and gossip with all other existing
            #    Pods, even if there are 3 or more Pods.
            # 3. It's harmless for `--join` to have 3 Pods even for 1-Pod
            #    clusters, while it gives us opportunity to scale up even if
            #    some Pods of existing cluster are down (for whatever reason).
            # See details explained here:
            # https://github.com/helm/charts/pull/18993#issuecomment-558795102
            - >-
              exec /cockroach/cockroach
              start --join=${STATEFULSET_NAME}-0.${STATEFULSET_FQDN}:26257,${STATEFULSET_NAME}-1.${STATEFULSET_FQDN}:26257,${STATEFULSET_NAME}-2.${STATEFULSET_FQDN}:26257
              --advertise-host=$(hostname).${STATEFULSET_FQDN}
              --certs-dir=/cockroach/cockroach-certs/
              --http-port=8080
              --port=26257
              --cache=25%
              --max-sql-memory=25%
              --logtostderr=INFO
          env:
            - name: STATEFULSET_NAME
              value: release-name-cockroachdb
            - name: STATEFULSET_FQDN
              value: release-name-cockroachdb.default.svc.cluster.local
            - name: COCKROACH_CHANNEL
              value: kubernetes-helm
          ports:
            - name: grpc
              containerPort: 26257
              protocol: TCP
            - name: http
              containerPort: 8080
              protocol: TCP
          volumeMounts:
            - name: datadir
              mountPath: /cockroach/cockroach-data/
            - name: certs
              mountPath: /cockroach/cockroach-certs/
          livenessProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /health?ready=1
              port: http
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 2
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            privileged: false
            readOnlyRootFilesystem: true
      volumes:
        - name: datadir
          persistentVolumeClaim:
            claimName: datadir
        - name: certs
          emptyDir: {}
        - name: certs-secret
          projected:
            sources:
            - secret:
                name: release-name-cockroachdb-node-secret
                items:
                - key: ca.crt
                  path: ca.crt
                  mode: 256
                - key: tls.crt
                  path: node.crt
                  mode: 256
                - key: tls.key
                  path: node.key
                  mode: 256
      securityContext:
        fsGroup: 1000
        runAsGroup: 1000
        runAsUser: 1000
        runAsNonRoot: true
  volumeClaimTemplates:
    - metadata:
        name: datadir
        labels:
          app.kubernetes.io/name: cockroachdb
          app.kubernetes.io/instance: "release-name"
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: "100Gi"
---
# Source: cryptellation/charts/nats/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-nats
  namespace: default
  labels:
    helm.sh/chart: nats-0.19.13
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.9.16"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: nats
      app.kubernetes.io/instance: release-name
  replicas: 1
  serviceName: release-name-nats

  podManagementPolicy: Parallel

  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "7777"
        prometheus.io/scrape: "true"
        checksum/config: 897f848aa1c97e7473dce2307faa5c2617a37779c14a7467d5b40748da87f282
      labels:
        app.kubernetes.io/name: nats
        app.kubernetes.io/instance: release-name
    spec:
      dnsPolicy: ClusterFirst
      # Common volumes for the containers.
      volumes:
      - name: config-volume
        configMap:
          name: release-name-nats-config

      # Local volume shared with the reloader.
      - name: pid
        emptyDir: {}

      #################
      #               #
      #  TLS Volumes  #
      #               #
      #################

      serviceAccountName: release-name-nats

      # Required to be able to HUP signal and apply config
      # reload to the server without restarting the pod.
      shareProcessNamespace: true

      #################
      #               #
      #  NATS Server  #
      #               #
      #################
      terminationGracePeriodSeconds: 60
      containers:
      - name: nats
        image: nats:2.9.16-alpine
        imagePullPolicy: IfNotPresent
        resources:
          {}
        ports:
        - containerPort: 4222
          name: client
        - containerPort: 6222
          name: cluster
        - containerPort: 8222
          name: monitor

        command:
        - "nats-server"
        - "--config"
        - "/etc/nats-config/nats.conf"

        # Required to be able to define an environment variable
        # that refers to other environment variables.  This env var
        # is later used as part of the configuration file.
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_NAME
          value: $(POD_NAME)
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CLUSTER_ADVERTISE
          value: $(POD_NAME).release-name-nats.$(POD_NAMESPACE).svc.cluster.local
        volumeMounts:
        - name: config-volume
          mountPath: /etc/nats-config
        - name: pid
          mountPath: /var/run/nats
        

        #######################
        #                     #
        # Healthcheck Probes  #
        #                     #
        #######################
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 8222
          initialDelaySeconds: 10
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 8222
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        startupProbe:
          # for NATS server versions >=2.7.1, /healthz will be enabled
          # startup probe checks that the JS server is enabled, is current with the meta leader,
          # and that all streams and consumers assigned to this JS server are current
          failureThreshold: 90
          httpGet:
            path: /healthz
            port: 8222
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5

        # Gracefully stop NATS Server on pod deletion or image upgrade.
        #
        lifecycle:
          preStop:
            exec:
              # send the lame duck shutdown signal to trigger a graceful shutdown
              # nats-server will ignore the TERM signal it receives after this
              #
              command:
              - "nats-server"
              - "-sl=ldm=/var/run/nats/nats.pid"

      #################################
      #                               #
      #  NATS Configuration Reloader  #
      #                               #
      #################################
      - name: reloader
        image: natsio/nats-server-config-reloader:0.10.1
        imagePullPolicy: IfNotPresent
        resources:
          {}
        command:
        - "nats-server-config-reloader"
        - "-pid"
        - "/var/run/nats/nats.pid"
        - "-config"
        - "/etc/nats-config/nats.conf"
        volumeMounts:
        - name: config-volume
          mountPath: /etc/nats-config
        - name: pid
          mountPath: /var/run/nats
        

      ##############################
      #                            #
      #  NATS Prometheus Exporter  #
      #                            #
      ##############################
      - name: metrics
        image: natsio/prometheus-nats-exporter:0.10.1
        imagePullPolicy: IfNotPresent
        resources:
          {}
        args:
        - -connz
        - -routez
        - -subz
        - -varz
        - -prefix=nats
        - -use_internal_server_id
        - http://localhost:8222/
        ports:
        - containerPort: 7777
          name: metrics

  volumeClaimTemplates:
---
# Source: cryptellation/charts/cockroachdb/templates/cronjob-ca-certSelfSigner.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: release-name-cockroachdb-rotate-self-signer
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
spec:
  schedule: 0 0 1 */11 *
  jobTemplate:
    spec:
      backoffLimit: 1
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: cert-rotate-job
            image: "gcr.io/cockroachlabs-helm-charts/cockroach-self-signer-cert:1.3"
            imagePullPolicy: "IfNotPresent"
            args:
            - rotate
            - --ca
            - --ca-duration=43800h
            - --ca-expiry=648h
            - --ca-cron=0 0 1 */11 *
            - --readiness-wait=30s
            - --pod-update-timeout=2m
            env:
            - name: STATEFULSET_NAME
              value: release-name-cockroachdb
            - name: NAMESPACE
              value: default
            - name: CLUSTER_DOMAIN
              value: cluster.local
          serviceAccountName: release-name-cockroachdb-rotate-self-signer
---
# Source: cryptellation/charts/cockroachdb/templates/cronjob-client-node-certSelfSigner.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: release-name-cockroachdb-rotate-self-signer-client
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
spec:
  schedule: 0 0 */26 * *
  jobTemplate:
    spec:
      backoffLimit: 1
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: cert-rotate-job
            image: "gcr.io/cockroachlabs-helm-charts/cockroach-self-signer-cert:1.3"
            imagePullPolicy: "IfNotPresent"
            args:
            - rotate
            - --ca-duration=43800h
            - --ca-expiry=648h
            - --client
            - --client-duration=672h
            - --client-expiry=48h
            - --node
            - --node-duration=8760h
            - --node-expiry=168h
            - --node-client-cron=0 0 */26 * *
            - --readiness-wait=30s
            - --pod-update-timeout=2m
            env:
            - name: STATEFULSET_NAME
              value: release-name-cockroachdb
            - name: NAMESPACE
              value: default
            - name: CLUSTER_DOMAIN
              value: cluster.local
          serviceAccountName: release-name-cockroachdb-rotate-self-signer
---
# Source: cryptellation/charts/cockroachdb/templates/serviceaccount-certSelfSigner.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: release-name-cockroachdb-self-signer
  namespace: "default"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "1"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
---
# Source: cryptellation/charts/cockroachdb/templates/role-certSelfSigner.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-cockroachdb-self-signer
  namespace: "default"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "2"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["create", "get", "update", "delete"]
  - apiGroups: ["apps"]
    resources: ["statefulsets"]
    verbs: ["get"]
    resourceNames:
      - release-name-cockroachdb
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["delete", "get"]
---
# Source: cryptellation/charts/cockroachdb/templates/rolebinding-certSelfSigner.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-cockroachdb-self-signer
  namespace: "default"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "3"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-cockroachdb-self-signer
subjects:
  - kind: ServiceAccount
    name: release-name-cockroachdb-self-signer
    namespace: "default"
---
# Source: cryptellation/charts/cockroachdb/templates/tests/client.yaml
kind: Pod
apiVersion: v1
metadata:
  name: release-name-cockroachdb-test
  namespace: "default"
  annotations:
    helm.sh/hook: test-success
spec:
  restartPolicy: Never
  containers:
    - name: client-test
      image: "cockroachdb/cockroach:v22.2.8"
      imagePullPolicy: "IfNotPresent"
      command:
        - /cockroach/cockroach
        - sql
        - --insecure
        - --host
        - release-name-cockroachdb-public.default
        - --port
        - "26257"
        - -e
        - SHOW DATABASES;
---
# Source: cryptellation/charts/nats/templates/tests/test-request-reply.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-nats-test-request-reply"
  labels:
    chart: nats-0.19.13
    app: release-name-nats-test-request-reply
  annotations:
    "helm.sh/hook": test
spec:
  containers:
  - name: nats-box
    image: natsio/nats-box:0.13.8
    env:
    - name: NATS_HOST
      value: release-name-nats
    command:
    - /bin/sh
    - -ec
    - |
      nats reply -s nats://$NATS_HOST:4222 'name.>' --command "echo 1" &
    - |
      "&&"
    - |
      name=$(nats request -s nats://$NATS_HOST:4222 name.test '' 2>/dev/null)
    - |
      "&&"
    - |
      [ $name = test ]

  restartPolicy: Never
---
# Source: cryptellation/charts/cockroachdb/templates/job-certSelfSigner.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-cockroachdb-self-signer
  namespace: "default"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "4"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
spec:
  template:
    metadata:
      name: release-name-cockroachdb-self-signer
      labels:
        helm.sh/chart: cockroachdb-10.0.8
        app.kubernetes.io/name: cockroachdb
        app.kubernetes.io/instance: "release-name"
        app.kubernetes.io/managed-by: "Helm"
    spec:
      securityContext:
        runAsGroup: 1000
        runAsUser: 1000
        fsGroup: 1000
        runAsNonRoot: true
      restartPolicy: Never
      containers:
        - name: cert-generate-job
          image: "gcr.io/cockroachlabs-helm-charts/cockroach-self-signer-cert:1.3"
          imagePullPolicy: "IfNotPresent"
          args:
            - generate
            - --ca-duration=43800h
            - --ca-expiry=648h
            - --client-duration=672h
            - --client-expiry=48h
            - --node-duration=8760h
            - --node-expiry=168h
          env:
          - name: STATEFULSET_NAME
            value: release-name-cockroachdb
          - name: NAMESPACE
            value: "default"
          - name: CLUSTER_DOMAIN
            value: cluster.local
      serviceAccountName: release-name-cockroachdb-self-signer
---
# Source: cryptellation/charts/cockroachdb/templates/job-cleaner.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-cockroachdb-self-signer-cleaner
  namespace: "default"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "helm.sh/hook": pre-delete
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
spec:
  backoffLimit: 1
  template:
    metadata:
      name: release-name-cockroachdb-self-signer-cleaner
      labels:
        helm.sh/chart: cockroachdb-10.0.8
        app.kubernetes.io/name: cockroachdb
        app.kubernetes.io/instance: "release-name"
        app.kubernetes.io/managed-by: "Helm"
    spec:
      securityContext:
        runAsGroup: 1000
        runAsUser: 1000
        fsGroup: 1000
        runAsNonRoot: true
      restartPolicy: Never
      containers:
        - name: cleaner
          image: "gcr.io/cockroachlabs-helm-charts/cockroach-self-signer-cert:1.3"
          imagePullPolicy: "IfNotPresent"
          args:
            - cleanup
            - --namespace=default
          env:
          - name: STATEFULSET_NAME
            value: release-name-cockroachdb
      serviceAccountName: release-name-cockroachdb-rotate-self-signer
---
# Source: cryptellation/charts/cockroachdb/templates/job.init.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: release-name-cockroachdb-init
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-10.0.8
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: init
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cockroachdb
        app.kubernetes.io/instance: "release-name"
        app.kubernetes.io/component: init
    spec:
      securityContext:
        runAsGroup: 1000
        runAsUser: 1000
        fsGroup: 1000
        runAsNonRoot: true
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      serviceAccountName: release-name-cockroachdb
      initContainers:
        - name: copy-certs
          image: "busybox"
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/sh
            - -c
            - "cp -f /certs/* /cockroach-certs/; chmod 0400 /cockroach-certs/*.key"
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - name: client-certs
              mountPath: /cockroach-certs/
            - name: certs-secret
              mountPath: /certs/
      containers:
        - name: cluster-init
          image: "cockroachdb/cockroach:v22.2.8"
          imagePullPolicy: "IfNotPresent"
          # Run the command in an `while true` loop because this Job is bound
          # to come up before the CockroachDB Pods (due to the time needed to
          # get PersistentVolumes attached to Nodes), and sleeping 5 seconds
          # between attempts is much better than letting the Pod fail when
          # the init command does and waiting out Kubernetes' non-configurable
          # exponential back-off for Pod restarts.
          # Command completes either when cluster initialization succeeds,
          # or when cluster has been initialized already.
          command:
          - /bin/bash
          - -c
          - >-
              initCluster() {
                while true; do
                  local output=$(
                    set -x;

                    /cockroach/cockroach init \
                      --certs-dir=/cockroach-certs/ \
                      --host=release-name-cockroachdb-0.release-name-cockroachdb:26257
                  2>&1);

                  local exitCode="$?";
                  echo $output;

                  if [[ "$exitCode" == "0" || "$output" == *"cluster has already been initialized"* ]]
                    then break;
                  fi

                  sleep 5;
                done
              }

              initCluster;
          env:
          volumeMounts:
            - name: client-certs
              mountPath: /cockroach-certs/
      volumes:
        - name: client-certs
          emptyDir: {}
        - name: certs-secret
          projected:
            sources:
            - secret:
                name: release-name-cockroachdb-client-secret
                items:
                - key: ca.crt
                  path: ca.crt
                  mode: 0400
                - key: tls.crt
                  path: client.root.crt
                  mode: 0400
                - key: tls.key
                  path: client.root.key
                  mode: 0400

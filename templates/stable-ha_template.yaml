---
# Source: stable-ha/charts/nats/templates/pod-disruption-budget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.12
    helm.sh/chart: nats-1.1.10
  name: release-name-nats
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: nats
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: nats
---
# Source: stable-ha/charts/nats/templates/nats-box/contexts-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  labels:
    app.kubernetes.io/component: nats-box
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.12
    helm.sh/chart: nats-1.1.10
  name: release-name-nats-box-contexts
stringData:
  default.json: |
    {
      "url": "nats://release-name-nats"
    }
type: Opaque
---
# Source: stable-ha/charts/nats/templates/config-map.yaml
apiVersion: v1
data:
  nats.conf: |
    {
      "http_port": 8222,
      "lame_duck_duration": "30s",
      "lame_duck_grace_period": "10s",
      "pid_file": "/var/run/nats/nats.pid",
      "port": 4222,
      "server_name": $SERVER_NAME
    }
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.12
    helm.sh/chart: nats-1.1.10
  name: release-name-nats-config
---
# Source: stable-ha/charts/nats/templates/headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.12
    helm.sh/chart: nats-1.1.10
  name: release-name-nats-headless
spec:
  clusterIP: None
  ports:
  - appProtocol: tcp
    name: nats
    port: 4222
    targetPort: nats
  - appProtocol: http
    name: monitor
    port: 8222
    targetPort: monitor
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: nats
---
# Source: stable-ha/charts/nats/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.12
    helm.sh/chart: nats-1.1.10
  name: release-name-nats
spec:
  ports:
  - appProtocol: tcp
    name: nats
    port: 4222
    targetPort: nats
  selector:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: nats
---
# Source: stable-ha/templates/services/admin-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-admin-service
  labels:
    app: release-name-admin-service
  annotations:
    metallb.universe.tf/address-pool: dev-admin-ip
    metallb.universe.tf/allow-shared-ip: "admin"
spec:
  type: LoadBalancer
  # loadBalancerIP: 
  externalTrafficPolicy: Cluster
  selector:
    app: release-name-admin-service
  ports:
    - port: 443
      targetPort: 4001
      protocol: TCP
      name: web
    - port: 80
      targetPort: 4080
      protocol: TCP
      name: api
    # - port: 22
    #   targetPort: 3022
    #   protocol: TCP
    #   name: sftp
---
# Source: stable-ha/templates/services/primary-api-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-primary-api
  labels:
    app: release-name-primary-api
  annotations:
    metallb.universe.tf/address-pool: dev-primary-ip
    metallb.universe.tf/allow-shared-ip: "primary"
spec:
  # loadBalancerIP: 192.168.0.235
  type: LoadBalancer
  externalTrafficPolicy: Cluster
  selector:
    app: release-name-api
  ports:
    - port: 80
      targetPort: 4080
      protocol: TCP
      name: web
    - port: 22
      targetPort: 3022
      protocol: TCP
      name: sftp
---
# Source: stable-ha/templates/services/secondary-api-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-secondary-api
  labels:
    app: release-name-secondary-api
  annotations:
    metallb.universe.tf/address-pool: dev-secondary-ip
    metallb.universe.tf/allow-shared-ip: "secondary"
spec:
  # loadBalancerIP: 192.168.0.236
  type: LoadBalancer
  externalTrafficPolicy: Cluster
  selector:
    app: release-name-api
  ports:
    - port: 80
      targetPort: 4080
      protocol: TCP
      name: web
    - port: 22
      targetPort: 3022
      protocol: TCP
      name: sftp
---
# Source: stable-ha/templates/services/traceroute-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: traceroute
  labels:
    app: release-name-traceroute-service
spec:
  type: ClusterIP
  selector:
    app: release-name-traceroute-service
  ports:
    - port: 4100
      targetPort: 4100
      protocol: TCP
      name: api
---
# Source: stable-ha/charts/nats/templates/nats-box/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: nats-box
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.12
    helm.sh/chart: nats-1.1.10
  name: release-name-nats-box
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: nats-box
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: nats
  template:
    metadata:
      labels:
        app.kubernetes.io/component: nats-box
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: nats
        app.kubernetes.io/version: 2.10.12
        helm.sh/chart: nats-1.1.10
    spec:
      containers:
      - args:
        - trap true INT TERM; sleep infinity & wait
        command:
        - sh
        - -ec
        - |
          work_dir="$(pwd)"
          mkdir -p "$XDG_CONFIG_HOME/nats"
          cd "$XDG_CONFIG_HOME/nats"
          if ! [ -s context ]; then
            ln -s /etc/nats-contexts context
          fi
          if ! [ -f context.txt ]; then
            echo -n "default" > context.txt
          fi
          cd "$work_dir"
          exec sh -ec "$0"
        image: natsio/nats-box:0.14.2
        name: nats-box
        volumeMounts:
        - mountPath: /etc/nats-contexts
          name: contexts
      enableServiceLinks: false
      volumes:
      - name: contexts
        secret:
          secretName: release-name-nats-box-contexts
---
# Source: stable-ha/templates/deployments/admin-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-admin-service
  labels:
    app: release-name-admin-service
spec:
  selector:
    matchLabels:
      app: release-name-admin-service
  replicas: 1
  strategy:
    # type: Recreate
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 70%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: release-name-admin-service
    spec:
      securityContext:
        runAsUser: 1001
      tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 2
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 2
      terminationGracePeriodSeconds: 5
      affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - release-name-admin-service
                topologyKey: "kubernetes.io/hostname"
      containers:
        - name: admin-service
          image: calltelemetry/web:0.8.1-rc51
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 4001
            - containerPort: 4080
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
            limits:
              # memory: "1024Mi"
              cpu: 2
          # livenessProbe:
          #   httpsGet:
          #     path: /
          #     port: 4001
          #   initialDelaySeconds: 15
          livenessProbe:
            httpGet:
              scheme: HTTPS
              path: /
              port: 4001
            initialDelaySeconds: 15
            periodSeconds: 3
          readinessProbe:
            httpGet:
              scheme: HTTPS
              path: /
              port: 4001
            initialDelaySeconds: 3
            periodSeconds: 3
          env:
            - name: EXTERNAL_IP
              value: 192.168.123.205
            - name: DB_USER
              valueFrom: { secretKeyRef: { name: hippo-pguser-calltelemetry, key: user} }
            - name: DB_PASSWORD
              valueFrom: { secretKeyRef: { name: hippo-pguser-calltelemetry, key: password } }
            - name: DB_NAME
              valueFrom: { secretKeyRef: { name: hippo-pguser-calltelemetry, key: dbname } }
            - name: DB_HOSTNAME
              valueFrom: { secretKeyRef: { name: hippo-pguser-calltelemetry, key: host} }
            - name: CDR_ROOT_PATH
              value: /tmp
            - name: DB_PORT
              value: "5432"
            - name: DB_SSL_ENABLED
              value: "true"
            - name: LOGGING_LEVEL
              value: warning
            - name: LOCAL_NATS
              value: release-name-nats
            - name: WORKER_NODE
              value: "TRUE"
            - name: ADMIN_NODE
              value: "TRUE"
---
# Source: stable-ha/templates/deployments/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-api
  labels:
    app: release-name-api
spec:
  selector:
    matchLabels:
      app: release-name-api
  replicas: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 2
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: release-name-api
    spec:
      securityContext:
        runAsUser: 1001
      tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 2
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 2
      terminationGracePeriodSeconds: 5
      affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - release-name-api
                topologyKey: "kubernetes.io/hostname"
      containers:
        - name: primary-api
          image: calltelemetry/web:0.8.1-rc51
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 4080
            - containerPort: 4000
            - containerPort: 3022
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "1024Mi"
              cpu: 2
          livenessProbe:
            httpGet:
              path: /healthz
              port: 4080
            initialDelaySeconds: 15
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /healthz
              port: 4080
            initialDelaySeconds: 3
            periodSeconds: 3
          env:
            - name: EXTERNAL_IP
              value: 192.168.123.205
            - name: DB_USER
              valueFrom: { secretKeyRef: { name: hippo-pguser-calltelemetry, key: user} }
            - name: DB_PASSWORD
              valueFrom: { secretKeyRef: { name: hippo-pguser-calltelemetry, key: password } }
            - name: DB_NAME
              valueFrom: { secretKeyRef: { name: hippo-pguser-calltelemetry, key: dbname } }
            - name: DB_HOSTNAME
              valueFrom: { secretKeyRef: { name: hippo-pguser-calltelemetry, key: host} }
            - name: DB_PORT
              value: "5432"
            - name: DB_SSL_ENABLED
              value: "true"
            - name: CDR_ROOT_PATH
              value: /tmp
            - name: LOGGING_LEVEL
              value: warning
            - name: TRACEROUTE_SERVICE
              value: release-name-traceroute-service
            - name: LOCAL_NATS
              value: release-name-nats
            - name: ADMIN_NODE
              value: "FALSE"
            - name: WORKER_NODE
              value: "TRUE"
---
# Source: stable-ha/templates/deployments/traceroute-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-traceroute-service
  labels:
    app: release-name-traceroute-service
spec:
  selector:
    matchLabels:
      app: release-name-traceroute-service
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 70%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: release-name-traceroute-service
    spec:
      securityContext:
        # allowPrivilegeEscalation: false
        # privileged: true
        runAsUser: 0
        # capabilities:
          # add: ["CAP_NET_RAW"]
      tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 2
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 2
      terminationGracePeriodSeconds: 5
      # affinity:
      #     podAntiAffinity:
      #       preferredDuringSchedulingIgnoredDuringExecution:
      #       - weight: 100
      #         podAffinityTerm:
      #           labelSelector:
      #             matchExpressions:
      #             - key: app
      #               operator: In
      #               values:
      #               - release-name-primary-web
      #               - release-name-secondary-web
      #           topologyKey: "kubernetes.io/hostname"
      containers:
        - name: release-name-traceroute
          image: calltelemetry/traceroute:0.7.2
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 4100
          resources:
            requests:
              # memory: "512Mi"
              cpu: "256m"
            limits:
              # memory: "1024Mi"
              cpu: 1
          env:
            - name: NATS_HOSTNAME
              value: release-name-nats
---
# Source: stable-ha/templates/sftp-server.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-sftp-service
  labels:
    app: release-name-sftp-service
    deployment: release-name-sftp-service
spec:
  selector:
    matchLabels:
      app: release-name-sftp-service
      deployment: release-name-sftp-service
  replicas: 1
  strategy:
    # type: Recreate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 2
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: release-name-sftp-service
        deployment: release-name-sftp-service
    spec:
      tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 2
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 2
      terminationGracePeriodSeconds: 5
      # affinity:
      #     podAntiAffinity:
      #       preferredDuringSchedulingIgnoredDuringExecution:
      #       - weight: 100
      #         podAffinityTerm:
      #           labelSelector:
      #             matchExpressions:
      #             - key: app
      #               operator: In
      #               values:
      #               - release-name-primary-web
      #               - release-name-secondary-web
      #           topologyKey: "kubernetes.io/hostname"
      containers:
        - name: sftp-service
          image: calltelemetry/sftp:0.6.8
          imagePullPolicy: 
          ports:
            - containerPort: 4000
          resources:
            requests:
              # memory: "512Mi"
              cpu: "100m"
            limits:
            #   # memory: "1024Mi"
              cpu: 1
          # livenessProbe:
          #   httpGet:
          #     path: /
          #     port: 8080
          #   initialDelaySeconds: 120
          env:
            - name: NATS_HOSTNAME
              value: release-name-nats
            # - name: LOGGING_LEVEL
              # value: info
---
# Source: stable-ha/charts/nats/templates/stateful-set.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/component: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.12
    helm.sh/chart: nats-1.1.10
  name: release-name-nats
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: nats
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/name: nats
  serviceName: release-name-nats-headless
  template:
    metadata:
      annotations:
        checksum/config: 6323b37fafebe14d9c59898769544408f1f2c28e7bc85057e78466d7a472fb8c
      labels:
        app.kubernetes.io/component: nats
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: nats
        app.kubernetes.io/version: 2.10.12
        helm.sh/chart: nats-1.1.10
    spec:
      containers:
      - args:
        - --config
        - /etc/nats-config/nats.conf
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_NAME
          value: $(POD_NAME)
        - name: GOMEMLIMIT
          value: 1GiB
        image: nats:2.10.12-alpine
        lifecycle:
          preStop:
            exec:
              command:
              - nats-server
              - -sl=ldm=/var/run/nats/nats.pid
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz?js-enabled-only=true
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        name: nats
        ports:
        - containerPort: 4222
          name: nats
        - containerPort: 8222
          name: monitor
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz?js-server-only=true
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: "1"
            memory: 1Gi
          requests:
            cpu: "1"
            memory: 1Gi
        startupProbe:
          failureThreshold: 90
          httpGet:
            path: /healthz
            port: monitor
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        volumeMounts:
        - mountPath: /etc/nats-config
          name: config
        - mountPath: /var/run/nats
          name: pid
      - args:
        - -pid
        - /var/run/nats/nats.pid
        - -config
        - /etc/nats-config/nats.conf
        image: natsio/nats-server-config-reloader:0.14.1
        name: reloader
        volumeMounts:
        - mountPath: /var/run/nats
          name: pid
        - mountPath: /etc/nats-config
          name: config
      enableServiceLinks: false
      shareProcessNamespace: true
      volumes:
      - configMap:
          name: release-name-nats-config
        name: config
      - emptyDir: {}
        name: pid
  volumeClaimTemplates: null
---
# Source: stable-ha/templates/ip_pool.yaml
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   # namespace: metallb-system
#   name: metallb-config
# data:
#   config: |
#     address-pools:
#     - name: primary-pool
#       protocol: layer2
#       addresses:
#       - 192.168.0.235/32
#     - name: secondary-pool
#       protocol: layer2
#       addresses:
#       - 192.168.0.236/32
#     - name: admin
#       protocol: layer2
#       addresses:
#       - /32
---
# Source: stable-ha/templates/metallb/metal-secret.yaml
# apiVersion: v1
# kind: Secret
# metadata:
#   name: metallb-secretkey
# type: Opaque
# data:
#   secretkey: CxKTsHCMFzqEKlN9Q96COZ68CPmwqAQ4z3z15RYRpKXaF3xyKjvydheX/PoFTgx6
---
# Source: stable-ha/templates/nats-stream.yaml
# ---
# apiVersion: jetstream.nats.io/v1beta2
# kind: Stream
# metadata:
#   name: uploads
# spec:
#   name: uploads
#   subjects: ["upload.*"]
#   storage: file
#   maxAge: 1h
# ---
---
# Source: stable-ha/templates/services/nats-service.yaml
# apiVersion: v1
# kind: Service
# metadata:
#   name: nats
#   labels:
#     app: release-name-nats
# spec:
#   type: ClusterIP
#   selector:
#     app.kubernetes.io/name: nats
#   ports:
#     - port: 4100
#       targetPort: 4100
#       protocol: TCP
#       name: api
---
# Source: stable-ha/templates/metallb/admin_ip.yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: dev-admin-ip
  namespace: metallb-system
spec:
  addresses:
  - /32
  autoAssign: false
status: {}
---
# Source: stable-ha/templates/metallb/primary_ip.yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: dev-primary-ip
  namespace: metallb-system
spec:
  addresses:
  - 192.168.0.235/32
  autoAssign: false
status: {}
---
# Source: stable-ha/templates/metallb/secondary_ip.yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: dev-secondary-ip
  namespace: metallb-system
spec:
  addresses:
  - 192.168.0.236/32
  autoAssign: false
status: {}
---
# Source: stable-ha/templates/metallb/l2.yaml
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: dev-l2-advert
  namespace: metallb-system
spec:
  ipAddressPools:
  - dev-primary-ip
  - dev-secondary-ip
  - dev-admin-ip
---
# Source: stable-ha/charts/nats/templates/tests/request-reply.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    helm.sh/hook: test
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
  labels:
    app.kubernetes.io/component: test-request-reply
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: nats
    app.kubernetes.io/version: 2.10.12
    helm.sh/chart: nats-1.1.10
  name: release-name-nats-test-request-reply
spec:
  containers:
  - args:
    - nats reply --echo echo & pid="$!"; sleep 1; nats request echo hi > /tmp/resp;
      kill "$pid"; wait; grep -qF hi /tmp/resp
    command:
    - sh
    - -ec
    - |
      work_dir="$(pwd)"
      mkdir -p "$XDG_CONFIG_HOME/nats"
      cd "$XDG_CONFIG_HOME/nats"
      if ! [ -s context ]; then
        ln -s /etc/nats-contexts context
      fi
      if ! [ -f context.txt ]; then
        echo -n "default" > context.txt
      fi
      cd "$work_dir"
      exec sh -ec "$0"
    image: natsio/nats-box:0.14.2
    name: nats-box
    volumeMounts:
    - mountPath: /etc/nats-contexts
      name: contexts
  enableServiceLinks: false
  restartPolicy: Never
  volumes:
  - name: contexts
    secret:
      secretName: release-name-nats-box-contexts

---
# Source: amundsen/templates/neo4j-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-amundsen-neo4j-configmap
  labels:
    app.kubernetes.io/name: amundsen
    app.kubernetes.io/component: neo4j
    helm.sh/chart: amundsen-1.0.0
    app.kubernetes.io/instance: release-name
    heritage: Helm
    app.kubernetes.io/managed-by: Helm
data:
  neo4j.conf: |-
    apoc.import.file.enabled=true
    cypher.forbid_shortestpath_common_nodes=false
    dbms.connector.bolt.enabled=true
    dbms.connector.http.enabled=true
    dbms.connector.https.enabled=true
    dbms.shell.enabled=true
    dbms.shell.host=0.0.0.0
    dbms.connectors.default_listen_address=0.0.0.0
    dbms.directories.import=/mnt
    dbms.jvm.additional=-Djdk.tls.ephemeralDHKeySize=2048
    dbms.jvm.additional=-Dunsupported.dbms.udc.source=tarball
    dbms.jvm.additional=-XX:+AlwaysPreTouch
    dbms.jvm.additional=-XX:+DisableExplicitGC
    dbms.jvm.additional=-XX:+UseG1GC
    dbms.logs.query.enabled=true
    dbms.logs.query.rotation.keep_number=7
    dbms.logs.query.rotation.size=20m
    dbms.memory.heap.initial_size=1G
    dbms.memory.heap.max_size=2G
    dbms.memory.pagecache.size=2G
    dbms.security.allow_csv_import_from_file_urls=true
    dbms.security.procedures.unrestricted=algo.*,apoc.*
    dbms.security.auth_enabled=false
    dbms.windows_service_name=neo4j
    apoc.export.file.enabled=true
    apoc.import.file.enabled=true
---
# Source: amundsen/charts/elasticsearch/templates/coordinating/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-elasticsearch-coordinating-hl
  namespace: "default"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: coordinating-only
spec:
  type: ClusterIP
  clusterIP: ""
  publishNotReadyAddresses: true
  ports:
    - name: tcp-rest-api
      port: 9200
      targetPort: rest-api
    - name: tcp-transport
      port: 9300
      targetPort: transport
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: coordinating-only
---
# Source: amundsen/charts/elasticsearch/templates/data/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-elasticsearch-data-hl
  namespace: "default"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: data
spec:
  type: ClusterIP
  clusterIP: ""
  publishNotReadyAddresses: true
  ports:
    - name: tcp-rest-api
      port: 9200
      targetPort: rest-api
    - name: tcp-transport
      port: 9300
      targetPort: transport
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: data
---
# Source: amundsen/charts/elasticsearch/templates/ingest/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-elasticsearch-ingest-hl
  namespace: "default"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ingest
spec:
  type: ClusterIP
  clusterIP: ""
  publishNotReadyAddresses: true
  ports:
    - name: tcp-rest-api
      port: 9200
      targetPort: rest-api
    - name: tcp-transport
      port: 9300
      targetPort: transport
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: ingest
---
# Source: amundsen/charts/elasticsearch/templates/master/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-elasticsearch-master-hl
  namespace: "default"
  labels: 
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  clusterIP: ""
  publishNotReadyAddresses: true
  ports:
    - name: tcp-rest-api
      port: 9200
      targetPort: rest-api
    - name: tcp-transport
      port: 9300
      targetPort: transport
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: master
---
# Source: amundsen/charts/elasticsearch/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-elasticsearch
  namespace: "default"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: coordinating-only
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-rest-api
      port: 9200
      targetPort: rest-api
      nodePort: null
    - name: tcp-transport
      port: 9300
      nodePort: null
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: coordinating-only
---
# Source: amundsen/templates/frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-amundsen-frontend
  labels:
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: amundsen
    helm.sh/chart: amundsen-1.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm   
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: amundsen
    app.kubernetes.io/instance: release-name
  ports:
    - name: amundsen-frontend-http
      port: 5000
      targetPort: frontendport
---
# Source: amundsen/templates/metadata-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-amundsen-metadata
  labels:
    app.kubernetes.io/component: metadata
    app.kubernetes.io/name: amundsen
    helm.sh/chart: amundsen-1.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm           
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/component: metadata
    app.kubernetes.io/name: amundsen
    app.kubernetes.io/instance: release-name
  ports:
    - name: amundsen-metadata-http
      port: 5002
      targetPort: metadataport
---
# Source: amundsen/templates/neo4j-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-amundsen-neo4j
  labels:
    app.kubernetes.io/component: neo4j
    app.kubernetes.io/name: amundsen-neo4j
    helm.sh/chart: amundsen-1.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm   
spec:
  type: 
  selector:
    app.kubernetes.io/component: neo4j
    app.kubernetes.io/name: amundsen-neo4j
    app.kubernetes.io/instance: release-name
  ports:
    - port: 7473
      name: neo4j-https
      targetPort: 7473
    - port: 7474
      name: neo4j-http
      targetPort: 7474
    - port: 7687
      name: neo4j-bolt
      targetPort: 7687
    - port: 1337
      name: neo4j-shell
      targetPort: 1337
---
# Source: amundsen/templates/search-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-amundsen-search
  labels:
    app.kubernetes.io/component: search
    app.kubernetes.io/name: amundsen
    helm.sh/chart: amundsen-1.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm   
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/component: search
    app.kubernetes.io/name: amundsen
    app.kubernetes.io/instance: release-name
  ports:
    - name: amundsen-search-http
      port: 5001
      targetPort: searchport
---
# Source: amundsen/templates/frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-amundsen-frontend
  labels:
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: amundsen
    helm.sh/chart: amundsen-1.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/name: amundsen
      app.kubernetes.io/instance: release-name
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/component: frontend
        app.kubernetes.io/name: amundsen
        app.kubernetes.io/instance: release-name
    spec:
      volumes:
      containers:
      - name: amundsen-frontend
        image: amundsendev/amundsen-frontend:2.1.1
        imagePullPolicy: Always
        ports:
          - name: frontendport
            containerPort: 5000
        env:
          - name: FRONTEND_BASE
            value: http://localhost
          - name: SEARCHSERVICE_BASE
            value: http://release-name-amundsen-search.default.svc.cluster.local:5001
          - name: METADATASERVICE_BASE
            value: http://release-name-amundsen-metadata.default.svc.cluster.local:5002
          - name: LONG_RANDOM_STRING
            value: "1234"

        livenessProbe:
          httpGet:
            path: "/healthcheck"
            port: 5000
          initialDelaySeconds: 60
          periodSeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        volumeMounts:
---
# Source: amundsen/templates/metadata-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-amundsen-metadata
  labels:
    app.kubernetes.io/component: metadata
    app.kubernetes.io/name: amundsen
    helm.sh/chart: amundsen-1.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm

spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: metadata
      app.kubernetes.io/name: amundsen
      helm.sh/chart: amundsen-1.0.0
      app.kubernetes.io/instance: release-name
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/component: metadata
        app.kubernetes.io/name: amundsen
        helm.sh/chart: amundsen-1.0.0
        app.kubernetes.io/instance: release-name
    spec:
      volumes:
      containers:
      - name: amundsen-metadata
        image: amundsendev/amundsen-metadata:2.5.4
        imagePullPolicy: Always
        ports:
        - name: metadataport
          containerPort: 5002
        env:
        - name: PROXY_HOST
          value: bolt://release-name-amundsen-neo4j.default.svc.cluster.local
        livenessProbe:
          httpGet:
            path: "/healthcheck"
            port: 5002
          initialDelaySeconds: 60
          periodSeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        volumeMounts:
---
# Source: amundsen/templates/neo4j-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-amundsen-neo4j
  labels:
    app.kubernetes.io/component: neo4j
    app.kubernetes.io/name: amundsen-neo4j
    helm.sh/chart: amundsen-1.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: amundsen-neo4j
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: neo4j
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: amundsen-neo4j
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/component: neo4j
    spec:
      initContainers:
        - name: init-neo4j-plugins
          image: "appropriate/curl:latest"
          imagePullPolicy: "IfNotPresent"
          command:
            - "/bin/sh"
            - "-c"
            - |
              curl -L https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/3.3.0.4/apoc-3.3.0.4-all.jar -O
              curl -L https://github.com/neo4j-contrib/neo4j-graph-algorithms/releases/download/3.3.5.0/graph-algorithms-algo-3.3.5.0.jar -O
              curl -L https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-core/1.11.250/aws-java-sdk-core-1.11.250.jar -O
              curl -L https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-s3/1.11.250/aws-java-sdk-s3-1.11.250.jar -O
              curl -L https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.4/httpclient-4.5.4.jar -O
              curl -L https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.8/httpcore-4.4.8.jar -O
              curl -L https://repo1.maven.org/maven2/joda-time/joda-time/2.9.9/joda-time-2.9.9.jar -O
              chmod 755 *.jar
              mv *.jar /var/lib/neo4j/plugins
          volumeMounts:
            - name: plugins
              mountPath: /var/lib/neo4j/plugins
      containers:
      - name: neo4j
        image: neo4j:3.3.0
        ports:
        - containerPort: 7474
        - containerPort: 7687
        - containerPort: 1337
        env:
          - name: NEO4J_CONF
            value: "/conf"
        volumeMounts:
        - name: conf
          mountPath: /conf
        - name: plugins
          mountPath: /var/lib/neo4j/plugins
      volumes:
        - name: conf
          configMap:
            name: release-name-amundsen-neo4j-configmap
        - name: plugins
          hostPath:
            path: "/mnt/ephemeral/neo4j/plugins"
            type: DirectoryOrCreate
---
# Source: amundsen/templates/search-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-amundsen-search
  labels:
    app.kubernetes.io/component: search
    app.kubernetes.io/name: amundsen
    helm.sh/chart: amundsen-1.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/component: search
      app.kubernetes.io/name: amundsen
      app.kubernetes.io/instance: release-name
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/component: search
        app.kubernetes.io/name: amundsen
        app.kubernetes.io/instance: release-name
    spec:
      containers:
      - name: amundsen-search
        image: amundsendev/amundsen-search:2.4.0
        ports:
        - name: searchport
          containerPort: 5001
        env:
        - name: PROXY_ENDPOINT
          value: release-name-elasticsearch-client.default.svc.cluster.local
        livenessProbe:
          httpGet:
            path: "/healthcheck"
            port: 5001
          initialDelaySeconds: 60
          periodSeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
---
# Source: amundsen/charts/elasticsearch/templates/coordinating/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-elasticsearch-coordinating
  namespace: "default"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: coordinating-only
    ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
    app: coordinating-only
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: coordinating-only
  updateStrategy:
    type: RollingUpdate
  serviceName: release-name-elasticsearch-coordinating-hl
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-19.5.6
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: coordinating-only
        ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
        app: coordinating-only
      annotations:
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        ## Image that performs the sysctl operation to modify Kernel settings (needed sometimes to avoid boot errors)
        - name: sysctl
          image: docker.io/bitnami/bitnami-shell:11-debian-11-r68
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
            runAsUser: 0
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: docker.io/bitnami/elasticsearch:8.5.3-debian-11-r9
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: "elastic"
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "yes"
            - name: ELASTICSEARCH_NODE_ROLES
              value: ""
            - name: ELASTICSEARCH_TRANSPORT_PORT_NUMBER
              value: "9300"
            - name: ELASTICSEARCH_HTTP_PORT_NUMBER
              value: "9200"
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: "release-name-elasticsearch-master-hl.default.svc.cluster.local,release-name-elasticsearch-coordinating-hl.default.svc.cluster.local,release-name-elasticsearch-data-hl.default.svc.cluster.local,release-name-elasticsearch-ingest-hl.default.svc.cluster.local,"
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "2"
            - name: ELASTICSEARCH_CLUSTER_MASTER_HOSTS
              value: release-name-elasticsearch-master-0 
            - name: ELASTICSEARCH_MINIMUM_MASTER_NODES
              value: "1"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).release-name-elasticsearch-coordinating-hl.default.svc.cluster.local"
            - name: ELASTICSEARCH_HEAP_SIZE
              value: "128m"
          ports:
            - name: rest-api
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits: {}
            requests:
              cpu: 25m
              memory: 256Mi
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes:
        - name: "data"
          emptyDir: {}
---
# Source: amundsen/charts/elasticsearch/templates/data/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-elasticsearch-data
  namespace: "default"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: data
    ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
    app: data
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: data
  serviceName: release-name-elasticsearch-data-hl
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-19.5.6
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: data
        ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
        app: data
      annotations:
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        ## Image that performs the sysctl operation to modify Kernel settings (needed sometimes to avoid boot errors)
        - name: sysctl
          image: docker.io/bitnami/bitnami-shell:11-debian-11-r68
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
            runAsUser: 0
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: docker.io/bitnami/elasticsearch:8.5.3-debian-11-r9
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "yes"
            - name: ELASTICSEARCH_NODE_ROLES
              value: "data"
            - name: ELASTICSEARCH_TRANSPORT_PORT_NUMBER
              value: "9300"
            - name: ELASTICSEARCH_HTTP_PORT_NUMBER
              value: "9200"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: "elastic"
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: "release-name-elasticsearch-master-hl.default.svc.cluster.local,release-name-elasticsearch-coordinating-hl.default.svc.cluster.local,release-name-elasticsearch-data-hl.default.svc.cluster.local,release-name-elasticsearch-ingest-hl.default.svc.cluster.local,"
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "2"
            - name: ELASTICSEARCH_CLUSTER_MASTER_HOSTS
              value: release-name-elasticsearch-master-0 
            - name: ELASTICSEARCH_MINIMUM_MASTER_NODES
              value: "1"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).release-name-elasticsearch-data-hl.default.svc.cluster.local"
            - name: ELASTICSEARCH_HEAP_SIZE
              value: "1024m"
          ports:
            - name: rest-api
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits: {}
            requests:
              cpu: 25m
              memory: 2048Mi
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: "data"
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: amundsen/charts/elasticsearch/templates/ingest/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-elasticsearch-ingest
  namespace: "default"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ingest
    ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
    app: ingest
spec:
  replicas: 2
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: ingest
  serviceName: release-name-elasticsearch-ingest-hl
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-19.5.6
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: ingest
        ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
        app: ingest
      annotations:
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        ## Image that performs the sysctl operation to modify Kernel settings (needed sometimes to avoid boot errors)
        - name: sysctl
          image: docker.io/bitnami/bitnami-shell:11-debian-11-r68
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
            runAsUser: 0
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: docker.io/bitnami/elasticsearch:8.5.3-debian-11-r9
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "yes"
            - name: ELASTICSEARCH_NODE_ROLES
              value: "ingest"
            - name: ELASTICSEARCH_TRANSPORT_PORT_NUMBER
              value: "9300"
            - name: ELASTICSEARCH_HTTP_PORT_NUMBER
              value: "9200"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: "elastic"
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: "release-name-elasticsearch-master-hl.default.svc.cluster.local,release-name-elasticsearch-coordinating-hl.default.svc.cluster.local,release-name-elasticsearch-data-hl.default.svc.cluster.local,release-name-elasticsearch-ingest-hl.default.svc.cluster.local,"
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "2"
            - name: ELASTICSEARCH_CLUSTER_MASTER_HOSTS
              value: release-name-elasticsearch-master-0 
            - name: ELASTICSEARCH_MINIMUM_MASTER_NODES
              value: "1"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).release-name-elasticsearch-ingest-hl.default.svc.cluster.local"
            - name: ELASTICSEARCH_HEAP_SIZE
              value: "128m"
          ports:
            - name: rest-api
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits: {}
            requests:
              cpu: 25m
              memory: 256Mi
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes:
        - name: "data"
          emptyDir: {}
---
# Source: amundsen/charts/elasticsearch/templates/master/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-elasticsearch-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.6
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
    ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
    app: master
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: master
  serviceName: release-name-elasticsearch-master-hl
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-19.5.6
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
        ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
        app: master
      annotations:
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        ## Image that performs the sysctl operation to modify Kernel settings (needed sometimes to avoid boot errors)
        - name: sysctl
          image: docker.io/bitnami/bitnami-shell:11-debian-11-r68
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
            runAsUser: 0
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: docker.io/bitnami/elasticsearch:8.5.3-debian-11-r9
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "yes"
            - name: ELASTICSEARCH_NODE_ROLES
              value: "master"
            - name: ELASTICSEARCH_TRANSPORT_PORT_NUMBER
              value: "9300"
            - name: ELASTICSEARCH_HTTP_PORT_NUMBER
              value: "9200"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: "elastic"
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: "release-name-elasticsearch-master-hl.default.svc.cluster.local,release-name-elasticsearch-coordinating-hl.default.svc.cluster.local,release-name-elasticsearch-data-hl.default.svc.cluster.local,release-name-elasticsearch-ingest-hl.default.svc.cluster.local,"
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "2"
            - name: ELASTICSEARCH_CLUSTER_MASTER_HOSTS
              value: release-name-elasticsearch-master-0 
            - name: ELASTICSEARCH_MINIMUM_MASTER_NODES
              value: "1"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).release-name-elasticsearch-master-hl.default.svc.cluster.local"
            - name: ELASTICSEARCH_HEAP_SIZE
              value: "128m"
          ports:
            - name: rest-api
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits: {}
            requests:
              cpu: 25m
              memory: 256Mi
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: "data"
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"

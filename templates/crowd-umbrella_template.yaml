---
# Source: crowd-umbrella/charts/crowd/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-crowd
  labels:
    helm.sh/chart: crowd-1.0.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.3.5-jdk11"
    app.kubernetes.io/managed-by: Helm
---
# Source: crowd-umbrella/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-postgresql
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-10.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  namespace: default
type: Opaque
data:
  postgresql-postgres-password: "Y3Jvd2Q="
  postgresql-password: "Y3Jvd2Q="
---
# Source: crowd-umbrella/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-crowd-secrets
  labels:
    helm.sh/chart: crowd-umbrella-1.0.2
    app.kubernetes.io/name: crowd-umbrella
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.3.5-jdk11"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  postgresql-password: "Y3Jvd2Q="
---
# Source: crowd-umbrella/charts/crowd/templates/config-jvm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-crowd-jvm-config
  labels:
    helm.sh/chart: crowd-1.0.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.3.5-jdk11"
    app.kubernetes.io/managed-by: Helm
    
data:
  additional_jvm_args: >-
    -Dcrowd.cluster.hazelcast.listenPort=5701
    -Dcrowd.clusterNodeName.useHostname=true
    -Datlassian.logging.cloud.enabled=false
    -XX:ActiveProcessorCount=2
  max_heap: 768m
  min_heap: 384m
---
# Source: crowd-umbrella/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-crowd-server-config
  labels:
    helm.sh/chart: crowd-umbrella-1.0.2
    app.kubernetes.io/name: crowd-umbrella
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.3.5-jdk11"
    app.kubernetes.io/managed-by: Helm
data:
  restore-db.sh: |-
    #! /bin/bash -xe
    until timeout 1 bash -c 'cat < /dev/null > /dev/tcp/release-name-postgresql/5432'; do echo "WAITING: db host release-name-postgresql port 5432"; sleep 1; done
    psql -h release-name-postgresql -p 5432 -U crowd -d crowd -c 'SELECT 1 FROM cwd_user' && exit 0
    unset PGPASSWORD

  crowd.cfg.xml: |-
    <?xml version="1.0" encoding="UTF-8"?>

    <application-configuration>
    <setupStep>complete</setupStep>
    <setupType>install.new</setupType>
    <buildNumber>1581</buildNumber>
    <properties>
        <property name="crowd.clustering.enabled">false</property>
        <property name="crowd.server.id">AB0C-1D2E-FGHI-JKL3</property>
        <property name="hibernate.c3p0.acquire_increment">1</property>
        <property name="hibernate.c3p0.idle_test_period">100</property>
        <property name="hibernate.c3p0.max_size">30</property>
        <property name="hibernate.c3p0.max_statements">0</property>
        <property name="hibernate.c3p0.min_size">0</property>
        <property name="hibernate.c3p0.timeout">30</property>
        <property name="hibernate.connection.driver_class">org.postgresql.Driver</property>
        <property name="hibernate.connection.url">jdbc:postgresql://release-name-postgresql:5432/crowd?reWriteBatchedInserts=true</property>
        <property name="hibernate.connection.username">crowd</property>
        <property name="hibernate.connection.password">crowd</property>
        <property name="hibernate.dialect">org.hibernate.dialect.PostgreSQLDialect</property>
        <property name="hibernate.setup">true</property>
        <property name="license">AAABgQ0ODAoPeNp1kstuwjAQRff5CktdB4XXAqQsIHELLS+RQCvUjWMGcBvsaJxA+fuaQNQkopJX19d3zsz4aaokCSAhzQ5pdvutVr/dJTQISctxelbEjpFSjYngIDXQrUiFki6dhXS5WI4Dau2EPsAFCkd4ScB9Nhq9wGef+HCCWCWAFo/VCbBi83Kp4pplxwhwvltpQO12La7krsF4Kk7gppiBtciQH5gGn6XgXgltc5odq5Q7Y0dwfbqmk/mCLosb+pMIvOTPFu1R0Vc5OgA0OGPfHb70Qvtjte7Yb5vNyB46zXfrSyCrwL+OlwNCZQqYoNC1Xq/UlU6NEGcg+QNf0bIXZ9qkzdQWtOvUBp+nDHPpv6Jlwgd74phxEcXVRXl3sRI0ZcJUkMzQ1obGUZ23tQCjVF7fb80YV/JbqrO05rhnUmiWEw3SmGktmPwDKu/AQ8h99fXeKpedxb8raz5ojiLJC4WgUxLfYMhOIUnibC8k2Rak+jax8vv7Hy1Lv3FyG1gwLAIUHEP4zbqEf1DeOWKb7SpxQ1MEd8ACFGxZV5doE27OtsuNDT50k2xIhAozX02im</property>
    </properties>
    </application-configuration>

binaryData:
  javax.crypto.spec.SecretKeySpec: "rO0ABXNyAB9qYXZheC5jcnlwdG8uc3BlYy5TZWNyZXRLZXlTcGVjW0cLZuIwYU0CAAJMAAlhbGdvcml0aG10ABJMamF2YS9sYW5nL1N0cmluZztbAANrZXl0AAJbQnhwdAADQUVTdXIAAltCrPMX+AYIVOACAAB4cAAAABBNSpMiggVtJ0tCP6lXt8VF"
---
# Source: crowd-umbrella/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-crowd-dump-config
  labels:
    helm.sh/chart: crowd-umbrella-1.0.2
    app.kubernetes.io/name: crowd-umbrella
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.3.5-jdk11"
    app.kubernetes.io/managed-by: Helm
data:
  db.dump: |-
    -- Dumped from database version 11.12
    -- Dumped by pg_dump version 11.12
---
# Source: crowd-umbrella/charts/crowd/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-crowd
  labels:
    helm.sh/chart: crowd-1.0.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.3.5-jdk11"
    app.kubernetes.io/managed-by: Helm
    
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
    - port: 5701
      targetPort: hazelcast
      protocol: TCP
      name: hazelcast
  selector:
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: release-name
---
# Source: crowd-umbrella/charts/postgresql/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql-headless
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-10.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  namespace: default
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: release-name
---
# Source: crowd-umbrella/charts/postgresql/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-10.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
  namespace: default
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: release-name
    role: primary
---
# Source: crowd-umbrella/charts/crowd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-crowd
  labels:
    helm.sh/chart: crowd-1.0.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.3.5-jdk11"
    app.kubernetes.io/managed-by: Helm
    
spec:
  replicas: 1
  serviceName: release-name-crowd
  selector:
    matchLabels:
      app.kubernetes.io/name: crowd
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: crowd
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-crowd
      terminationGracePeriodSeconds: 30
      
      securityContext:
        # This is intended to ensure that the shared-home volume is group-writeable by the GID used by the Crowd container.
        # However, this doesn't appear to work for NFS volumes due to a K8s bug: https://github.com/kubernetes/examples/issues/260
        fsGroup: 2004
      
      initContainers:
        # -- Additional initContainer to copy the Crowd configuration.
        # Copies crowd.cfg.xml file to the proper location.
        # Copies Crypto key generated by the initial DB setup.
        # The key belongs to the database and must have his specific unique ending.
        # Missing this key would cause errors by Crowd server
        - name: copy-config
          image: busybox
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
          args:
            - '-c'
            - >-
              set -x &&
              cp /tmp/crowd.cfg.xml /var/atlassian/application-data/crowd/shared/crowd.cfg.xml &&
              mkdir -p /var/atlassian/application-data/crowd/shared/keys/ &&
              cp /tmp/key/javax.crypto.spec.SecretKeySpec
              /var/atlassian/application-data/crowd/shared/keys/javax.crypto.spec.SecretKeySpec_1619611916201
          resources: {}
          volumeMounts:
            - name: server-config
              mountPath: /tmp/crowd.cfg.xml
              subPath: crowd.cfg.xml
            - name: shared-home
              mountPath: /var/atlassian/application-data/crowd/shared/
            - name: crowd-key
              mountPath: /tmp/key
        # -- Additional initContainer to load initial Crowd database.
        # The initial Crowd setup was performed in order to connect to a ready Postgresql database.
        # After the chart deployment a default Crowd user is able immediately to login without init routine
        - name: init-db
          image: docker.io/bitnami/postgresql:11
          imagePullPolicy: IfNotPresent
          resources: {}
          volumeMounts:
            - name: server-config
              mountPath: /tmp/restore-db.sh
              subPath: restore-db.sh
            - name: dump-config
              mountPath: /tmp/db.dump
              subPath: db.dump
          env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-crowd-secrets
                  key: postgresql-password
          args: ['/tmp/restore-db.sh']
        - name: plugins
          image: ghcr.io/aservo/mapi:0.0.3
          imagePullPolicy: Always
          resources: {}
          volumeMounts:
            - name: shared-home
              mountPath: /mnt
          command:
            - /bin/sh
          args:
            - '-c'
            - >-
              set -x &&
              echo "Start installing plugins..." &&
              mkdir -p /mnt/plugins &&
              mkdir -p /mnt/additional-plugins &&
              ./cleanup.sh /mnt/plugins jar &&
              echo "Finish installing plugins..."
        - name: nfs-permission-fixer
          image: alpine
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0 # make sure we run as root so we get the ability to change the volume permissions
          volumeMounts:
            - name: shared-home
              mountPath: "/shared-home"
          command: ["sh", "-c", "(chgrp 2004 /shared-home; chmod g+w /shared-home)"]
      containers:
        - name: crowd
          image: "atlassian/crowd:4.3.5-jdk11"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8095
              protocol: TCP
            - name: hazelcast
              containerPort: 5701
              protocol: TCP
          readinessProbe:
            httpGet:
              port: 8095
              path: /
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 30
          resources:
            requests:
              cpu: "2"
              memory: 1G
          volumeMounts:
            
            - name: local-home
              mountPath: "/var/atlassian/application-data/crowd"
            - name: local-home
              mountPath: "/opt/atlassian/crowd/logs"
              subPath: "logs"
            - name: shared-home
              mountPath: "/var/atlassian/application-data/crowd/shared"
            
            
            
            
          env:
            
            - name: ATL_TOMCAT_SCHEME
              value: "https"
            - name: ATL_TOMCAT_SECURE
              value: "true"
            
            - name: ATL_TOMCAT_ACCESS_LOG
              value: "true"
            - name: UMASK
              value: "0022"
            - name: SET_PERMISSIONS
              value: "true"
            - name: ATL_PRODUCT_HOME_SHARED
              value: "/var/atlassian/application-data/crowd/shared"
            - name: JVM_MINIMUM_MEMORY
              valueFrom:
                configMapKeyRef:
                  key: min_heap
                  name: release-name-crowd-jvm-config
            - name: JVM_MAXIMUM_MEMORY
              valueFrom:
                configMapKeyRef:
                  key: max_heap
                  name: release-name-crowd-jvm-config
            
          lifecycle:
            preStop:
              exec:
                command: ["sh", "-c", "/shutdown-wait.sh"]
        
        
        
      volumes:
        
        
        
        - name: local-home
        
          emptyDir: {}
        - name: shared-home
        
          emptyDir: {}
        # -- Volume with additional configuration files
        - name: server-config
          configMap:
            name: release-name-crowd-server-config
            items:
            - key: restore-db.sh
              path: restore-db.sh
              mode: 0755
            - key: crowd.cfg.xml
              path: crowd.cfg.xml
              mode: 0755
        # -- Volume with additional encryption key
        - name: crowd-key
          configMap:
            name: release-name-crowd-server-config
            items:
            - key: javax.crypto.spec.SecretKeySpec
              path: javax.crypto.spec.SecretKeySpec
              mode: 0755
        # -- Volume with additional dump file for SQL import to the database
        - name: dump-config
          configMap:
            name: release-name-crowd-dump-config
            items:
            - key: db.dump
              path: db.dump
---
# Source: crowd-umbrella/charts/postgresql/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-postgresql
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-10.3.17
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
  namespace: default
spec:
  serviceName: release-name-postgresql-headless
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/instance: release-name
      role: primary
  template:
    metadata:
      name: release-name-postgresql
      labels:
        app.kubernetes.io/name: postgresql
        helm.sh/chart: postgresql-10.3.17
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        role: primary
        app.kubernetes.io/component: primary
    spec:      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: primary
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      containers:
        - name: release-name-postgresql
          image: docker.io/bitnami/postgresql:11.11.0-debian-10-r62
          imagePullPolicy: "IfNotPresent"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-postgresql
                  key: postgresql-postgres-password
            - name: POSTGRES_USER
              value: "crowd"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-postgresql
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "crowd"
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "crowd" -d "dbname=crowd" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "crowd" -d "dbname=crowd" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
              subPath: 
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: crowd-umbrella/charts/crowd/templates/tests/test-application-status.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-crowd-application-status-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: crowd-1.0.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.3.5-jdk11"
    app.kubernetes.io/managed-by: Helm
    
spec:
  containers:
    - name: test
      image: alpine
      env:
        - name: STATUS_URL
          value: "http://release-name-crowd:80/crowd/status"
      command:
        - /bin/sh
        - -ec
        - |
          apk add -q jq curl
          STATUS=$(curl -s "$STATUS_URL")
          echo "Verifying application state is RUNNING or FIRST_RUN: $STATUS"
          echo $STATUS | jq -e '.state|test("RUNNING|FIRST_RUN")'
  restartPolicy: Never
---
# Source: crowd-umbrella/charts/crowd/templates/tests/test-shared-home-permissions.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-crowd-shared-home-permissions-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: crowd-1.0.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.3.5-jdk11"
    app.kubernetes.io/managed-by: Helm
    
spec:
  containers:
    - name: test
      image: debian:stable-slim
      imagePullPolicy: IfNotPresent
      securityContext:
        # Slightly dodgy; we assume that the UID and GID used by the product images are the same, which in practice they are
        runAsUser: 2004
        runAsGroup: 2004
      volumeMounts:
        - name: shared-home
          mountPath: /shared-home
      command:
        - /bin/sh
        - -ec
        - |
          ls -ld /shared-home
          echo "Creating temporary file in shared home as user $(id -u):$(id -g)"
          touch /shared-home/permissions-test
          ls -l /shared-home/permissions-test
          rm /shared-home/permissions-test
  volumes:
    
    - name: shared-home
    
      emptyDir: {}
  restartPolicy: Never
---
# Source: crowd-umbrella/charts/crowd/templates/nfs-permission-fixer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-crowd-nfs-fixer
  labels:
    helm.sh/chart: crowd-1.0.0
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "4.3.5-jdk11"
    app.kubernetes.io/managed-by: Helm
    
  annotations:
    "helm.sh/hook": pre-install
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
spec:
  template:
    metadata:
      name: release-name-crowd-nfs-fixer
      labels:
        helm.sh/chart: crowd-1.0.0
        app.kubernetes.io/name: crowd
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "4.3.5-jdk11"
        app.kubernetes.io/managed-by: Helm
        
    spec:
      restartPolicy: Never
      containers:
        - name: nfs-fixer
          image: alpine
          securityContext:
            runAsUser: 0 # make sure we run as root so we get the ability to change the volume permissions
          volumeMounts:
            - name: shared-home
              mountPath: "/shared-home"
          command: ["sh", "-c", "(chgrp 2004 /shared-home; chmod g+w /shared-home)"]
      volumes:
        
        - name: shared-home
        
          emptyDir: {}

---
# Source: one-green-core/charts/grafana/templates/podsecuritypolicy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-grafana
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default'
    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'
    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    # Default set from Docker, with DAC_OVERRIDE and CHOWN
      - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'csi'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false
---
# Source: one-green-core/charts/grafana/templates/tests/test-podsecuritypolicy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-grafana-test
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
spec:
  allowPrivilegeEscalation: true
  privileged: false
  hostNetwork: false
  hostIPC: false
  hostPID: false
  fsGroup:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  volumes:
  - configMap
  - downwardAPI
  - emptyDir
  - projected
  - csi
  - secret
---
# Source: one-green-core/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
  name: release-name-grafana
  namespace: default
---
# Source: one-green-core/charts/grafana/templates/tests/test-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
  name: release-name-grafana-test
  namespace: default
---
# Source: one-green-core/charts/rabbitmq/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-10.1.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
secrets:
  - name: release-name-rabbitmq
---
# Source: one-green-core/charts/redis/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: release-name-redis
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.11.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: one-green-core/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  admin-user: "YWRtaW4="
  admin-password: "RG9sRW1FdGhab2RzcDFRcm5HMlRKOGhlcUNib1BuQmc4RW5XbURPcQ=="
  ldap-toml: ""
---
# Source: one-green-core/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  postgres-password: "YW55cmFuZG9tcGFzc3dvcmQ="
  password: "ZUw1dkoyZUcweQ=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: one-green-core/charts/rabbitmq/templates/config-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-rabbitmq-config
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-10.1.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  rabbitmq.conf: |-
    IyMgVXNlcm5hbWUgYW5kIHBhc3N3b3JkCiMjCmRlZmF1bHRfdXNlciA9IG1xdHQKZGVmYXVsdF9wYXNzID0gQ0hBTkdFTUUKIyMgQ2x1c3RlcmluZwojIwpjbHVzdGVyX2Zvcm1hdGlvbi5wZWVyX2Rpc2NvdmVyeV9iYWNrZW5kICA9IHJhYmJpdF9wZWVyX2Rpc2NvdmVyeV9rOHMKY2x1c3Rlcl9mb3JtYXRpb24uazhzLmhvc3QgPSBrdWJlcm5ldGVzLmRlZmF1bHQKY2x1c3Rlcl9mb3JtYXRpb24ubm9kZV9jbGVhbnVwLmludGVydmFsID0gMTAKY2x1c3Rlcl9mb3JtYXRpb24ubm9kZV9jbGVhbnVwLm9ubHlfbG9nX3dhcm5pbmcgPSB0cnVlCmNsdXN0ZXJfcGFydGl0aW9uX2hhbmRsaW5nID0gYXV0b2hlYWwKIyBxdWV1ZSBtYXN0ZXIgbG9jYXRvcgpxdWV1ZV9tYXN0ZXJfbG9jYXRvciA9IG1pbi1tYXN0ZXJzCiMgZW5hYmxlIGd1ZXN0IHVzZXIKbG9vcGJhY2tfdXNlcnMuZ3Vlc3QgPSBmYWxzZQojZGVmYXVsdF92aG9zdCA9IGRlZmF1bHQtdmhvc3QKI2Rpc2tfZnJlZV9saW1pdC5hYnNvbHV0ZSA9IDUwTUI=
---
# Source: one-green-core/charts/rabbitmq/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-10.1.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  rabbitmq-password: "YW55cmFuZG9tcGFzc3dvcmQ="
  
  rabbitmq-erlang-cookie: "c2VjcmV0Y29va2ll"
---
# Source: one-green-core/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "release-name-secrets"
type: Opaque
data:
  # PostgreSQL db user and password definition
  POSTGRES_USER: "cG9zdGdyZXM="
  POSTGRES_PASSWORD: "YW55cmFuZG9tcGFzc3dvcmQ="
  # influxdb
  DOCKER_INFLUXDB_INIT_USERNAME: "YWRtaW4="
  DOCKER_INFLUXDB_INIT_PASSWORD: "YW55cmFuZG9tcGFzc3dvcmQ="
  DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: "Y2hhbmdlX3RoaXNfdG9rZW4="
  # DJANGO ADMIN PASSWORD
  DJANGO_ADMIN_USERNAME: "YWRtaW4="
  DJANGO_ADMIN_PASSWORD: "YWRtaW4="
---
# Source: one-green-core/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
data:
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning

  datasources.yaml: |
    apiVersion: 1
    datasources:
    - access: proxy
      jsonData:
        defaultBucket: 'release-name-bucket'
        organization: 'release-name'
        tlsSkipVerify: true
        version: Flux
      name: InfluxDB_v2_Flux
      secureJsonData:
        token: change_this_token
      type: influxdb
      url: http://release-name-influxdb:8086
---
# Source: one-green-core/charts/grafana/templates/tests/test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-grafana-test
  namespace: default
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
data:
  run.sh: |-
    @test "Test Health" {
      url="http://release-name-grafana/api/health"

      code=$(wget --server-response --spider --timeout 10 --tries 1 ${url} 2>&1 | awk '/^  HTTP/{print $2}')
      [ "$code" == "200" ]
    }
---
# Source: one-green-core/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-redis-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.11.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    slave-read-only yes
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: one-green-core/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-redis-health
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.11.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: one-green-core/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-redis-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.11.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
  start-replica.sh: |
    #!/bin/bash

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        echo "${hostname}.${HEADLESS_SERVICE}"
    }

    REDISPORT=$(get_port "$HOSTNAME" "REDIS")

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
    fi
    if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi

    echo "" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-port $REDISPORT" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis/etc/replica.conf
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--slaveof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: one-green-core/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-name-config"
data:
  # Set timezone UTC by default
  TZ: UTC
  # postgres configuration
  POSTGRES_HOST: "release-name-postgresql"
  POSTGRES_PORT: "5432"
  POSTGRES_DB: "one_green"
  # mqtt configuration
  MQTT_HOST: "release-name-mqtt"
  MQTT_PORT: "1883"
  # influxdb configuration
  INFLUXDB_HOST: "release-name-influxdb"
  INFLUXDB_PORT: "8086"
  INFLUXDB_URL: "http://release-name-influxdb:8086"
  DOCKER_INFLUXDB_INIT_MODE: setup  # create user authorisation without using web-ui
  DOCKER_INFLUXDB_INIT_ORG: "release-name"
  DOCKER_INFLUXDB_INIT_BUCKET: "release-name-bucket"
  DOCKER_INFLUXDB_INIT_RETENTION: "1w"
  # REDIS (for Celery IoT async controller)
  REDIS_HOST: "release-name-redis-headless"
  REDIS_PORT: "6379"
  # Django config
  # internal service
  ONE_GREEN_API_HOST: "release-name-api"
  ONE_GREEN_API_PORT: "8080"
  ONE_GREEN_API_URL: "http://release-name-api:8080"
  # django settings.py
  CSRF_TRUSTED_ORIGINS: "https://api.dev1.og-ingest1.com"
  ALLOWED_HOSTS: "api.dev1.og-ingest1.com"
  DEBUG: "false"
---
# Source: one-green-core/templates/telegraf-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: "release-name-telegraf-config"
data:
  # Configuration for
  # mqtt consumer to influx db sink
  telegraf.conf: |
    [global_tags]
    [agent]
      interval = "1s"
      round_interval = true
      metric_batch_size = 1000
      metric_buffer_limit = 10000
      collection_jitter = "0s"
      flush_interval = "10s"
      flush_jitter = "0s"
      precision = ""
      debug = false
      hostname = ""
      omit_hostname = false

    # output
    [[outputs.influxdb_v2]]
      urls = ["$INFLUXDB_URL"]
      token = "$DOCKER_INFLUXDB_INIT_ADMIN_TOKEN"
      organization = "$DOCKER_INFLUXDB_INIT_ORG"
      bucket = "$DOCKER_INFLUXDB_INIT_BUCKET"

    # input influx line protocol format
    [[inputs.mqtt_consumer]]
       servers = ["tcp://$MQTT_HOST:1883"]
       topics = [
                  "water/sensor",
                  "sprinkler/sensor",
                  "cooler/sensor",
                  "heater/sensor",
                  "air_humidifier/sensor",
                ]
       qos = 0
       username = "$MQTT_USERNAME"
       password = "$MQTT_PASSWORD"
       data_format = "influx"
    # input json format
    [[inputs.mqtt_consumer]]
       name_override = "controllers"
       servers = ["tcp://$MQTT_HOST:1883"]
       topics = [
                  "water/controller/+",
                  "sprinkler/controller/+",
                  "cooler/controller/+",
                  "heater/controller/+",
                  "air_humidifier/controller/+"
                ]
       qos = 0
       username = "$MQTT_USERNAME"
       password = "$MQTT_PASSWORD"
       data_format = "json"
       tag_keys = [
        "controller_type",
        "tag"
        ]
---
# Source: one-green-core/charts/grafana/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
  finalizers:
    - kubernetes.io/pvc-protection
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "1Gi"
  storageClassName: scw-bssd
---
# Source: one-green-core/charts/grafana/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
  name: release-name-grafana-clusterrole
rules: []
---
# Source: one-green-core/charts/grafana/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-grafana-clusterrolebinding
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: release-name-grafana
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: one-green-core/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:      ['extensions']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [release-name-grafana]
---
# Source: one-green-core/charts/grafana/templates/tests/test-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-grafana-test
  namespace: default
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:      ['policy']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [release-name-grafana-test]
---
# Source: one-green-core/charts/rabbitmq/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-rabbitmq-endpoint-reader
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-10.1.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create"]
---
# Source: one-green-core/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-grafana
subjects:
- kind: ServiceAccount
  name: release-name-grafana
  namespace: default
---
# Source: one-green-core/charts/grafana/templates/tests/test-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-grafana-test
  namespace: default
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-grafana-test
subjects:
- kind: ServiceAccount
  name: release-name-grafana-test
  namespace: default
---
# Source: one-green-core/charts/rabbitmq/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-rabbitmq-endpoint-reader
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-10.1.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: release-name-rabbitmq
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-rabbitmq-endpoint-reader
---
# Source: one-green-core/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000

  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
---
# Source: one-green-core/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql-hl
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: primary
---
# Source: one-green-core/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: primary
---
# Source: one-green-core/charts/rabbitmq/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-rabbitmq-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-10.1.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  ports:
    - name: epmd
      port: 4369
      targetPort: epmd
    - name: amqp
      port: 5672
      targetPort: amqp
    - name: dist
      port: 25672
      targetPort: dist
    - name: http-stats
      port: 15672
      targetPort: stats
  selector: 
    app.kubernetes.io/name: rabbitmq
    app.kubernetes.io/instance: release-name
  publishNotReadyAddresses: true
---
# Source: one-green-core/charts/rabbitmq/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-10.1.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: amqp
      port: 5672
      targetPort: amqp
      nodePort: null
    - name: epmd
      port: 4369
      targetPort: epmd
      nodePort: null
    - name: dist
      port: 25672
      targetPort: dist
      nodePort: null
    - name: http-stats
      port: 15672
      targetPort: stats
      nodePort: null
  selector: 
    app.kubernetes.io/name: rabbitmq
    app.kubernetes.io/instance: release-name
---
# Source: one-green-core/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-redis-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.11.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
---
# Source: one-green-core/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.11.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: master
---
# Source: one-green-core/charts/redis/templates/replicas/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.11.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: replica
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: replica
---
# Source: one-green-core/templates/api-gateway-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: "release-name-api"
spec:
  selector:
    app: "release-name-api"
  ports:
    - port: 8080
  type: ClusterIP
---
# Source: one-green-core/templates/influxdb-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: "release-name-influxdb"
spec:
  selector:
    app: "release-name-influxdb"
  ports:
    - port: 8086
  type: ClusterIP
---
# Source: one-green-core/templates/rabbitmqtt-extra-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-mqtt
spec:
  selector:
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/name: rabbitmq
  
  type: NodePort
  
  ports:
    - port: 1883
      
      nodePort: 30182
---
# Source: one-green-core/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: release-name
      annotations:
        checksum/config: 8e6076ca99376a68dea38a19670b964dc1b307ef60c05089733899bb924ff038
        checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/sc-dashboard-provider-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/secret: 6a03faee1593139893ce15d9076e46a0d70bdef5385a17eccbdd4ba0ca577e2a
    spec:
      
      serviceAccountName: release-name-grafana
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsUser: 472
      initContainers:
        - name: init-chown-data
          image: "busybox:1.31.1"
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: false
            runAsUser: 0
          command: ["chown", "-R", "472:472", "/var/lib/grafana"]
          resources:
            {}
          volumeMounts:
            - name: storage
              mountPath: "/var/lib/grafana"
      enableServiceLinks: true
      containers:
        - name: grafana
          image: "grafana/grafana:8.5.3"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: config
              mountPath: "/etc/grafana/provisioning/datasources/datasources.yaml"
              subPath: "datasources.yaml"
          ports:
            - name: service
              containerPort: 80
              protocol: TCP
            - name: grafana
              containerPort: 3000
              protocol: TCP
          env:
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: release-name-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-grafana
                  key: admin-password
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: release-name-grafana
        - name: storage
          persistentVolumeClaim:
            claimName: release-name-grafana
---
# Source: one-green-core/templates/api-gateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "release-name-api"
spec:
  selector:
    matchLabels:
      app: "release-name-api"
  replicas: 1
  template:
    metadata:
      labels:
        app: "release-name-api"
    spec:
      initContainers:
        - name: initdb
          image: "docker.io/shanisma/og-core:0.0.10"
          command: [ '/bin/sh', '-c' ]
          args: [ "python init.py" ]
          imagePullPolicy: "Always"
          envFrom:
            - configMapRef:
                name: "release-name-config"
            - secretRef:
                name:  "release-name-secrets"
      containers:
        - name: api
          image: "docker.io/shanisma/og-core:0.0.10"
          command: [ '/bin/sh', '-c' ]
          args:
            - >
              python manage.py collectstatic --noinput
              &&
              python -m
              gunicorn
              --workers=1
              --bind=0.0.0.0:8080
              --access-logfile -
              --log-file -
              --log-level=info
              project.wsgi
          imagePullPolicy: "Always"
          envFrom:
          - configMapRef:
              name: "release-name-config"
          - secretRef:
              name:  "release-name-secrets"
          env:
            - name: "HEALTH_CHECK_TYPE"
              value: "all"
            - name: MQTT_USERNAME
              value: mqtt
            - name: MQTT_PASSWORD
              value: anyrandompassword
          ports:
            - containerPort: 8080
              name: api
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 15
            timeoutSeconds: 10
            exec:
              command:
                - python
                - health_check.py
---
# Source: one-green-core/templates/celery-worker-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "release-name-worker"
spec:
  selector:
    matchLabels:
      app: "release-name-worker"
  replicas: 3
  template:
    metadata:
      labels:
        app: "release-name-worker"
    spec:
      containers:
        - name: worker
          image: "docker.io/shanisma/og-core:0.0.10"
          command: [ '/bin/sh', '-c' ]
          args:
            - >
              python -m celery -A project worker
              -l info
          imagePullPolicy: "Always"
          envFrom:
          - configMapRef:
              name: "release-name-config"
          - secretRef:
              name:  "release-name-secrets"
          env:
            - name: MQTT_USERNAME
              value: mqtt
            - name: MQTT_PASSWORD
              value: anyrandompassword
            - name: C_FORCE_ROOT
              value: "true"
            - name: "HEALTH_CHECK_TYPE"
              value: "redis,pg,mqtt"
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 15
            timeoutSeconds: 10
            exec:
              command:
                - python
                - health_check.py
---
# Source: one-green-core/templates/celery-worker-deployment.yaml
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: "release-name-worker"
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: "release-name-worker"
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: 30
---
# Source: one-green-core/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-postgresql
  namespace: "default"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-11.6.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
spec:
  replicas: 1
  serviceName: release-name-postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: release-name-postgresql
      labels:
        app.kubernetes.io/name: postgresql
        helm.sh/chart: postgresql-11.6.3
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: primary
      annotations:
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: primary
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      initContainers:
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:14.3.0-debian-10-r22
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-postgresql
                  key: postgres-password
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                
                - |
                  exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
        storageClassName: scw-bssd
---
# Source: one-green-core/charts/rabbitmq/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-10.1.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: release-name-rabbitmq-headless
  podManagementPolicy: OrderedReady
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: rabbitmq
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: rabbitmq
        helm.sh/chart: rabbitmq-10.1.5
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: b5997b3face984b94658630cfd0530cf10034974545667ad1f5e238827d76db1
        checksum/secret: ccb9c13be59a010bca84a0b08ae73be025340a0ee6243108719d66f7db7ece78
    spec:
      
      serviceAccountName: release-name-rabbitmq
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: rabbitmq
                    app.kubernetes.io/instance: release-name
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      terminationGracePeriodSeconds: 120
      initContainers:
      containers:
        - name: rabbitmq
          image: docker.io/bitnami/rabbitmq:3.10.5-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/bash
                  - -ec
                  - |
                    if [[ -f /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh ]]; then
                        /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh -t "120" -d "false"
                    else
                        rabbitmqctl stop_app
                    fi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_SERVICE_NAME
              value: release-name-rabbitmq-headless
            - name: K8S_ADDRESS_TYPE
              value: hostname
            - name: RABBITMQ_FORCE_BOOT
              value: "no"
            - name: RABBITMQ_NODE_NAME
              value: "rabbit@$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: K8S_HOSTNAME_SUFFIX
              value: ".$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: RABBITMQ_MNESIA_DIR
              value: "/bitnami/rabbitmq/mnesia/$(RABBITMQ_NODE_NAME)"
            - name: RABBITMQ_LDAP_ENABLE
              value: "no"
            - name: RABBITMQ_LOGS
              value: "-"
            - name: RABBITMQ_ULIMIT_NOFILES
              value: "65536"
            - name: RABBITMQ_USE_LONGNAME
              value: "true"
            - name: RABBITMQ_ERL_COOKIE
              valueFrom:
                secretKeyRef:
                  name: release-name-rabbitmq
                  key: rabbitmq-erlang-cookie
            - name: RABBITMQ_LOAD_DEFINITIONS
              value: "no"
            - name: RABBITMQ_DEFINITIONS_FILE
              value: "/app/load_definition.json"
            - name: RABBITMQ_SECURE_PASSWORD
              value: "yes"
            - name: RABBITMQ_USERNAME
              value: "mqtt"
            - name: RABBITMQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-rabbitmq
                  key: rabbitmq-password
            - name: RABBITMQ_PLUGINS
              value: "rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_mqtt"
          envFrom:
          ports:
            - name: amqp
              containerPort: 5672
            - name: dist
              containerPort: 25672
            - name: stats
              containerPort: 15672
            - name: epmd
              containerPort: 4369
            - containerPort: 1883
              name: mqtt
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 20
            exec:
              command:
                - /bin/bash
                - -ec
                - rabbitmq-diagnostics -q ping
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 20
            exec:
              command:
                - /bin/bash
                - -ec
                - rabbitmq-diagnostics -q check_running && rabbitmq-diagnostics -q check_local_alarms
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: configuration
              mountPath: /bitnami/rabbitmq/conf
            - name: data
              mountPath: /bitnami/rabbitmq/mnesia
      volumes:
        - name: configuration
          secret:
            secretName: release-name-rabbitmq-config
            items:
              - key: rabbitmq.conf
                path: rabbitmq.conf
  volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app.kubernetes.io/name: rabbitmq
          app.kubernetes.io/instance: release-name
      spec:
        accessModes:
            - "ReadWriteOnce"
        resources:
          requests:
            storage: "1Gi"
        storageClassName: scw-bssd
---
# Source: one-green-core/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.11.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: master
  serviceName: release-name-redis-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-16.11.3
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 3a32821bb0a5d241f0e5075b1d414a0cbe56d228d9f2a1b4a2d86aead781dead
        checksum/health: 726f38471c36d4c558571582f2f6a28dd72dde2739bfbc9c0ed84039b822214f
        checksum/scripts: f5f316cf73e68654b0893af97b9d9cbe01c8d91c0d5b0564584b60de297fde1c
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: release-name-redis
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: master
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:6.2.7-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
              subPath: 
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc/
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: start-scripts
          configMap:
            name: release-name-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: release-name-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: release-name-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: tmp
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: redis
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "1Gi"
        storageClassName: scw-bssd
---
# Source: one-green-core/charts/redis/templates/replicas/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-16.11.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: replica
spec:
  replicas: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: replica
  serviceName: release-name-redis-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-16.11.3
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: replica
      annotations:
        checksum/configmap: 3a32821bb0a5d241f0e5075b1d414a0cbe56d228d9f2a1b4a2d86aead781dead
        checksum/health: 726f38471c36d4c558571582f2f6a28dd72dde2739bfbc9c0ed84039b822214f
        checksum/scripts: f5f316cf73e68654b0893af97b9d9cbe01c8d91c0d5b0564584b60de297fde1c
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: release-name-redis
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: replica
                namespaces:
                  - "default"
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:6.2.7-debian-11-r0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-replica.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: slave
            - name: REDIS_MASTER_HOST
              value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: redis
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local_and_master.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local_and_master.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
              subPath: 
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc
      volumes:
        - name: start-scripts
          configMap:
            name: release-name-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: release-name-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: release-name-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: redis-data
          emptyDir: {}
---
# Source: one-green-core/templates/influxdb-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "release-name-influxdb"
spec:
  serviceName: "release-name-influxdb"
  selector:
    matchLabels:
      app: "release-name-influxdb"
  replicas: 1
  template:
    metadata:
      name: "release-name-influxdb"
      labels:
        app: "release-name-influxdb"
    spec:
      containers:
        - name: influxdb
          image: "influxdb:2.0.8"
          env:
            - name: DOCKER_INFLUXDB_INIT_MODE
              valueFrom:
                configMapKeyRef:
                  key: "DOCKER_INFLUXDB_INIT_MODE"
                  name: "release-name-config"
            - name: DOCKER_INFLUXDB_INIT_USERNAME
              valueFrom:
                secretKeyRef:
                  key: "DOCKER_INFLUXDB_INIT_USERNAME"
                  name: "release-name-secrets"
            - name: DOCKER_INFLUXDB_INIT_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: "DOCKER_INFLUXDB_INIT_PASSWORD"
                  name: "release-name-secrets"
            - name: DOCKER_INFLUXDB_INIT_ADMIN_TOKEN
              valueFrom:
                secretKeyRef:
                  key: "DOCKER_INFLUXDB_INIT_ADMIN_TOKEN"
                  name: "release-name-secrets"
            - name: DOCKER_INFLUXDB_INIT_ORG
              valueFrom:
                configMapKeyRef:
                  key: DOCKER_INFLUXDB_INIT_ORG
                  name: "release-name-config"
            - name: DOCKER_INFLUXDB_INIT_BUCKET
              valueFrom:
                configMapKeyRef:
                  key: DOCKER_INFLUXDB_INIT_BUCKET
                  name: "release-name-config"
            - name: DOCKER_INFLUXDB_INIT_RETENTION
              valueFrom:
                configMapKeyRef:
                  key: DOCKER_INFLUXDB_INIT_RETENTION
                  name: "release-name-config"
          
          volumeMounts:
            - name: "release-name-influxdb-data"
              mountPath: /var/lib/influxdb
          
          ports:
            - containerPort: 8086
  
  volumeClaimTemplates:
    - metadata:
        name: "release-name-influxdb-data"
      spec:
        storageClassName: "scw-bssd"
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "1Gi"
---
# Source: one-green-core/templates/light-controller-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "release-name-light-controller"
spec:
  selector:
    matchLabels:
      app: "release-name-light-controller"
  serviceName: "release-name-light-controller"
  replicas: 1
  template:
    metadata:
      labels:
        app: "release-name-light-controller"
    spec:
      containers:
        - name: light-controller
          image: "docker.io/shanisma/og-core:0.0.10"
          command: [ '/bin/sh', '-c' ]
          args:
            - python manage.py light_controller
          imagePullPolicy: "Always"
          envFrom:
            - configMapRef:
                name: "release-name-config"
            - secretRef:
                name:  "release-name-secrets"
          env:
            - name: MQTT_USERNAME
              value: mqtt
            - name: MQTT_PASSWORD
              value: anyrandompassword
            - name: "HEALTH_CHECK_TYPE"
              value: "redis,pg,mqtt"
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 15
            timeoutSeconds: 10
            exec:
              command:
                - python
                - health_check.py
---
# Source: one-green-core/templates/sprinklers-controller-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "release-name-sprinklers-controller"
spec:
  selector:
    matchLabels:
      app: "release-name-sprinklers-controller"
  serviceName: "release-name-sprinklers-controller"
  replicas: 1
  template:
    metadata:
      labels:
        app: "release-name-sprinklers-controller"
    spec:
      containers:
        - name: sprinklers-controller
          image: "docker.io/shanisma/og-core:0.0.10"
          command: [ '/bin/sh', '-c' ]
          args:
            - python manage.py sprinkler_controller
          imagePullPolicy: "Always"
          envFrom:
            - configMapRef:
                name: "release-name-config"
            - secretRef:
                name:  "release-name-secrets"
          env:
            - name: MQTT_USERNAME
              value: mqtt
            - name: MQTT_PASSWORD
              value: anyrandompassword
            - name: "HEALTH_CHECK_TYPE"
              value: "redis,pg,mqtt"
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 15
            timeoutSeconds: 10
            exec:
              command:
                - python
                - health_check.py
---
# Source: one-green-core/templates/telegraf-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-telegraf
spec:
  selector:
    matchLabels:
      app: release-name-telegraf
  serviceName: release-name-telegraf
  replicas: 1
  template:
    metadata:
      labels:
        app: release-name-telegraf
    spec:
      containers:
        - name: telegraf
          imagePullPolicy: IfNotPresent
          image: telegraf:1.20.4
          env:
            - name: MQTT_USERNAME
              value: mqtt
            - name: MQTT_PASSWORD
              value: anyrandompassword
          envFrom:
            - configMapRef:
                name: release-name-config
            - secretRef:
                name: release-name-secrets
          volumeMounts:
            - name: conf
              mountPath: /etc/telegraf
      volumes:
        - name: conf
          configMap:
            name: release-name-telegraf-config
---
# Source: one-green-core/templates/water-controller-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "release-name-water-controller"
spec:
  selector:
    matchLabels:
      app: "release-name-water-controller"
  serviceName: "water-controller"
  replicas: 1
  template:
    metadata:
      labels:
        app: "release-name-water-controller"
    spec:
      containers:
        - name: water-controller
          image: "docker.io/shanisma/og-core:0.0.10"
          command: [ '/bin/sh', '-c' ]
          args:
            - python manage.py water_controller
          imagePullPolicy: "Always"
          envFrom:
            - configMapRef:
                name: "release-name-config"
            - secretRef:
                name:  "release-name-secrets"
          env:
            - name: MQTT_USERNAME
              value: mqtt
            - name: MQTT_PASSWORD
              value: anyrandompassword
            - name: "HEALTH_CHECK_TYPE"
              value: "redis,pg,mqtt"
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 15
            timeoutSeconds: 10
            exec:
              command:
                - python
                - health_check.py
---
# Source: one-green-core/templates/api-gateway-deployment.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: "release-name-api"
  annotations:
    
    cert-manager.io/cluster-issuer: letsencrypt-prod
    
spec:
  rules:
    - host: "api.dev1.og-ingest1.com"
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: "release-name-api"
                port:
                  number: 8080
  tls:
    - hosts:
        - "api.dev1.og-ingest1.com"
      secretName: "release-name-api-tls"
---
# Source: one-green-core/charts/grafana/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: release-name-grafana-test
  labels:
    helm.sh/chart: grafana-6.29.6
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.5.3"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  namespace: default
spec:
  serviceAccountName: release-name-grafana-test
  containers:
    - name: release-name-test
      image: "bats/bats:v1.4.1"
      imagePullPolicy: "IfNotPresent"
      command: ["/opt/bats/bin/bats", "-t", "/tests/run.sh"]
      volumeMounts:
        - mountPath: /tests
          name: tests
          readOnly: true
  volumes:
  - name: tests
    configMap:
      name: release-name-grafana-test
  restartPolicy: Never

---
# Source: v3-backend/templates/admin-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: release-name-v3-backend-admin
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    component: "admin"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: v3-backend
      app.kubernetes.io/instance: release-name
      component: "admin"
---
# Source: v3-backend/templates/app-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: release-name-v3-backend-app
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    component: "app"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: v3-backend
      app.kubernetes.io/instance: release-name
      component: "app"
---
# Source: v3-backend/charts/ipfs/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-ipfs
  labels:
    app.kubernetes.io/name: ipfs
    helm.sh/chart: ipfs-1.2.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: v3-backend/charts/rabbitmq/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.3.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
secrets:
  - name: release-name-rabbitmq
---
# Source: v3-backend/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-v3-backend
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: v3-backend/charts/rabbitmq/templates/config-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-rabbitmq-config
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.3.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  rabbitmq.conf: |-
    IyMgVXNlcm5hbWUgYW5kIHBhc3N3b3JkCiMjCmRlZmF1bHRfdXNlciA9IHJhYmJpdG1xCiMjIENsdXN0ZXJpbmcKIyMKY2x1c3Rlcl9mb3JtYXRpb24ucGVlcl9kaXNjb3ZlcnlfYmFja2VuZCAgPSByYWJiaXRfcGVlcl9kaXNjb3ZlcnlfazhzCmNsdXN0ZXJfZm9ybWF0aW9uLms4cy5ob3N0ID0ga3ViZXJuZXRlcy5kZWZhdWx0CmNsdXN0ZXJfZm9ybWF0aW9uLm5vZGVfY2xlYW51cC5pbnRlcnZhbCA9IDEwCmNsdXN0ZXJfZm9ybWF0aW9uLm5vZGVfY2xlYW51cC5vbmx5X2xvZ193YXJuaW5nID0gdHJ1ZQpjbHVzdGVyX3BhcnRpdGlvbl9oYW5kbGluZyA9IGF1dG9oZWFsCiMgcXVldWUgbWFzdGVyIGxvY2F0b3IKcXVldWVfbWFzdGVyX2xvY2F0b3IgPSBtaW4tbWFzdGVycwojIGVuYWJsZSBndWVzdCB1c2VyCmxvb3BiYWNrX3VzZXJzLmd1ZXN0ID0gZmFsc2UKI2RlZmF1bHRfdmhvc3QgPSBkZWZhdWx0LXZob3N0CiNkaXNrX2ZyZWVfbGltaXQuYWJzb2x1dGUgPSA1ME1C
---
# Source: v3-backend/charts/rabbitmq/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.3.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  rabbitmq-password: "cmFiYml0bXE="
  
  rabbitmq-erlang-cookie: "bGRFNk1KUFlXRk9NM2dPcjNEUWdwbkoyak4xWEhOUXg="
---
# Source: v3-backend/templates/common-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-v3-backend
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  GCP_STORAGE_OAUTH_CREDENTIALS: ""
  GCP_PERSPECTIVE_API_KEY: ""
  GCP_CLOUD_VISION_API_KEY: ""

  NOTIFIERS_TELEGRAM_TOKEN: ""
  NOTIFIERS_TELEGRAM_CHAT_ID: ""
  MULTISIG_OWNERS_TELEGRAM_TOKEN: ""
  MULTISIG_OWNERS_TELEGRAM_CHAT_ID: ""
  MAILGUN_API_KEY: ""

  IPFS_INFURA_CLIENT_USERNAME: ""
  IPFS_INFURA_CLIENT_PASSWORD: ""
  IPFS_PINATA_API_KEY: ""
  IPFS_PINATA_SECRET_KEY: ""
  IPFS_FILEBASE_API_TOKEN: ""
  IPFS_QUICKNODE_API_TOKEN: ""

  BROKER_URL: ""
  MEMCACHED_URL: ""
  POSTGRES_PASSWORD: ""

  FAUCET_PRIVATE_KEY: ""

  GRAPH_POSTGRES_USER: ""
  GRAPH_POSTGRES_PASSWORD: ""

  RATED_API_BEARER_TOKEN: ""
---
# Source: v3-backend/charts/ipfs/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-ipfs
  labels:
    app.kubernetes.io/name: ipfs
    helm.sh/chart: ipfs-1.2.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  IPFS_PATH: "/data/ipfs"
  IPFS_PROFILE: "server"
  IPFS_FD_MAX: "8192"
  LIBP2P_TCP_REUSEPORT: "true"
  IPFS_LOGGING: "error"
  IPFS_LOGGING_FMT: "color"
  IPFS_FUSE_DEBUG: "false"
  YAMUX_DEBUG: "false"
---
# Source: v3-backend/charts/ipfs/templates/entrypoint.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-ipfs-entrypoint
  labels:
    app.kubernetes.io/name: ipfs
    helm.sh/chart: ipfs-1.2.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  start_ipfs: |
    #!/bin/sh
    set -e
    user=ipfs
    repo="$IPFS_PATH"

    if [ `id -u` -eq 0 ]; then
      echo "Changing user to $user"
      # ensure folder is writable
      su-exec "$user" test -w "$repo" || chown -R -- "$user" "$repo"
      # restart script with new privileges
      exec su-exec "$user" "$0" "$@"
    fi

    # 2nd invocation with regular user
    ipfs version

    if [ -e "$repo/config" ]; then
      echo "Found IPFS fs-repo at $repo"
    else
      case "$IPFS_PROFILE" in
        "") INIT_ARGS="" ;;
        *) INIT_ARGS="--profile=$IPFS_PROFILE" ;;
      esac
      ipfs init $INIT_ARGS
      ipfs config Addresses.API /ip4/0.0.0.0/tcp/5001
      ipfs config Addresses.Gateway /ip4/0.0.0.0/tcp/8080
      ipfs config --json Peering.Peers "[{\"Addr\": [\"/dnsaddr/node-1.ingress.cloudflare-ipfs.com\"], \"ID\": \"QmcFf2FH3CEgTNHeMRGhN7HNHU1EXAxoEk6EFuSyXCsvRE\"}, {\"Addr\": [\"/dnsaddr/node-2.ingress.cloudflare-ipfs.com\"], \"ID\": \"QmcFmLd5ySfk2WZuJ1mfSWLDjdmHZq7rSAua4GoeSQfs1z\"}, {\"Addr\": [\"/dnsaddr/node-3.ingress.cloudflare-ipfs.com\"], \"ID\": \"QmcfFmzSDVbwexQ9Au2pt5YEXHK5xajwgaU6PpkbLWerMa\"}, {\"Addr\": [\"/dnsaddr/node-4.ingress.cloudflare-ipfs.com\"], \"ID\": \"QmcfJeB3Js1FG7T8YaZATEiaHqNKVdQfybYYkbT1knUswx\"}, {\"Addr\": [\"/dnsaddr/node-5.ingress.cloudflare-ipfs.com\"], \"ID\": \"QmcfVvzK4tMdFmpJjEKDUoqRgP4W9FnmJoziYX5GXJJ8eZ\"}, {\"Addr\": [\"/dnsaddr/node-6.ingress.cloudflare-ipfs.com\"], \"ID\": \"QmcfZD3VKrUxyP9BbyUnZDpbqDnT7cQ4WjPP8TRLXaoE7G\"}, {\"Addr\": [\"/dnsaddr/node-7.ingress.cloudflare-ipfs.com\"], \"ID\": \"QmcfZP2LuW4jxviTeG8fi28qjnZScACb8PEgHAc17ZEri3\"}, {\"Addr\": [\"/dnsaddr/node-8.ingress.cloudflare-ipfs.com\"], \"ID\": \"QmcfgsJsMtx6qJb74akCw1M24X1zFwgGo11h1cuhwQjtJP\"}, {\"Addr\": [\"/dnsaddr/node-9.ingress.cloudflare-ipfs.com\"], \"ID\": \"Qmcfr2FC7pFzJbTSDfYaSy1J8Uuy8ccGLeLyqJCKJvTHMi\"}, {\"Addr\": [\"/dnsaddr/node-10.ingress.cloudflare-ipfs.com\"], \"ID\": \"QmcfR3V5YAtHBzxVACWCzXTt26SyEkxdwhGJ6875A8BuWx\"}, {\"Addr\": [\"/dnsaddr/node-11.ingress.cloudflare-ipfs.com\"], \"ID\": \"Qmcfuo1TM9uUiJp6dTbm915Rf1aTqm3a3dnmCdDQLHgvL5\"}, {\"Addr\": [\"/dnsaddr/node-12.ingress.cloudflare-ipfs.com\"], \"ID\": \"QmcfV2sg9zaq7UUHVCGuSvT2M2rnLBAPsiE79vVyK3Cuev\"}, {\"Addr\": [\"/dnsaddr/fra1-1.hostnodes.pinata.cloud\"], \"ID\": \"QmWaik1eJcGHq1ybTWe7sezRfqKNcDRNkeBaLnGwQJz1Cj\"}, {\"Addr\": [\"/dnsaddr/fra1-2.hostnodes.pinata.cloud\"], \"ID\": \"QmNfpLrQQZr5Ns9FAJKpyzgnDL2GgC6xBug1yUZozKFgu4\"}, {\"Addr\": [\"/dnsaddr/fra1-3.hostnodes.pinata.cloud\"], \"ID\": \"QmPo1ygpngghu5it8u4Mr3ym6SEU2Wp2wA66Z91Y1S1g29\"}, {\"Addr\": [\"/dnsaddr/nyc1-1.hostnodes.pinata.cloud\"], \"ID\": \"QmRjLSisUCHVpFa5ELVvX3qVPfdxajxWJEHs9kN3EcxAW6\"}, {\"Addr\": [\"/dnsaddr/nyc1-2.hostnodes.pinata.cloud\"], \"ID\": \"QmPySsdmbczdZYBpbi2oq2WMJ8ErbfxtkG8Mo192UHkfGP\"}, {\"Addr\": [\"/dnsaddr/nyc1-3.hostnodes.pinata.cloud\"], \"ID\": \"QmSarArpxemsPESa6FNkmuu9iSE1QWqPX2R3Aw6f5jq4D5\"}]"


      # Set up the swarm key, if provided

      SWARM_KEY_FILE="$repo/swarm.key"
      SWARM_KEY_PERM=0400

      # Create a swarm key from a given environment variable
      if [ ! -z "$IPFS_SWARM_KEY" ] ; then
        echo "Copying swarm key from variable..."
        echo -e "$IPFS_SWARM_KEY" >"$SWARM_KEY_FILE" || exit 1
        chmod $SWARM_KEY_PERM "$SWARM_KEY_FILE"
      fi

      # Unset the swarm key variable
      unset IPFS_SWARM_KEY

      # Check during initialization if a swarm key was provided and
      # copy it to the ipfs directory with the right permissions
      # WARNING: This will replace the swarm key if it exists
      if [ ! -z "$IPFS_SWARM_KEY_FILE" ] ; then
        echo "Copying swarm key from file..."
        install -m $SWARM_KEY_PERM "$IPFS_SWARM_KEY_FILE" "$SWARM_KEY_FILE" || exit 1
      fi

      # Unset the swarm key file variable
      unset IPFS_SWARM_KEY_FILE

    fi

    exec ipfs "$@"
---
# Source: v3-backend/templates/common-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-v3-backend
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  NETWORK: "goerli"
  SENTRY_DSN: ""
  LOG_LEVEL: "INFO"
  CELERY_CONCURRENCY: "8"

  GCP_STORAGE_CONTENT_BUCKET: ""
  GCP_STORAGE_DOMAIN_ALIAS: ""
  GCP_STORAGE_IPFS_BUCKET: ""

  SEND_EMAILS: "False"
  MAILGUN_SENDER_DOMAIN: "mail.stakewise.io"
  MAILGUN_API_URL: "https://api.eu.mailgun.net/v3"
  DEFAULT_FROM_EMAIL: "\"Stakewise\" <no-reply@mail.stakewise.io>"

  IPFS_FILEBASE_BUCKET: ""
  IPFS_PINATA_PIN_ENDPOINT: "https://api.pinata.cloud/pinning/pinJSONToIPFS"
  IPFS_FETCH_ENDPOINTS: "[\"http://cloudflare-ipfs.com\",\"https://ipfs.io\",\"https://gateway.pinata.cloud\"]"

  EXECUTION_ENDPOINTS: ""
  CONSENSUS_ENDPOINTS: ""
  
  SEND_TELEGRAM_NOTIFICATIONS: "false"
  IPFS_LOCAL_CLIENT_ENDPOINT: "/dns/release-name-ipfs/tcp/5001/http"

  POSTGRES_HOST: "localhost"
  POSTGRES_PORT: "5432"
  POSTGRES_DB: "v3-backend"
  POSTGRES_USER: ""

  GRAPH_API_URL: ""
  GRAPH_POSTGRES_HOST: ""
  GRAPH_POSTGRES_PORT: ""
  GRAPH_POSTGRES_DB: ""
  GRAPH_VAULTS_TABLE: ""
  GRAPH_EXIT_REQUESTS_TABLE: ""
---
# Source: v3-backend/charts/rabbitmq/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-rabbitmq-endpoint-reader
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.3.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create"]
---
# Source: v3-backend/charts/rabbitmq/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-rabbitmq-endpoint-reader
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.3.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: release-name-rabbitmq
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-rabbitmq-endpoint-reader
---
# Source: v3-backend/charts/ipfs/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-ipfs
  labels:
    app.kubernetes.io/name: ipfs
    helm.sh/chart: ipfs-1.2.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 4001
      targetPort: p2p-tcp
      protocol: TCP
      name: p2p-tcp
    - port: 4001
      targetPort: p2p-udp
      protocol: UDP
      name: p2p-udp
    - port: 5001
      targetPort: api
      protocol: TCP
      name: api
    - port: 8080
      targetPort: gateway
      protocol: TCP
      name: gateway
    - port: 8081
      targetPort: websocket
      protocol: TCP
      name: websocket
  selector:
    app.kubernetes.io/name: ipfs
    app.kubernetes.io/instance: release-name
---
# Source: v3-backend/charts/memcached/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-memcached
  namespace: default
  labels:
    app.kubernetes.io/name: memcached
    helm.sh/chart: memcached-6.3.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: memcache
      port: 11211
      targetPort: memcache
      nodePort: null
  selector:
    app.kubernetes.io/name: memcached
    app.kubernetes.io/instance: release-name
---
# Source: v3-backend/charts/rabbitmq/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-rabbitmq-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.3.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  ports:
    - name: epmd
      port: 4369
      targetPort: epmd
    - name: amqp
      port: 5672
      targetPort: amqp
    - name: dist
      port: 25672
      targetPort: dist
    - name: http-stats
      port: 15672
      targetPort: stats
  selector: 
    app.kubernetes.io/name: rabbitmq
    app.kubernetes.io/instance: release-name
  publishNotReadyAddresses: true
---
# Source: v3-backend/charts/rabbitmq/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.3.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: amqp
      port: 5672
      targetPort: amqp
      nodePort: null
    - name: epmd
      port: 4369
      targetPort: epmd
      nodePort: null
    - name: dist
      port: 25672
      targetPort: dist
      nodePort: null
    - name: http-stats
      port: 15672
      targetPort: stats
      nodePort: null
  selector: 
    app.kubernetes.io/name: rabbitmq
    app.kubernetes.io/instance: release-name
---
# Source: v3-backend/templates/admin-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-v3-backend-admin
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    component: "admin"
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8000
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/name: v3-backend
    app.kubernetes.io/instance: release-name
    component: "admin"
---
# Source: v3-backend/templates/app-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-v3-backend-app
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    component: "app"
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8000
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/name: v3-backend
    app.kubernetes.io/instance: release-name
    component: "app"
---
# Source: v3-backend/charts/memcached/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-memcached
  namespace: default
  labels:
    app.kubernetes.io/name: memcached
    helm.sh/chart: memcached-6.3.2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: memcached
      app.kubernetes.io/instance: release-name
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: memcached
        helm.sh/chart: memcached-6.3.2
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
    spec:
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: memcached
                    app.kubernetes.io/instance: release-name
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: default
      containers:
        - name: memcached
          image: docker.io/bitnami/memcached:1.6.17-debian-11-r35
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MEMCACHED_PORT_NUMBER
              value: "11211"
          ports:
            - name: memcache
              containerPort: 11211
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: memcache
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
            tcpSocket:
              port: memcache
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: tmp
          emptyDir: {}
---
# Source: v3-backend/templates/admin-deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: release-name-v3-backend-admin
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    component: "admin"
spec:
  replicas: 1
  minReadySeconds: 15
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: v3-backend
      app.kubernetes.io/instance: release-name
      component: "admin"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: v3-backend
        app.kubernetes.io/instance: release-name
        component: "admin"
      annotations:
        checksum/common-secret: ba4baad7c3ce1528e1d612dc81f50a4320ddcd97349ec877e084ca67d47f8f53
        checksum/common-configmap: a7cdd02272a86577026e93611e51c4be1e153306e5a08f106c50f8b78f665a16
    spec:
      securityContext:
        
        fsGroup: 65534
        runAsUser: 65534
      serviceAccountName: release-name-v3-backend
      priorityClassName: ""
      containers:
        - name: v3-backend
          image: "europe-west4-docker.pkg.dev/stakewiselabs/private/v3-backend:v1.5.3"
          imagePullPolicy: IfNotPresent
          command: ["sh", "./scripts/admin.sh"]
          envFrom:
            - secretRef:
                name: release-name-v3-backend
            - configMapRef:
                name: release-name-v3-backend
          resources:
            limits:
              cpu: 1000m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 512Mi
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
---
# Source: v3-backend/templates/app-deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: release-name-v3-backend-app
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    component: "app"
spec:
  replicas: 2
  minReadySeconds: 15
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: v3-backend
      app.kubernetes.io/instance: release-name
      component: "app"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: v3-backend
        app.kubernetes.io/instance: release-name
        component: "app"
      annotations:
        checksum/common-secret: ba4baad7c3ce1528e1d612dc81f50a4320ddcd97349ec877e084ca67d47f8f53
        checksum/common-configmap: a7cdd02272a86577026e93611e51c4be1e153306e5a08f106c50f8b78f665a16
    spec:
      securityContext:
        
        fsGroup: 65534
        runAsUser: 65534
      serviceAccountName: release-name-v3-backend
      priorityClassName: ""
      containers:
        - name: v3-backend
          image: "europe-west4-docker.pkg.dev/stakewiselabs/private/v3-backend:v1.5.3"
          imagePullPolicy: IfNotPresent
          command: ["sh", "./scripts/app.sh"]
          envFrom:
            - secretRef:
                name: release-name-v3-backend
            - configMapRef:
                name: release-name-v3-backend
          resources:
            limits:
              cpu: 1000m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 512Mi
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
---
# Source: v3-backend/templates/beat.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: release-name-v3-backend-beat
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    component: "beat"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: v3-backend
      app.kubernetes.io/instance: release-name
      component: "beat"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: v3-backend
        app.kubernetes.io/instance: release-name
        component: "beat"
      annotations:
        checksum/common-secret: ba4baad7c3ce1528e1d612dc81f50a4320ddcd97349ec877e084ca67d47f8f53
        checksum/common-configmap: a7cdd02272a86577026e93611e51c4be1e153306e5a08f106c50f8b78f665a16
    spec:
      securityContext:
        
        fsGroup: 65534
        runAsUser: 65534
      serviceAccountName: release-name-v3-backend
      priorityClassName: ""
      containers:
        - name: v3-backend
          image: "europe-west4-docker.pkg.dev/stakewiselabs/private/v3-backend:v1.5.3"
          imagePullPolicy: IfNotPresent
          command: ["sh", "./scripts/beat.sh"]
          envFrom:
            - secretRef:
                name: release-name-v3-backend
            - configMapRef:
                name: release-name-v3-backend
          resources:
            limits:
              cpu: 1000m
              memory: 1024Mi
            requests:
              cpu: 1000m
              memory: 512Mi
---
# Source: v3-backend/templates/worker.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: release-name-v3-backend-worker
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    component: "worker"
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: v3-backend
      app.kubernetes.io/instance: release-name
      component: "worker"
  template:
    metadata:
      labels:
        app.kubernetes.io/name: v3-backend
        app.kubernetes.io/instance: release-name
        component: "worker"
      annotations:
        checksum/common-secret: ba4baad7c3ce1528e1d612dc81f50a4320ddcd97349ec877e084ca67d47f8f53
        checksum/common-configmap: a7cdd02272a86577026e93611e51c4be1e153306e5a08f106c50f8b78f665a16
    spec:
      securityContext:
        
        fsGroup: 65534
        runAsUser: 65534
      serviceAccountName: release-name-v3-backend
      priorityClassName: ""
      containers:
        - name: v3-backend
          image: "europe-west4-docker.pkg.dev/stakewiselabs/private/v3-backend:v1.5.3"
          imagePullPolicy: IfNotPresent
          command: ["sh", "./scripts/worker.sh"]
          envFrom:
            - secretRef:
                name: release-name-v3-backend
            - configMapRef:
                name: release-name-v3-backend
          resources:
            limits:
              cpu: 1000m
              memory: 1024Mi
            requests:
              cpu: 1000m
              memory: 512Mi
---
# Source: v3-backend/charts/ipfs/templates/statefulset.yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: release-name-ipfs
  labels:
    app.kubernetes.io/name: ipfs
    helm.sh/chart: ipfs-1.2.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ipfs
      app.kubernetes.io/instance: release-name
  serviceName: release-name-ipfs
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ipfs
        app.kubernetes.io/instance: release-name
    spec:
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
      serviceAccountName: release-name-ipfs
      priorityClassName: ""
      initContainers:
        - name: init-chown
          image: "docker.io/busybox:1.34"
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
          command: ["chown", "-R", "1001:1001", "/data/ipfs"]
          volumeMounts:
            - name: data
              mountPath: /data/ipfs
      containers:
        - name: release-name-ipfs
          image: "docker.io/ipfs/kubo:v0.18.1"
          imagePullPolicy: IfNotPresent
          envFrom:
            - configMapRef:
                name: release-name-ipfs
          ports:
            - containerPort: 4001
              name: p2p-tcp
              protocol: TCP
            - containerPort: 4001
              name: p2p-udp
              protocol: UDP
            - containerPort: 5001
              name: api
              protocol: TCP
            - containerPort: 8080
              name: gateway
              protocol: TCP
            - containerPort: 8081
              name: websocket
              protocol: TCP
          volumeMounts:
            - name: data
              mountPath: /data/ipfs
            - name: fuse-ipfs
              mountPath: /ipfs
            - name: fuse-ipns
              mountPath: /ipns
            - name: entrypoint
              subPath: start_ipfs
              mountPath: /usr/local/bin/start_ipfs
          livenessProbe:
            httpGet:
              path: /debug/metrics/prometheus
              port: api
            initialDelaySeconds: 15
            periodSeconds: 3
          readinessProbe:
            httpGet:
              path: /debug/metrics/prometheus
              port: api
            initialDelaySeconds: 15
            periodSeconds: 3
      volumes:
        - name: entrypoint
          configMap:
            name: release-name-ipfs-entrypoint
            defaultMode: 0777
  volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app.kubernetes.io/name: ipfs
          helm.sh/chart: ipfs-1.2.1
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/managed-by: Helm
      spec:
        accessModes: [ReadWriteOnce]
        storageClassName: 
        resources:
          requests:
            storage: "25Gi"
    - metadata:
        name: fuse-ipfs
        labels:
          app.kubernetes.io/name: ipfs
          helm.sh/chart: ipfs-1.2.1
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/managed-by: Helm
      spec:
        accessModes: [ReadWriteOnce]
        storageClassName: 
        resources:
          requests:
            storage: "1Gi"
    - metadata:
        name: fuse-ipns
        labels:
          app.kubernetes.io/name: ipfs
          helm.sh/chart: ipfs-1.2.1
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/managed-by: Helm
      spec:
        accessModes: [ReadWriteOnce]
        storageClassName: 
        resources:
          requests:
            storage: "1Gi"
---
# Source: v3-backend/charts/rabbitmq/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-rabbitmq
  namespace: "default"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-11.3.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: release-name-rabbitmq-headless
  podManagementPolicy: OrderedReady
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: rabbitmq
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: rabbitmq
        helm.sh/chart: rabbitmq-11.3.0
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/config: 3592afff13c49c6c019110cb46f4f118e00f72f84b6af8e01dd76b51f9f9ad6b
        checksum/secret: 5cb25730f5bdb188885fe35c62d6bb38903ba3dbf3ac6a16fbdea80efd99e03b
    spec:
      
      serviceAccountName: release-name-rabbitmq
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: rabbitmq
                    app.kubernetes.io/instance: release-name
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      terminationGracePeriodSeconds: 120
      initContainers:
      containers:
        - name: rabbitmq
          image: docker.io/bitnami/rabbitmq:3.11.5-debian-11-r2
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/bash
                  - -ec
                  - |
                    if [[ -f /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh ]]; then
                        /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh -t "120" -d "false"
                    else
                        rabbitmqctl stop_app
                    fi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_SERVICE_NAME
              value: release-name-rabbitmq-headless
            - name: K8S_ADDRESS_TYPE
              value: hostname
            - name: RABBITMQ_FORCE_BOOT
              value: "no"
            - name: RABBITMQ_NODE_NAME
              value: "rabbit@$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: K8S_HOSTNAME_SUFFIX
              value: ".$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: RABBITMQ_MNESIA_DIR
              value: "/bitnami/rabbitmq/mnesia/$(RABBITMQ_NODE_NAME)"
            - name: RABBITMQ_LDAP_ENABLE
              value: "no"
            - name: RABBITMQ_LOGS
              value: "-"
            - name: RABBITMQ_ULIMIT_NOFILES
              value: "65536"
            - name: RABBITMQ_USE_LONGNAME
              value: "true"
            - name: RABBITMQ_ERL_COOKIE
              valueFrom:
                secretKeyRef:
                  name: release-name-rabbitmq
                  key: rabbitmq-erlang-cookie
            - name: RABBITMQ_LOAD_DEFINITIONS
              value: "no"
            - name: RABBITMQ_DEFINITIONS_FILE
              value: "/app/load_definition.json"
            - name: RABBITMQ_SECURE_PASSWORD
              value: "yes"
            - name: RABBITMQ_USERNAME
              value: "rabbitmq"
            - name: RABBITMQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-rabbitmq
                  key: rabbitmq-password
            - name: RABBITMQ_PLUGINS
              value: "rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap"
          envFrom:
          ports:
            - name: amqp
              containerPort: 5672
            - name: dist
              containerPort: 25672
            - name: stats
              containerPort: 15672
            - name: epmd
              containerPort: 4369
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 20
            exec:
              command:
                - /bin/bash
                - -ec
                - rabbitmq-diagnostics -q ping
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 20
            exec:
              command:
                - /bin/bash
                - -ec
                - rabbitmq-diagnostics -q check_running && rabbitmq-diagnostics -q check_local_alarms
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: configuration
              mountPath: /bitnami/rabbitmq/conf
            - name: data
              mountPath: /bitnami/rabbitmq/mnesia
      volumes:
        - name: configuration
          projected:
            sources:
              - secret:
                  name: release-name-rabbitmq-config
        - name: data
          emptyDir: {}
---
# Source: v3-backend/templates/admin-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: release-name-v3-backend-admin
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    component: "admin"
spec:
  rules:
---
# Source: v3-backend/templates/app-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: release-name-v3-backend-app
  labels:
    app.kubernetes.io/name: v3-backend
    helm.sh/chart: v3-backend-3.2.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    component: "app"
spec:
  rules:

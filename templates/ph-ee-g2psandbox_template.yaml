---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/elasticsearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "ph-ee-elasticsearch-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "ph-ee-elasticsearch"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/billPay/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "ph-ee-connector-bill-pay"
  annotations:
  labels:
    app: "ph-ee-connector-bill-pay"
    chart: "billPay-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/charts/zeebe-gateway/templates/gateway-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-zeebe-gateway-gateway
  labels:
    app: camunda-platform
    app.kubernetes.io/name: zeebe-gateway
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: zeebe-gateway-8.2.12
    app.kubernetes.io/version: "8.2.12"
    app.kubernetes.io/component: zeebe-gateway
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/charts/zeebe/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-zeebe
  labels:
    app: camunda-platform
    app.kubernetes.io/name: zeebe
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: zeebe-8.2.12
    app.kubernetes.io/version: "8.2.8"
    app.kubernetes.io/component: zeebe-broker
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/channel/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "ph-ee-connector-channel"
  annotations:
  labels:
    app: "ph-ee-connector-channel"
    chart: "channel-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/connector_bulk/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ph-ee-connector-bulk
  annotations:
  labels:
    app: ph-ee-connector-bulk
    chart: "connector_bulk-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/crm/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "ph-ee-connector-crm"
  annotations:
  labels:
    app: "ph-ee-connector-crm"
    chart: "crm-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_es/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ph-ee-importer-es
  annotations:
  labels:
    app: ph-ee-importer-es
    chart: "importer_es-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_rdbms/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ph-ee-importer-rdbms
  annotations:
  labels:
    app: ph-ee-importer-rdbms
    chart: "importer_rdbms-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kafka/templates/rbac/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kafka
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-25.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
  annotations:
automountServiceAccountToken: true
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/messagegateway/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: message-gateway
  annotations:
  labels:
    app: message-gateway
    chart: "messagegateway-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "minio-sa"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/notifications/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ph-ee-connector-notification
  annotations:
  labels:
    app: ph-ee-connector-notification
    chart: "notifications-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_app/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ph-ee-operations-app
  annotations:
  labels:
    app: ph-ee-operations-app
    chart: "operations_app-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_web/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ph-ee-operations-web
  annotations:
  labels:
    app: ph-ee-operations-web
    chart: "operations_web-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operationsmysql/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: operationsmysql
  namespace: "default"
  labels:
    app.kubernetes.io/name: operationsmysql
    helm.sh/chart: operationsmysql-9.4.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
automountServiceAccountToken: true
secrets:
  - name: operationsmysql
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_ams_mifos/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ph-ee-connector-ams-mifos
  annotations:
  labels:
    app: ph-ee-connector-ams-mifos
    chart: "ph_ee_connector_ams_mifos-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_gsma/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ph-ee-connector-gsma
  annotations:
  labels:
    app: ph-ee-connector-gsma
    chart: "ph_ee_connector_gsma-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_mojaloop/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "ph-ee-connector-mojaloop-java"
  annotations:
  labels:
    app: "ph-ee-connector-mojaloop-java"
    chart: "ph_ee_connector_mojaloop-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/redis/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: release-name-redis
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.9.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/zeebe_ops/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "ph-ee-zeebe-ops"
  annotations:
  labels:
    app: "ph-ee-zeebe-ops"
    chart: "zeebe_ops-1.0.0"
    heritage: "Helm"
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/templates/post-installation-job/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: job-creator
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/connector_bulk/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: bulk-processor-secret
type: Opaque
data:
  aws-region: YXAtc291dGgtMQ==
  aws-access-key: cm9vdA==
  aws-secret-key: cGFzc3dvcmQ=
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_rdbms/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: importer-rdbms-secret
type: Opaque
data:
  database-password: cGFzc3dvcmQ=
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kafka/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: kafka-kraft-cluster-id
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-25.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  kraft-cluster-id: "c3BEbm40b1NyNkRMS1B4M2NFaGVMcA=="
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/messagegateway/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: messagegateway-secret
type: Opaque
data:
  api-key: "PGFwaV9rZXk+"
  project-id: "PHByb2plY3RfaWQ+"
  database-password: "cGFzc3dvcmQ="
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: minio
  labels:
    app: minio
    chart: minio-5.0.14
    release: release-name
    heritage: Helm
type: Opaque
data:
  rootUser: "cm9vdA=="
  rootPassword: "cGFzc3dvcmQ="
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_app/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: operations-app-secret
type: Opaque
data:
  database-password: cGFzc3dvcmQ=
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operationsmysql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: operationsmysql
  namespace: "default"
  labels:
    app.kubernetes.io/name: operationsmysql
    helm.sh/chart: operationsmysql-9.4.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  mysql-root-password: "ZXRoaWVUaWVDaDhhaHY="
  mysql-password: "cGFzc3dvcmQ="
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/redis/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-redis
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.9.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  redis-password: "Wmw4VU1vSktSQg=="
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/charts/zeebe-gateway/templates/configmap.yaml
kind: ConfigMap
metadata:
  name: release-name-zeebe-gateway-gateway
  labels:
    app: camunda-platform
    app.kubernetes.io/name: zeebe-gateway
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: zeebe-gateway-8.2.12
    app.kubernetes.io/version: "8.2.12"
    app.kubernetes.io/component: zeebe-gateway
apiVersion: v1
data:
  gateway-log4j2.xml: |
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/charts/zeebe/templates/configmap.yaml
kind: ConfigMap
metadata:
  name: release-name-zeebe
  labels:
    app: camunda-platform
    app.kubernetes.io/name: zeebe
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: zeebe-8.2.12
    app.kubernetes.io/version: "8.2.8"
    app.kubernetes.io/component: zeebe-broker
apiVersion: v1
data:
  startup.sh: |
    #!/usr/bin/env bash
    set -eux -o pipefail

    export ZEEBE_BROKER_CLUSTER_NODEID=${ZEEBE_BROKER_CLUSTER_NODEID:-${K8S_NAME##*-}}

    if [ "$(ls -A /exporters/)" ]; then
      mkdir /usr/local/zeebe/exporters/
      cp -a /exporters/*.jar /usr/local/zeebe/exporters/
    else
      echo "No exporters available."
    fi

    exec /usr/local/zeebe/bin/broker

  broker-log4j2.xml: |
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/templates/release-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-camunda-platform-release
  labels:
    app: camunda-platform
    app.kubernetes.io/name: camunda-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: camunda-platform-8.2.12
    app.kubernetes.io/version: "8.2.12"
  annotations:
    {}
data:
  
  info: |
    - name: release-name
      namespace: default
      version: 8.2.12
      components:
      
      
      
      
      - name: Zeebe Gateway
        url: grpc://
        readiness:  http://release-name-zeebe-gateway.default:9600/actuator/health
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/elasticsearch/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ph-ee-elasticsearch-config
  labels:
    heritage: "Helm"
    release: "release-name"
    chart: "elasticsearch"
    app: "ph-ee-elasticsearch"
data:
  elasticsearch.yml: |
    xpack.security.enabled: false
    xpack.security.transport.ssl.enabled: false
    xpack.security.transport.ssl.verification_mode: certificate
    xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12
    xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12
    xpack.security.http.ssl.enabled: false
    xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12
    xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kafka/templates/controller-eligible/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-controller-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-25.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
data:
  server.properties: |-
    # Listeners configuration
    listeners=CLIENT://:9092,INTERNAL://:9094,CONTROLLER://:9093
    advertised.listeners=CLIENT://advertised-address-placeholder:9092,INTERNAL://advertised-address-placeholder:9094
    listener.security.protocol.map=CLIENT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
    # KRaft process roles
    process.roles=controller,broker
    #node.id=
    controller.listener.names=CONTROLLER
    controller.quorum.voters=0@kafka-controller-0.kafka-controller-headless.default.svc.cluster.local:9093,1@kafka-controller-1.kafka-controller-headless.default.svc.cluster.local:9093,2@kafka-controller-2.kafka-controller-headless.default.svc.cluster.local:9093
    log.dir=/bitnami/kafka/data
    # Interbroker configuration
    inter.broker.listener.name=INTERNAL
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-25.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  kafka-init.sh: |-
    #!/bin/bash

    set -o errexit
    set -o nounset
    set -o pipefail

    error(){
      local message="${1:?missing message}"
      echo "ERROR: ${message}"
      exit 1
    }

    retry_while() {
        local -r cmd="${1:?cmd is missing}"
        local -r retries="${2:-12}"
        local -r sleep_time="${3:-5}"
        local return_value=1

        read -r -a command <<< "$cmd"
        for ((i = 1 ; i <= retries ; i+=1 )); do
            "${command[@]}" && return_value=0 && break
            sleep "$sleep_time"
        done
        return $return_value
    }

    replace_in_file() {
        local filename="${1:?filename is required}"
        local match_regex="${2:?match regex is required}"
        local substitute_regex="${3:?substitute regex is required}"
        local posix_regex=${4:-true}

        local result

        # We should avoid using 'sed in-place' substitutions
        # 1) They are not compatible with files mounted from ConfigMap(s)
        # 2) We found incompatibility issues with Debian10 and "in-place" substitutions
        local -r del=$'\001' # Use a non-printable character as a 'sed' delimiter to avoid issues
        if [[ $posix_regex = true ]]; then
            result="$(sed -E "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        else
            result="$(sed "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        fi
        echo "$result" > "$filename"
    }

    kafka_conf_set() {
        local file="${1:?missing file}"
        local key="${2:?missing key}"
        local value="${3:?missing value}"

        # Check if the value was set before
        if grep -q "^[#\\s]*$key\s*=.*" "$file"; then
            # Update the existing key
            replace_in_file "$file" "^[#\\s]*${key}\s*=.*" "${key}=${value}" false
        else
            # Add a new key
            printf '\n%s=%s' "$key" "$value" >>"$file"
        fi
    }

    replace_placeholder() {
      local placeholder="${1:?missing placeholder value}"
      local password="${2:?missing password value}"
      sed -i "s/$placeholder/$password/g" "$KAFKA_CONFIG_FILE"
    }

    configure_external_access() {
      # Configure external hostname
      if [[ -f "/shared/external-host.txt" ]]; then
        host=$(cat "/shared/external-host.txt")
      elif [[ -n "${EXTERNAL_ACCESS_HOST:-}" ]]; then
        host="$EXTERNAL_ACCESS_HOST"
      elif [[ -n "${EXTERNAL_ACCESS_HOSTS_LIST:-}" ]]; then
        read -r -a hosts <<<"$(tr ',' ' ' <<<"${EXTERNAL_ACCESS_HOSTS_LIST}")"
        host="${hosts[$POD_ID]}"
      elif [[ "$EXTERNAL_ACCESS_HOST_USE_PUBLIC_IP" =~ ^(yes|true)$ ]]; then
        host=$(curl -s https://ipinfo.io/ip)
      else
        error "External access hostname not provided"
      fi

      # Configure external port
      if [[ -f "/shared/external-port.txt" ]]; then
        port=$(cat "/shared/external-port.txt")
      elif [[ -n "${EXTERNAL_ACCESS_PORT:-}" ]]; then
        if [[ "${EXTERNAL_ACCESS_PORT_AUTOINCREMENT:-}" =~ ^(yes|true)$ ]]; then
          port="$((EXTERNAL_ACCESS_PORT + POD_ID))"
        else
          port="$EXTERNAL_ACCESS_PORT"
        fi
      elif [[ -n "${EXTERNAL_ACCESS_PORTS_LIST:-}" ]]; then
        read -r -a ports <<<"$(tr ',' ' ' <<<"${EXTERNAL_ACCESS_PORTS_LIST}")"
        port="${ports[$POD_ID]}"
      else
        error "External access port not provided"
      fi
      # Configure Kafka advertised listeners
      sed -i -E "s|^(advertised\.listeners=\S+)$|\1,EXTERNAL://${host}:${port}|" "$KAFKA_CONFIG_FILE"
    }

    export KAFKA_CONFIG_FILE=/config/server.properties
    cp /configmaps/server.properties $KAFKA_CONFIG_FILE

    # Get pod ID and role, last and second last fields in the pod name respectively
    POD_ID=$(echo "$MY_POD_NAME" | rev | cut -d'-' -f 1 | rev)
    POD_ROLE=$(echo "$MY_POD_NAME" | rev | cut -d'-' -f 2 | rev)

    # Configure node.id and/or broker.id
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        if grep -q "broker.id" /bitnami/kafka/data/meta.properties; then
          ID="$(grep "broker.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
          kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
        else
          ID="$(grep "node.id" /bitnami/kafka/data/meta.properties | awk -F '=' '{print $2}')"
          kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
        fi
    else
        ID=$((POD_ID + KAFKA_MIN_ID))
        kafka_conf_set "$KAFKA_CONFIG_FILE" "node.id" "$ID"
    fi
    replace_placeholder "advertised-address-placeholder" "${MY_POD_NAME}.kafka-${POD_ROLE}-headless.default.svc.cluster.local"
    if [[ "${EXTERNAL_ACCESS_ENABLED:-false}" =~ ^(yes|true)$ ]]; then
      configure_external_access
    fi
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kibana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ph-ee-kibana-config
  labels: 
    app: kibana
    release: "release-name"
    heritage: Helm
data:
  kibana.yml: |
    monitoring.enabled: false
    xpack.encryptedSavedObjects.encryptionKey: 5f4dcc3b5aa765d61d8327deb882cf99
    server.ssl:
      enabled: false
      key: /usr/share/kibana/config/certs/elastic-certificate.pem
      certificate: /usr/share/kibana/config/certs/elastic-certificate.pem
    xpack.security.encryptionKey: ${KIBANA_ENCRYPTION_KEY}
    elasticsearch.ssl:
      certificateAuthorities: /usr/share/kibana/config/certs/elastic-certificate.pem
      verificationMode: certificate
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/minio/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: minio
  labels:
    app: minio
    chart: minio-5.0.14
    release: release-name
    heritage: Helm
data:
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of `mc ls`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(${MC} stat myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }
    
    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
      VERSIONING=$4
      OBJECTLOCKING=$5
    
      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for `true`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo "Purging bucket '$BUCKET'."
          set +e ; # don't exit if this fails
          ${MC} rm -r --force myminio/$BUCKET
          set -e ; # reset `e` as active
        else
          echo "Bucket '$BUCKET' does not exist, skipping purge."
        fi
      fi
    
    # Create the bucket if it does not exist and set objectlocking if enabled (NOTE: versioning will be not changed if OBJECTLOCKING is set because it enables versioning to the Buckets created)
    if ! checkBucketExists $BUCKET ; then
        if [ ! -z $OBJECTLOCKING ] ; then
          if [ $OBJECTLOCKING = true ] ; then
              echo "Creating bucket with OBJECTLOCKING '$BUCKET'"
              ${MC} mb --with-lock myminio/$BUCKET
          elif [ $OBJECTLOCKING = false ] ; then
                echo "Creating bucket '$BUCKET'"
                ${MC} mb myminio/$BUCKET
          fi
      elif [ -z $OBJECTLOCKING ] ; then
            echo "Creating bucket '$BUCKET'"
            ${MC} mb myminio/$BUCKET
      else
        echo "Bucket '$BUCKET' already exists."  
      fi
      fi
    
    
      # set versioning for bucket if objectlocking is disabled or not set
      if [ $OBJECTLOCKING = false ] ; then
      if [ ! -z $VERSIONING ] ; then
        if [ $VERSIONING = true ] ; then
            echo "Enabling versioning for '$BUCKET'"
            ${MC} version enable myminio/$BUCKET
        elif [ $VERSIONING = false ] ; then
            echo "Suspending versioning for '$BUCKET'"
            ${MC} version suspend myminio/$BUCKET
        fi
        fi
      else
          echo "Bucket '$BUCKET' versioning unchanged."
      fi
    
    
      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
      ${MC} anonymous set $POLICY myminio/$BUCKET
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
    # Create the buckets
    createBucket paymenthub-ee "public" false false false
    
  add-user: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # AccessKey and secretkey credentials file are added to prevent shell execution errors caused by special characters.
    # Special characters for example : ',",<,>,{,}
    MINIO_ACCESSKEY_SECRETKEY_TMP="/tmp/accessKey_and_secretKey_tmp"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkUserExists ()
    # Check if the user exists, by using the exit code of `mc admin user info`
    checkUserExists() {
      CMD=$(${MC} admin user info myminio $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) > /dev/null 2>&1)
      return $?
    }
    
    # createUser ($policy)
    createUser() {
      POLICY=$1
      #check accessKey_and_secretKey_tmp file
      if [[ ! -f $MINIO_ACCESSKEY_SECRETKEY_TMP ]];then
        echo "credentials file does not exist"
        return 1
      fi
      if [[ $(cat $MINIO_ACCESSKEY_SECRETKEY_TMP|wc -l) -ne 2 ]];then
        echo "credentials file is invalid"
        rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
        return 1
      fi
      USER=$(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP)
      # Create the user if it does not exist
      if ! checkUserExists ; then
        echo "Creating user '$USER'"
        cat $MINIO_ACCESSKEY_SECRETKEY_TMP | ${MC} admin user add myminio
      else
        echo "User '$USER' already exists."
      fi
      #clean up credentials files.
      rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
    
      # set policy for user
      if [ ! -z $POLICY -a $POLICY != " " ] ; then
          echo "Adding policy '$POLICY' for '$USER'"
          set +e ; # policy already attach errors out, allow it.
          ${MC} admin policy attach myminio $POLICY --user=$USER
          set -e
      else
          echo "User '$USER' has no policy attached."
      fi
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
    # Create the users
    echo console > $MINIO_ACCESSKEY_SECRETKEY_TMP
    echo console123 >> $MINIO_ACCESSKEY_SECRETKEY_TMP
    createUser consoleAdmin
    
  add-policy: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkPolicyExists ($policy)
    # Check if the policy exists, by using the exit code of `mc admin policy info`
    checkPolicyExists() {
      POLICY=$1
      CMD=$(${MC} admin policy info myminio $POLICY > /dev/null 2>&1)
      return $?
    }
    
    # createPolicy($name, $filename)
    createPolicy () {
      NAME=$1
      FILENAME=$2
    
      # Create the name if it does not exist
      echo "Checking policy: $NAME (in /config/$FILENAME.json)"
      if ! checkPolicyExists $NAME ; then
        echo "Creating policy '$NAME'"
      else
        echo "Policy '$NAME' already exists."
      fi
      ${MC} admin policy create myminio $NAME /config/$FILENAME.json
    
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
  add-svcacct: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # AccessKey and secretkey credentials file are added to prevent shell execution errors caused by special characters.
    # Special characters for example : ',",<,>,{,}
    MINIO_ACCESSKEY_SECRETKEY_TMP="/tmp/accessKey_and_secretKey_svcacct_tmp"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 2 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkSvcacctExists ()
    # Check if the svcacct exists, by using the exit code of `mc admin user svcacct info`
    checkSvcacctExists() {
      CMD=$(${MC} admin user svcacct info myminio $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) > /dev/null 2>&1)
      return $?
    }
    
    # createSvcacct ($user)
    createSvcacct () {
      USER=$1
      FILENAME=$2
      #check accessKey_and_secretKey_tmp file
      if [[ ! -f $MINIO_ACCESSKEY_SECRETKEY_TMP ]];then
        echo "credentials file does not exist"
        return 1
      fi
      if [[ $(cat $MINIO_ACCESSKEY_SECRETKEY_TMP|wc -l) -ne 2 ]];then
        echo "credentials file is invalid"
        rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
        return 1
      fi
      SVCACCT=$(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP)
      # Create the svcacct if it does not exist
      if ! checkSvcacctExists ; then
        echo "Creating svcacct '$SVCACCT'"
        # Check if policy file is define
        if [ -z $FILENAME ]; then
          ${MC} admin user svcacct add --access-key $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) --secret-key $(tail -n1 $MINIO_ACCESSKEY_SECRETKEY_TMP) myminio $USER
        else
          ${MC} admin user svcacct add --access-key $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) --secret-key $(tail -n1 $MINIO_ACCESSKEY_SECRETKEY_TMP) --policy /config/$FILENAME.json myminio $USER
        fi
      else
        echo "Svcacct '$SVCACCT' already exists."
      fi
      #clean up credentials files.
      rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
  custom-command: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # runCommand ($@)
    # Run custom mc command
    runCommand() {
      ${MC} "$@"
      return $?
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_web/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
data:
  configuration.properties: |
    oauth.enabled false
    oauth.basicAuth true
    oauth.basicAuthToken Y2xpZW50Og==
    oauth.serverUrl https://ops-bk.sandbox.mifos.io
    serverUrl https://ops.sandbox.mifos.io
    auth.enabled false
    auth.tenant phdefault
metadata:
  name: ph-ee-operations-web-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operationsmysql/templates/primary/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: operationsmysql
  namespace: "default"
  labels:
    app.kubernetes.io/name: operationsmysql
    helm.sh/chart: operationsmysql-9.4.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
data:
  my.cnf: |-
    [mysqld]
    default_authentication_plugin=mysql_native_password
    skip-name-resolve
    explicit_defaults_for_timestamp
    basedir=/opt/bitnami/mysql
    plugin_dir=/opt/bitnami/mysql/lib/plugin
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    datadir=/bitnami/mysql/data
    tmpdir=/opt/bitnami/mysql/tmp
    max_allowed_packet=16M
    bind-address=*
    pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
    log-error=/opt/bitnami/mysql/logs/mysqld.log
    character-set-server=UTF8
    collation-server=utf8_general_ci
    slow_query_log=0
    slow_query_log_file=/opt/bitnami/mysql/logs/mysqld.log
    long_query_time=10.0
    
    [client]
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    default-character-set=UTF8
    plugin_dir=/opt/bitnami/mysql/lib/plugin
    
    [manager]
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operationsmysql/templates/primary/initialization-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: operationsmysql-init-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: operationsmysql
    helm.sh/chart: operationsmysql-9.4.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
data:
  setup.sql: |-
    CREATE DATABASE messagegateway;
    CREATE DATABASE `rhino`;
    CREATE DATABASE `gorilla`;
    CREATE DATABASE `lion`;
    CREATE DATABASE `identity_account_mapper`;
    CREATE DATABASE `voucher_management`;
    GRANT ALL PRIVILEGES ON `rhino`.* TO 'mifos';
    GRANT ALL PRIVILEGES ON `gorilla`.* TO 'mifos';
    GRANT ALL PRIVILEGES ON `lion`.* TO 'mifos';
    GRANT ALL ON *.* TO 'root'@'%';
    GRANT ALL PRIVILEGES ON messagegateway.* TO 'mifos';
    GRANT ALL PRIVILEGES ON `identity_account_mapper`.* TO 'mifos';
    GRANT ALL PRIVILEGES ON `voucher_management`.* TO 'mifos';
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-redis-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.9.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-redis-health
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.9.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-redis-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.9.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--requirepass" "${REDIS_PASSWORD}")
    ARGS+=("--masterauth" "${REDIS_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
  start-replica.sh: |
    #!/bin/bash

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        full_hostname="${hostname}.${HEADLESS_SERVICE}"
        echo "${full_hostname}"
    }

    REDISPORT=$(get_port "$HOSTNAME" "REDIS")

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi

    echo "" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-port $REDISPORT" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis/etc/replica.conf
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--replicaof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    ARGS+=("--requirepass" "${REDIS_PASSWORD}")
    ARGS+=("--masterauth" "${REDIS_MASTER_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: ph-ee-g2psandbox/templates/config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ph-ee-config
data:

  application-bb.properties: |
    store.local.interop.host=https://rhino.mifos.g2pconnect.io
    store.local.customer.host=https://rhino.mifos.g2pconnect.io
    zeebe.broker.contactpoint={{ .Release.Name }}-zeebe-gateway:26500
  
    security.oauth2.resource.jwt.key-uri=http://ops-bk.mifos.g2pconnect.io/oauth/token_key
    rest.authorization.enabled=false
    rest.authorization.host=http://ops-bk.mifos.g2pconnect.io
  
    dfspids=rhino, gorilla
  application-fin12.properties: |-
    ams.localenabled=true
    zeebe.broker.contactpoint={{ .Release.Name }}-zeebe-gateway:26500
  application-tenants.properties: |-
    bpmns.tenants[0].id= lion
    bpmns.tenants[0].flows.payment-transfer= minimal_mock_fund_transfer_account_lookup-{dfspid}
    bpmns.tenants[1].id= rhino
    bpmns.tenants[1].flows.payment-transfer= minimal_mock_fund_transfer-{dfspid}
    bpmns.tenants[1].flows.outbound-transfer-request= minimal_mock_transfer_request-{dfspid}
    bpmns.tenants[2].id= gorilla
    bpmns.tenants[2].flows.payment-transfer= PayerFundTransfer-{dfspid}
    bpmns.tenants[2].flows.outbound-transfer-request= {ps}_flow_{ams}-{dfspid}
  application-tenantsConnection.properties: |-
    tenants.connections[0].auto_update=true
    tenants.connections[0].deadlock_max_retries=0
    tenants.connections[0].deadlock_max_retry_interval=1
    tenants.connections[0].driver_class=com.mysql.cj.jdbc.Driver
    tenants.connections[0].jdbcProtocol=jdbc
    tenants.connections[0].jdbcSubProtocol=mysql
    tenants.connections[0].name=gorilla
    tenants.connections[0].pool_abandon_when_percentage_full=50
    tenants.connections[0].pool_initial_size=5
    tenants.connections[0].pool_log_abandoned=1
    tenants.connections[0].pool_max_active=40
    tenants.connections[0].pool_max_idle=10
    tenants.connections[0].pool_min_evictable_idle_time_millis=60000
    tenants.connections[0].pool_min_idle=20
    tenants.connections[0].pool_remove_abandoned=1
    tenants.connections[0].pool_remove_abandoned_timeout=60
    tenants.connections[0].pool_suspect_timeout=60
    tenants.connections[0].pool_test_on_borrow=1
    tenants.connections[0].pool_time_between_eviction_runs_millis=34000
    tenants.connections[0].pool_validation_interval=30000
    tenants.connections[0].schema_connection_parameters=null
    tenants.connections[0].schema_name=gorilla
    tenants.connections[0].schema_password=password
    tenants.connections[0].schema_server=operationsmysql
    tenants.connections[0].schema_server_port=3306
    tenants.connections[0].schema_username=mifos
    tenants.connections[1].auto_update=true
    tenants.connections[1].deadlock_max_retries=0
    tenants.connections[1].deadlock_max_retry_interval=1
    tenants.connections[1].driver_class=com.mysql.cj.jdbc.Driver
    tenants.connections[1].jdbcProtocol=jdbc
    tenants.connections[1].jdbcSubProtocol=mysql
    tenants.connections[1].name=rhino
    tenants.connections[1].pool_abandon_when_percentage_full=50
    tenants.connections[1].pool_initial_size=5
    tenants.connections[1].pool_log_abandoned=1
    tenants.connections[1].pool_max_active=40
    tenants.connections[1].pool_max_idle=10
    tenants.connections[1].pool_min_evictable_idle_time_millis=60000
    tenants.connections[1].pool_min_idle=20
    tenants.connections[1].pool_remove_abandoned=1
    tenants.connections[1].pool_remove_abandoned_timeout=60
    tenants.connections[1].pool_suspect_timeout=60
    tenants.connections[1].pool_test_on_borrow=1
    tenants.connections[1].pool_time_between_eviction_runs_millis=34000
    tenants.connections[1].pool_validation_interval=30000
    tenants.connections[1].schema_connection_parameters=null
    tenants.connections[1].schema_name=rhino
    tenants.connections[1].schema_password=password
    tenants.connections[1].schema_server=operationsmysql
    tenants.connections[1].schema_server_port=3306
    tenants.connections[1].schema_username=mifos
    tenants.connections[2].auto_update=true
    tenants.connections[2].deadlock_max_retries=0
    tenants.connections[2].deadlock_max_retry_interval=1
    tenants.connections[2].driver_class=com.mysql.cj.jdbc.Driver
    tenants.connections[2].jdbcProtocol=jdbc
    tenants.connections[2].jdbcSubProtocol=mysql
    tenants.connections[2].name=lion
    tenants.connections[2].pool_abandon_when_percentage_full=50
    tenants.connections[2].pool_initial_size=5
    tenants.connections[2].pool_log_abandoned=1
    tenants.connections[2].pool_max_active=40
    tenants.connections[2].pool_max_idle=10
    tenants.connections[2].pool_min_evictable_idle_time_millis=60000
    tenants.connections[2].pool_min_idle=20
    tenants.connections[2].pool_remove_abandoned=1
    tenants.connections[2].pool_remove_abandoned_timeout=60
    tenants.connections[2].pool_suspect_timeout=60
    tenants.connections[2].pool_test_on_borrow=1
    tenants.connections[2].pool_time_between_eviction_runs_millis=34000
    tenants.connections[2].pool_validation_interval=30000
    tenants.connections[2].schema_connection_parameters=null
    tenants.connections[2].schema_name=lion
    tenants.connections[2].schema_password=password
    tenants.connections[2].schema_server=operationsmysql
    tenants.connections[2].schema_server_port=3306
    tenants.connections[2].schema_username=mifos
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/minio/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: minio
  labels:
    app: minio
    chart: minio-5.0.14
    release: release-name
    heritage: Helm
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "10Gi"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/billPay/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-connector-bill-pay-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/channel/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-connector-channel-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/connector_bulk/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-connector-bulk-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/crm/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-connector-crm-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_es/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-importer-es-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_rdbms/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-importer-rdbms-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/messagegateway/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: message-gateway-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/notifications/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-connector-notification-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_app/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-operations-app-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_web/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-operations-web-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_ams_mifos/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-connector-ams-mifos-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_gsma/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-connector-gsma-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_mojaloop/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-connector-mojaloop-java-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/zeebe_ops/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: ph-ee-zeebe-ops-c-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/billPay/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-connector-bill-pay-c-role-binding
subjects:
- kind: ServiceAccount
  name:  ph-ee-connector-bill-pay # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-connector-bill-pay-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/channel/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-connector-channel-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-channel # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-connector-channel-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/connector_bulk/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-connector-bulk-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-bulk # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-connector-bulk-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/crm/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-connector-crm-c-role-binding
subjects:
- kind: ServiceAccount
  name:  ph-ee-connector-crm # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-connector-crm-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_es/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-importer-es-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-importer-es # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-importer-es-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_rdbms/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-importer-rdbms-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-importer-rdbms # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-importer-rdbms-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/messagegateway/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: message-gateway-c-role-binding
subjects:
- kind: ServiceAccount
  name: message-gateway # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: message-gateway-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/notifications/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-connector-notification-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-notification # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-connector-notification-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_app/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-operations-app-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-operations-app # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-operations-app-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_web/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-operations-web-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-operations-web # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-operations-web-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_ams_mifos/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-connector-ams-mifos-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-ams-mifos # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-connector-ams-mifos-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_gsma/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-connector-gsma-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-gsma # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-connector-gsma-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_mojaloop/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-connector-mojaloop-java-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-mojaloop-java # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-connector-mojaloop-java-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/zeebe_ops/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ph-ee-zeebe-ops-c-role-binding
subjects:
- kind: ServiceAccount
  name: ph-ee-zeebe-ops # name of your service account
  namespace: default # this is the namespace your service account is in
roleRef: # referring to your ClusterRole
  kind: ClusterRole
  name: ph-ee-zeebe-ops-c-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/billPay/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-connector-bill-pay-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/channel/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-connector-channel-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/connector_bulk/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-connector-bulk-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/crm/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-connector-crm-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_es/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-importer-es-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_rdbms/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-importer-rdbms-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/messagegateway/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: message-gateway-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/notifications/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-connector-notification-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_app/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-operations-app-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_web/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-operations-web-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_ams_mifos/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-connector-ams-mifos-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_gsma/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-connector-gsma-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_mojaloop/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-connector-mojaloop-java-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/zeebe_ops/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ph-ee-zeebe-ops-role
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs: ["get", "create", "update"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/templates/post-installation-job/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: job-creator-role
rules:
- apiGroups: ['*']
  resources: ["secrets"]
  verbs: ["create", "get", "watch", "list"]
- apiGroups: ['*']
  resources: ["services", "pods"]
  verbs: [ "get", "watch", "list"]
- apiGroups: ['*']
  resources: ["pods/portforward"]
  verbs: [ "create"]
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/billPay/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-connector-bill-pay-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-connector-bill-pay-role
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-bill-pay
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/channel/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-connector-channel-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-connector-channel-role
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-channel
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/connector_bulk/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-connector-bulk-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-connector-bulk-role
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-bulk
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/crm/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-connector-crm-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-connector-crm-role
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-crm
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_es/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-importer-es-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-importer-es-role
subjects:
- kind: ServiceAccount
  name: ph-ee-importer-es
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_rdbms/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-importer-rdbms-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-importer-rdbms-role
subjects:
- kind: ServiceAccount
  name: ph-ee-importer-rdbms
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/messagegateway/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: message-gateway-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: message-gateway-role
subjects:
- kind: ServiceAccount
  name: message-gateway
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/notifications/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-connector-notification-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-connector-notification-role
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-notification
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_app/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-operations-app-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-operations-app-role
subjects:
- kind: ServiceAccount
  name: ph-ee-operations-app
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_web/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-operations-web-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-operations-web-role
subjects:
- kind: ServiceAccount
  name: ph-ee-operations-web
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_ams_mifos/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-connector-ams-mifos-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-connector-ams-mifos-role
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-ams-mifos
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_gsma/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-connector-gsma-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-connector-gsma-role
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-gsma
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_mojaloop/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-connector-mojaloop-java-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-connector-mojaloop-java-role
subjects:
- kind: ServiceAccount
  name: ph-ee-connector-mojaloop-java
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/zeebe_ops/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ph-ee-zeebe-ops-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ph-ee-zeebe-ops-role
subjects:
- kind: ServiceAccount
  name: ph-ee-zeebe-ops
  namespace: default
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/templates/post-installation-job/role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: job-creator-role-binding
subjects:
- kind: ServiceAccount
  name: job-creator
roleRef:
  kind: Role
  name: job-creator-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: ph-ee-g2psandbox/charts/account_mapper/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-identity-account-mapper
  name: ph-ee-identity-account-mapper
spec:
  ports:
    - name: port
      port: 80
      protocol: TCP
      targetPort: 8080
  selector:
    app: ph-ee-identity-account-mapper
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/billPay/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-connector-bill-pay
  name: ph-ee-connector-bill-pay
spec:
  ports:
    - name: port
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    app: ph-ee-connector-bill-pay
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/charts/zeebe-gateway/templates/gateway-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: "release-name-zeebe-gateway"
  labels:
    app: camunda-platform
    app.kubernetes.io/name: zeebe-gateway
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: zeebe-gateway-8.2.12
    app.kubernetes.io/version: "8.2.12"
    app.kubernetes.io/component: zeebe-gateway
  annotations:
spec:
  type: ClusterIP
  selector:
      app: camunda-platform
      app.kubernetes.io/name: zeebe-gateway
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: camunda-platform
      app.kubernetes.io/component: zeebe-gateway
  ports:
    - port: 9600
      protocol: TCP
      name: http
    - port: 26500
      protocol: TCP
      name: gateway
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/charts/zeebe/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: "release-name-zeebe"
  labels:
    app: camunda-platform
    app.kubernetes.io/name: zeebe
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: zeebe-8.2.12
    app.kubernetes.io/version: "8.2.8"
    app.kubernetes.io/component: zeebe-broker
  annotations:
    {}
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
    - port: 9600
      protocol: TCP
      name: http
    - port: 26502
      protocol: TCP
      name: internal
    - port: 26501
      protocol: TCP
      name: command
  selector:
    app: camunda-platform
    app.kubernetes.io/name: zeebe
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    app.kubernetes.io/component: zeebe-broker
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/channel/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-connector-channel
  name: ph-ee-connector-channel
spec:
  ports:
    - name: port
      port: 8443
      protocol: TCP
      targetPort: 8443
    - name: http
      port: 82
      protocol: TCP
      targetPort: 8443
  selector:
    app: ph-ee-connector-channel
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/connector_bulk/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-connector-bulk
  name: ph-ee-connector-bulk
  annotations:
    konghq.com/protocol: https
spec:
  ports:
    - name: port
      port: 8443
      protocol: TCP
      targetPort: 8443
  selector:
    app: ph-ee-connector-bulk
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/crm/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-connector-crm
  name: ph-ee-connector-crm
spec:
  ports:
    - name: port
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    app: ph-ee-connector-crm
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: ph-ee-elasticsearch
  labels:
    heritage: "Helm"
    release: "release-name"
    chart: "elasticsearch"
    app: "ph-ee-elasticsearch"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    release: "release-name"
    chart: "elasticsearch"
    app: "ph-ee-elasticsearch"
  publishNotReadyAddresses: false
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: ph-ee-elasticsearch-headless
  labels:
    heritage: "Helm"
    release: "release-name"
    chart: "elasticsearch"
    app: "ph-ee-elasticsearch"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "ph-ee-elasticsearch"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kafka/templates/controller-eligible/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-controller-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-25.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-interbroker
      port: 9094
      protocol: TCP
      targetPort: interbroker
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: client
    - name: tcp-controller
      protocol: TCP
      port: 9093
      targetPort: controller
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-25.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: client
      nodePort: null
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: kafka
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kibana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ph-ee-kibana
  labels: 
    app: kibana
    release: "release-name"
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - port: 5601
      protocol: TCP
      name: http
      targetPort: 5601
  selector:
    app: kibana
    release: "release-name"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/messagegateway/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: message-gateway
  name: message-gateway
spec:
  ports:
    - name: port
      port: 80
      protocol: TCP
      targetPort: 9191
  selector:
    app: message-gateway
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/minio/templates/console-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio-console
  labels:
    app: minio
    chart: minio-5.0.14
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9001
      protocol: TCP
      targetPort: 9001
  selector:
    app: minio
    release: release-name
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio
  labels:
    app: minio
    chart: minio-5.0.14
    release: release-name
    heritage: Helm
    monitoring: "true"
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio
    release: release-name
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/mockpayment/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-connector-mock-payment-schema
  name: ph-ee-connector-mock-payment-schema
spec:
  ports:
    - name: port
      port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    app: ph-ee-connector-mock-payment-schema
  sessionAffinity: None
  type: LoadBalancer
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/notifications/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-connector-notifications
  name: ph-ee-connector-notifications
spec:
  ports:
    - name: port
      port: 80
      protocol: TCP
      targetPort: 5000
    - name: actuator
      protocol: TCP
      port: 8080   # Port where Actuator endpoints are exposed
      targetPort: 8080
  selector:
    app: ph-ee-connector-notifications
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_app/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-operations-app
  name: ph-ee-operations-app
spec:
  ports:
    - name: port
      port: 80
      protocol: TCP
      targetPort: 5000
  selector:
    app: ph-ee-operations-app
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_web/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ph-ee-operations-web
spec:
  selector:
    app: ph-ee-operations-web
  ports:
    - protocol: TCP
      port: 4200
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operationsmysql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: operationsmysql-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: operationsmysql
    helm.sh/chart: operationsmysql-9.4.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: mysql
      port: 3306
      targetPort: mysql
  selector: 
    app.kubernetes.io/name: operationsmysql
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: primary
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operationsmysql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: operationsmysql
  namespace: "default"
  labels:
    app.kubernetes.io/name: operationsmysql
    helm.sh/chart: operationsmysql-9.4.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: mysql
      port: 3306
      protocol: TCP
      targetPort: mysql
      nodePort: null
  selector: 
    app.kubernetes.io/name: operationsmysql
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: primary
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_ams_mifos/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-connector-ams-mifos
  name: ph-ee-connector-ams-mifos
spec:
  ports:
    - name: port
      port: 80
      protocol: TCP
      targetPort: 5000
    - name: http
      port: 70
      protocol: TCP
      targetPort: 7070
  selector:
    app: ph-ee-connector-ams-mifos
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_gsma/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-connector-gsma
  name: ph-ee-connector-gsma
spec:
  ports:
    - name: port
      port: 80
      protocol: TCP
      targetPort: 5000
    - name: actuator
      protocol: TCP
      port: 8080   # Port where Actuator endpoints are exposed
      targetPort: 8080
  selector:
    app: ph-ee-connector-gsma
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_mojaloop/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-connector-mojaloop-java
  name: ph-ee-connector-mojaloop-java
spec:
  ports:
    - name: port
      port: 80
      protocol: TCP
      targetPort: 5000
    - name: actuator
      protocol: TCP
      port: 8080   # Port where Actuator endpoints are exposed
      targetPort: 8080
  selector:
    app: ph-ee-connector-mojaloop-java
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-redis-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.9.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.9.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: master
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/redis/templates/replicas/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.9.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: replica
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: replica
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/vouchers/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-vouchers
  name: ph-ee-vouchers
spec:
  ports:
    - name: port
      port: 80
      protocol: TCP
      targetPort: 8080

  selector:
    app: ph-ee-vouchers
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/zeebe_ops/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ph-ee-zeebe-ops
  name: ph-ee-zeebe-ops
spec:
  ports:
    - name: port
      port: 80
      protocol: TCP
      targetPort: 5000
    - name: actuator
      protocol: TCP
      port: 8080   # Port where Actuator endpoints are exposed
      targetPort: 8080
  selector:
    app: ph-ee-zeebe-ops
  sessionAffinity: None
  type: ClusterIP
---
# Source: ph-ee-g2psandbox/charts/account_mapper/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-identity-account-mapper
  labels:
    app: ph-ee-identity-account-mapper
spec:
  replicas: 
  selector:
    matchLabels:
      app: ph-ee-identity-account-mapper
  template:
    metadata:
      labels:
        app: ph-ee-identity-account-mapper
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      containers:
        - name: ph-ee-identity-account-mapper
          image: "docker.io/openmf/ph-ee-identity-account-mapper:v1.4.0"
          imagePullPolicy: "Always"
          ports:
            - containerPort: 8080
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: 8080
            initialDelaySeconds: 20
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: 8080
            initialDelaySeconds: 20
            periodSeconds: 30
          env:
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"
            - name: "SPRING_CACHE_TIME_TO_LIVE"
              value: "30"
            - name: "SPRING_CACHE_TIME_TO_IDLE"
              value: "30"
            - name: "SPRING_CACHE_MAX_ENTRIES_HEAP"
              value: "1000"
            - name: "SPRING_CACHE_MAX_BYTE_OFF_HEAP"
              value: "10"
            - name: "SPRING_CACHE_MAX_BYTE_DISK"
              value: "10"
            - name: "SPRING_DATASOURCE_URL"
              value: "jdbc:mysql:thin://operationsmysql:3306/identity_account_mapper"
            - name: "SPRING_DATASOURCE_USERNAME"
              value: "mifos"
            - name: "SPRING_DATASOURCE_PASSWORD"
              value: "password"
            - name:  "LOGGING_LEVEL_ROOT"
              value: ""
            - name: "ACCOUNT_VALIDATION_ENABLED"
              value: "false"
            - name: "ACCOUNT_VALIDATOR_CONNECTOR"
              value: "gsma"
            - name: "CALLBACK_ENABLED"
              value: ""
          envFrom: 
            null
          securityContext: 
            null
          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/billPay/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-connector-bill-pay
  labels:
    app: ph-ee-connector-bill-pay
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-connector-bill-pay
  template:
    metadata:
      labels:
        app: ph-ee-connector-bill-pay
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      terminationGracePeriodSeconds: 30
      containers:
        - name: ph-ee-connector-bill-pay
          image: "docker.io/openmf/ph-ee-bill-pay:v1.0.0"
          ports:
            - containerPort: 8080
          imagePullPolicy: "Always"
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
#          livenessProbe:
#            httpGet:
#              path: /actuator/health/liveness
#              port: 8443
#              scheme: HTTPS
#            initialDelaySeconds: 20
#            periodSeconds: 180
#          readinessProbe:
#            httpGet:
#              path: /actuator/health/readiness
#              port: 8443
#              scheme: HTTPS
#            initialDelaySeconds: 20
#            periodSeconds: 180
          env:
            - name: "SPRING_PROFILES_ACTIVE"
              value: "bb,tenants"
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"  
            - name: "DFSPIDS"
              value: "rhino,gorilla,lion"
            - name: "CONNECTOR_CONTACTPOINT"
              value: ""
            - name: "BILLPAY_CONTACTPOINT"
              value: ""        
          envFrom: 
            []
          securityContext: 
            null              
          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/charts/zeebe-gateway/templates/gateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "release-name-zeebe-gateway"
  labels:
    app: camunda-platform
    app.kubernetes.io/name: zeebe-gateway
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: zeebe-gateway-8.2.12
    app.kubernetes.io/version: "8.2.12"
    app.kubernetes.io/component: zeebe-gateway
  annotations:
    {}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: camunda-platform
      app.kubernetes.io/name: zeebe-gateway
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: camunda-platform
      app.kubernetes.io/component: zeebe-gateway
  template:
    metadata:
      labels:
        app: camunda-platform
        app.kubernetes.io/name: zeebe-gateway
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/part-of: camunda-platform
        helm.sh/chart: zeebe-gateway-8.2.12
        app.kubernetes.io/version: "8.2.12"
        app.kubernetes.io/component: zeebe-gateway
      annotations:
        {}
    spec:
      imagePullSecrets:
        []
      containers:
        - name: zeebe-gateway
          image: "camunda/zeebe:8.2.12"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9600
              name: http
            - containerPort: 26500
              name: gateway
            - containerPort: 26502
              name: internal
          env:
            - name: ZEEBE_STANDALONE_GATEWAY
              value: "true"
            - name: ZEEBE_GATEWAY_CLUSTER_CLUSTERNAME
              value: release-name-zeebe
            - name: ZEEBE_GATEWAY_CLUSTER_MEMBERID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ZEEBE_LOG_LEVEL
              value: "warn"
            - name: JAVA_TOOL_OPTIONS
              value: "-XX:+ExitOnOutOfMemoryError"
            - name: ZEEBE_GATEWAY_CLUSTER_CONTACTPOINT
              value: release-name-zeebe:26502
            - name: ZEEBE_GATEWAY_NETWORK_HOST
              value: 0.0.0.0
            - name: ZEEBE_GATEWAY_NETWORK_PORT
              value: "26500"
            - name: ZEEBE_GATEWAY_CLUSTER_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ZEEBE_GATEWAY_CLUSTER_PORT
              value: "26502"
            - name: ZEEBE_GATEWAY_MONITORING_HOST
              value: 0.0.0.0
            - name: ZEEBE_GATEWAY_MONITORING_PORT
              value: "9600"
            - name: ZEEBE_GATEWAY_THREADS_MANAGEMENTTHREADS
              value: "4"
            - name: ZEEBE_GATEWAY_MONITORING_ENABLED
              value: "true"
          volumeMounts:
          readinessProbe:
            httpGet:
              path: /actuator/health
              scheme: HTTP
              port: 9600
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 5
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 400m
              memory: 450Mi
            requests:
              cpu: 400m
              memory: 450Mi
      volumes:
        - name: config
          configMap:
            name: release-name-zeebe-gateway-gateway
            defaultMode: 484
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - zeebe-gateway
            topologyKey: kubernetes.io/hostname
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/channel/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-connector-channel
  labels:
    app: ph-ee-connector-channel
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-connector-channel
  template:
    metadata:
      labels:
        app: ph-ee-connector-channel
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      terminationGracePeriodSeconds: 30
      containers:
        - name: ph-ee-connector-channel
          image: "docker.io/openmf/ph-ee-connector-channel:v1.10.0"
          ports:
            - containerPort: 8443
          imagePullPolicy: "Always"
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 180
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 180
          env:
            - name: "SPRING_PROFILES_ACTIVE"
              value: "bb,tenants"
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"  
            - name: "DFSPIDS"
              value: "rhino,gorilla,lion"
            - name: "TRANSACTION-ID-LENGTH"
              value: "20"
            - name: "MPESA_NOTIFICATION_SUCCESS_ENABLED"
              value: "false"
            - name: "MPESA_NOTIFICATION_FAILURE_ENABLED"
              value: "false"
            - name:  "LOGGING_LEVEL_ROOT"
              value: "INFO"
            - name: "AMS"
              value: ""
            - name: "LOGGING_PATTERN_CONSOLE"
              value: "%d{dd-MM-yyyy HH:mm:ss.SSS} %magenta([%thread]) %highlight(%-5level) %logger.%M - %msg%n"
            - name: "gsma_payee_tenant"
              value: ""
            - name: "operations_url"
              value: "http://ph-ee-operations-app:80/api/v1"
            - name: "operations_auth-enabled"
              value: "false"
            - name: "CHANNEL_TENANTPRIMARY_CLIENTID"
              value: "mifos"
            - name: "CHANNEL_TENANTPRIMARY_CLIENTSECRET"
              value: "password"
            - name: "CHANNEL_TENANTPRIMARY_TENANT"
              value: "rhino"
            - name: "CHANNEL_TENANTSECONDARY_CLIENTID"
              value: "mifos"
            - name: "CHANNEL_TENANTSECONDARY_CLIENTSECRET"
              value: "password"
            - name: "CHANNEL_TENANTSECONDARY_TENANT"
              value: "gorilla"
            - name: "redis_idempotency_enabled"
              value: "true"
            - name: "redis_host"
              value: "127.0.0.1"
            - name: "redis_port"
              value: "6379"
            - name: "redis_idempotency_keyFormat"
              value: ""
            - name: "redis_idempotency_apiList"
              value: ""
            - name: "redis_password"
              valueFrom:
                secretKeyRef:
                  name: "release-name-redis" 
                  key: "redis-password"

            - name: "server_ssl_key-password"
              value: "password" 
            - name: "server_ssl_key-store-password"
              value: "password"        
          envFrom: 
            []
          securityContext: 
            privileged: false
            runAsUser: 0              
          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/connector_bulk/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-connector-bulk
  labels:
    app: ph-ee-connector-bulk
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-connector-bulk
  template:
    metadata:
      labels:
        app: ph-ee-connector-bulk
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      terminationGracePeriodSeconds: 30
      containers:
        - name: ph-ee-connector-bulk
          image: "docker.io/openmf/ph-ee-bulk-processor:v1.11.0"
          imagePullPolicy: "Always"
          ports:
            - containerPort: 5000
          resources:
            limits:
              memory: "512M"
              cpu: "256m"
            requests:
              memory: "256M"
              cpu: "100m"
          env:
            - name: "SPRING_PROFILES_ACTIVE"
              value: "bb"
            - name: "CAMEL_DISABLE-SSL"
              value: "true"
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"
            - name: "TENANTS"
              value: "rhino,gorilla,lion"
            - name: "APPLICATION_BUCKET-NAME"
              value: "paymenthub-ee"
            - name: "OPERATIONS-APP_CONTACTPOINT"
              value: "http://ph-ee-operations-app:80"
            - name: "OPERATIONS-APP_ENDPOINTS_BATCH-TRANSACTION"
              value: "/api/v1/batch/transactions"
            - name: "CONFIG_PARTYLOOKUP_ENABLE"
              value: "false"
            - name: "CONFIG_APPROVAL_ENABLE"
              value: "false"
            - name: "CONFIG_ORDERING_ENABLE"
              value: "false"
            - name: "CONFIG_ORDERING_FIELD"
              value: ""
            - name: "CONFIG_SPLITTING_ENABLE"
              value: "true"
            - name: "CONFIG_SPLITTING_SUB_BATCH_SIZE"
              value: "5"
            - name: "CONFIG_FORMATTING_ENABLE"
              value: "false"
            - name: "CONFIG_FORMATTING_STANDARD"
              value: "DEFAULT"
            - name: "CONFIG_MERGEBACK_ENABLE"
              value: "true"
            - name: "CONFIG_BACKPRESSURE_ENABLE"
              value: "false"
            - name: "CONFIG_COMPLETION-THRESHOLD-CHECK_ENABLE"
              value: "true"
            - name: "CONFIG_COMPLETION-THRESHOLD-CHECK_COMPLETION-RATE"
              value: "95"
            - name: "CLOUD_AWS_REGION_STATIC"
              valueFrom:
                secretKeyRef:
                  name: "bulk-processor-secret"
                  key: "aws-region"
            - name: "BP_JAVA_OPTS"
              value: "-Xmx400m -Xms400m"
            - name: "CLOUD_AWS_S3BASEURL"
              value: http://minio:9000
            - name: "AWS_ACCESS_KEY"
              valueFrom:
                secretKeyRef:
                  name: "bulk-processor-secret"
                  key: "aws-access-key"
            - name: "AWS_SECRET_KEY"
              valueFrom:
                secretKeyRef:
                  name: "bulk-processor-secret"
                  key: "aws-secret-key"
            - name: "IDENTITY_MAPPER_CONTACTPOINT"
              value: "http://ph-ee-identity-account-mapper:80"        
          envFrom: 
            []
          securityContext: 
            privileged: false
            runAsUser: 0
          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/crm/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-connector-crm
  labels:
    app: ph-ee-connector-crm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-connector-crm
  template:
    metadata:
      labels:
        app: ph-ee-connector-crm
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      terminationGracePeriodSeconds: 30
      containers:
        - name: ph-ee-connector-crm
          image: "docker.io/openmf/ph-ee-connector-crm:v1.0.0"
          ports:
            - containerPort: 8080
          imagePullPolicy: "Always"
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
#          livenessProbe:
#            httpGet:
#              path: /actuator/health/liveness
#              port: 8443
#              scheme: HTTPS
#            initialDelaySeconds: 20
#            periodSeconds: 180
#          readinessProbe:
#            httpGet:
#              path: /actuator/health/readiness
#              port: 8443
#              scheme: HTTPS
#            initialDelaySeconds: 20
#            periodSeconds: 180
          env:
            - name: "SPRING_PROFILES_ACTIVE"
              value: "bb,tenants"
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"  
            - name: "DFSPIDS"
              value: "rhino,gorilla,lion"        
          envFrom: 
            []
          securityContext: 
            null              
          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_es/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-importer-es
  labels:
    app: ph-ee-importer-es
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-importer-es
  template:
    metadata:
      labels:
        app: ph-ee-importer-es
    spec:
      containers:
        - name: ph-ee-importer-es
          image: "docker.io/openmf/ph-ee-importer-es:v1.14.0"
          imagePullPolicy: "Always"
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
          env:
          - name: "reporting.enabled"
            value: "false"
          - name: "LOGGING_LEVEL_ROOT"
            value: "INFO"
          - name: "importer_elasticsearch_url"
            value: "http://ph-ee-elasticsearch:9200/"
          - name: "JAVA_TOOL_OPTIONS"
            value: "-Xmx256M"
          - name: "LOGGING_PATTERN_CONSOLE"
            value: "%d{dd-MM-yyyy HH:mm:ss.SSS} %magenta([%thread]) %highlight(%-5level) %logger.%M - %msg%n"
          - name: "reporting.fields.amount"
            value: "false"
          - name: "reporting.fields.accountId"
            value: "false"
          - name: "reporting.fields.errorCode"
            value: "false"
          - name: "reporting.fields.errorDescription"
            value: "false"
          - name: "reporting.fields.externalId"
            value: "false"
          - name: "reporting.fields.initiator"
            value: "false"
          - name: "reporting.fields.initiatorType"
            value: "false"
          - name: "reporting.fields.isNotificationsFailureEnabled"
            value: "false"
          - name: "reporting.fields.isNotificationsSuccessEnabled"
            value: "false"
          - name: "reporting.fields.mpesaTransactionId"
            value: "false"
          - name: "reporting.fields.mpesaTransactionStatusRetryCount"
            value: "false"
          - name: "reporting.fields.originDate"
            value: "false"
          - name: "reporting.fields.partyLookupFailed"
            value: "false"
          - name: "reporting.fields.phoneNumber"
            value: "false"
          - name: "reporting.fields.processDefinitionKey"
            value: "false"
          - name: "reporting.fields.processInstanceKey"
            value: "false"
          - name: "reporting.fields.scenario"
            value: "false"
          - name: "reporting.fields.tenantId"
            value: "false"
          - name: "reporting.fields.timer"
            value: "false"
          - name: "reporting.fields.timestamp"
            value: "false"
          - name: "reporting.fields.transactionFailed"
            value: "false"
          - name: "reporting.fields.transactionId"
            value: "false"
          - name: "reporting.fields.transferCreateFailed"
            value: "false"
          - name: "reporting.fields.transferSettlementFailed"
            value: "false"
          - name: "reporting.fields.transferResponseCREATE"
            value: "false"
          - name: "reporting.fields.ams"
            value: "false"
          - name: "reporting.fields.currency"
            value: "false"
          - name: "reporting.fields.clientCorrelationId"
            value: "false"
          - name: "reporting.fields.errorInformation"
            value: "false"
          - name: "reporting.fields.customData"
            value: "false"
          - name: "reporting.fields.confirmationReceived"
            value: "false"
          - name: "ELASTICSEARCH_SECURITY_ENABLED"
            value: "false"
          - name: "ELASTICSEARCH_SSLVERIFICATION"
            value: "false"
          - name: "ELASTICSEARCH_USERNAME"
            valueFrom:
              secretKeyRef:
                name: elastic-credentials
                key: username
          - name: "ELASTICSEARCH_PASSWORD"
            valueFrom:
              secretKeyRef:
                name: elastic-credentials
                key: password        
          envFrom: 
            null
          securityContext: 
            null
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/importer_rdbms/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-importer-rdbms
  labels:
    app: ph-ee-importer-rdbms
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-importer-rdbms
  template:
    metadata:
      labels:
        app: ph-ee-importer-rdbms
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      containers:
        - name: ph-ee-importer-rdbms
          image: "docker.io/openmf/ph-ee-importer-rdbms:v1.12.0"
          imagePullPolicy: "Always"
          ports:
            - containerPort: 8000
          env:
          - name: "SPRING_PROFILES_ACTIVE"
            value: "local,tenantsConnection"
          - name: "DATASOURCE_CORE_USERNAME"
            value: "mifos" 
          - name: "DATASOURCE_CORE_PASSWORD"
            valueFrom:
              secretKeyRef:
                name: "importer-rdbms-secret"
                key: "database-password"
          - name: "DATASOURCE_CORE_HOST"
            value: "operationsmysql"
          - name: "DATASOURCE_CORE_PORT"
            value: "3306"
          - name: "DATASOURCE_CORE_SCHEMA"
            value: "tenants"
          - name: "LOGGING_LEVEL_ROOT"
            value: "INFO"
          - name: "LOGGING_PATTERN_CONSOLE"
            value: "%d{dd-MM-yyyy HH:mm:ss.SSS} %magenta([%thread]) %highlight(%-5level) %logger.%M - %msg%n"
          - name: "JAVA_TOOL_OPTIONS"
            value: "-Xmx256M"
          - name: "APPLICATION_BUCKET-NAME"
            value: "paymenthub-ee"
          - name: "CLOUD_AWS_S3BASEURL"
            value: http://minio:9000
          - name: "CLOUD_AWS_REGION_STATIC"
            valueFrom:
              secretKeyRef:
                name: "bulk-processor-secret"
                key: "aws-region"
          - name: "AWS_ACCESS_KEY"
            valueFrom:
              secretKeyRef:
                name: "bulk-processor-secret"
                key: "aws-access-key"
          - name: "AWS_SECRET_KEY"
            valueFrom:
              secretKeyRef:
                name: "bulk-processor-secret"
                key: "aws-secret-key"        
          envFrom: 
            []
          securityContext: 
            privileged: false
            runAsUser: 0              

          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kibana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-kibana
  labels: 
    app: kibana
    release: "release-name"
    heritage: Helm
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: kibana
      release: "release-name"
  template:
    metadata:
      labels:
        app: kibana
        release: "release-name"
      annotations:
        
        configchecksum: f72bb7f44d5d1004d3647831036a10109eb66e633ef4a214578ff26c1ee7d79
    spec:
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 1000
      volumes:
        - name: elastic-certificate-pem
          secret:
            secretName: elastic-certificate-pem
        - name: kibanaconfig
          configMap:
            name: ph-ee-kibana-config
      containers:
      - name: kibana
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/kibana/kibana:7.16.3"
        imagePullPolicy: "IfNotPresent"
        env:
          - name: ELASTICSEARCH_HOSTS
            value: "http://ph-ee-elasticsearch:9200/"
          - name: SERVER_HOST
            value: "0.0.0.0"
          - name: ELASTICSEARCH_USERNAME
            valueFrom:
              secretKeyRef:
                key: username
                name: elastic-credentials
          - name: ELASTICSEARCH_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: elastic-credentials
          - name: KIBANA_ENCRYPTION_KEY
            valueFrom:
              secretKeyRef:
                key: encryptionkey
                name: kibana
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 45
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 15
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Kibana Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                    local path="${1}"
                    set -- -XGET -s --fail -L

                    if [ -n "${ELASTICSEARCH_USERNAME}" ] && [ -n "${ELASTICSEARCH_PASSWORD}" ]; then
                      set -- "$@" -u "${ELASTICSEARCH_USERNAME}:${ELASTICSEARCH_PASSWORD}"
                    fi

                    STATUS=$(curl --output /dev/null --write-out "%{http_code}" -k "$@" "http://localhost:5601${path}")
                    if [[ "${STATUS}" -eq 200 ]]; then
                      exit 0
                    fi

                    echo "Error: Got HTTP code ${STATUS} but expected a 200"
                    exit 1
                }

                http "/app/kibana"
        ports:
        - containerPort: 5601
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 1000m
            memory: 2Gi
        volumeMounts:
          - name: elastic-certificate-pem
            mountPath: /usr/share/kibana/config/certs
          - name: kibanaconfig
            mountPath: /usr/share/kibana/config/kibana.yml
            subPath: kibana.yml
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/messagegateway/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: message-gateway
  labels:
    app: message-gateway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: message-gateway
  template:
    metadata:
      labels:
        app: message-gateway
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      containers:
        - name: message-gateway
          image: "docker.io/openmf/message-gateway:v1.1.0"  
          ports:
            - containerPort: 9191
          imagePullPolicy: "Always"
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: 9191
            initialDelaySeconds: 120
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: 9191
            initialDelaySeconds: 120
            periodSeconds: 30

          env:
            - name: "SPRING_DATASOURCE_URL"
              value: "jdbc:mysql:thin://operationsmysql:3306/messagegateway"
            - name: "MYSQL_USERNAME"
              value: "mifos"
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"
            - name: "MYSQL_PASSWORD"
              valueFrom:
                secretKeyRef:
                  name: "messagegateway-secret"
                  key: "database-password"
            - name: "CALLBACKCONFIG_HOST"
              value: "ph-ee-connector-notifications"
            - name: "HOSTCONFIG_HOST"
              value: "message-gateway"
            - name: "PROVIDERSOURCE_FROMDATABASE"
              value: "disabled"
            - name: "PROVIDERSOURCE_FROMYML"
              value: "enabled"
            - name: "LOGGING_LEVEL_ROOT"
              value: "INFO"
            - name: "PROVIDERKEYS_TELERIVETAPIKEY"
              valueFrom:
                secretKeyRef:
                  name: "messagegateway-secret"
                  key: "api-key"
            - name: "PROVIDERKEYS_TELERIVETPROJECTID"
              valueFrom:
                secretKeyRef:
                  name: "messagegateway-secret"
                  key: "project-id"
            - name : "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"        
          envFrom: 
            []
          securityContext: 
            privileged: false
            runAsUser: 0              

          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      initContainers:
        - name: wait-db
          image: jwilder/dockerize
          imagePullPolicy: IfNotPresent
          args:
            - -timeout=120s
            - -wait
            - tcp://operationsmysql:3306
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/minio/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio
  labels:
    app: minio
    chart: minio-5.0.14
    release: release-name
    heritage: Helm
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
  replicas: 1
  selector:
    matchLabels:
      app: minio
      release: release-name
  template:
    metadata:
      name: minio
      labels:
        app: minio
        release: release-name
      annotations:
        checksum/secrets: 7eee3f4a5408ee3d976150fdd4964c591900a7247e86c085a96fd55eba5f8396
        checksum/config: 269b72c5cf7a5aa81593149ea1b019ea980d50a20195c5f220c9e69bbe9af313
    spec:
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch
      
      serviceAccountName: minio-sa
      containers:
        - name: minio
          image: "quay.io/minio/minio:RELEASE.2023-09-30T07-02-29Z"
          imagePullPolicy: IfNotPresent
          command:
            - "/bin/sh"
            - "-ce"
            - "/usr/bin/docker-entrypoint.sh minio server /export -S /etc/minio/certs/ --address :9000 --console-address :9001"
          volumeMounts:
            - name: minio-user
              mountPath: "/tmp/credentials"
              readOnly: true
            - name: export
              mountPath: /export            
          ports:
            - name: http
              containerPort: 9000
            - name: http-console
              containerPort: 9001
          env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: minio
                  key: rootUser
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: minio
                  key: rootPassword
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
          resources:
            requests:
              memory: 256Mi      
      volumes:
        - name: export
          persistentVolumeClaim:
            claimName: minio
        - name: minio-user
          secret:
            secretName: minio
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/mockpayment/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "ph-ee-connector-mock-payment-schema"
  labels:
    app: ph-ee-connector-mock-payment-schema
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-connector-mock-payment-schema
  template:
    metadata:
      labels:
        app: ph-ee-connector-mock-payment-schema
      annotations:
    spec:
      terminationGracePeriodSeconds: 
      containers:
        - name: ph-ee-connector-mock-payment-schema
          image: "docker.io/openmf/ph-ee-connector-mock-payment-schema:v1.5.0"
          ports:
            - containerPort: 8080
          imagePullPolicy: "Always"
          resources:
            limits:
              memory: "768M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: 8080
            initialDelaySeconds: 20
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: 8080
            initialDelaySeconds: 20
            periodSeconds: 30
          env:
            - name: "SPRING_PROFILES_ACTIVE"
              value: "bb"
            - name: "DFSPIDS"
              value: "gorilla,lion"
            - name: "LOGGING_LEVEL_ROOT"
              value: "INFO"
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"
            - name : "mockFailure_percentage"
              value: "0"        
          envFrom: 
            null
          securityContext: 
            null
          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/notifications/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-connector-notifications
  labels:
    app: ph-ee-connector-notifications
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-connector-notifications
  template:
    metadata:
      labels:
        app: ph-ee-connector-notifications
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      containers:
        - name: "ph-ee-connector-notifications"
          image: "docker.io/openmf/ph-ee-notifications:v1.3.0"
          imagePullPolicy: "Always"
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
          ports:
           - containerPort: 5000
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: 8080
            initialDelaySeconds: 20
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: 8080
            initialDelaySeconds: 20
            periodSeconds: 30
          env:
            - name: "SPRING_PROFILES_ACTIVE"
              value: "bb"
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"
            - name: "HOSTCONFIG_HOST"
              value: "message-gateway"
            - name: "HOSTCONFIG_PORT"
              value: "80"
            - name: "MESSAGEGATEWAYCONFIG_HOST"
              value: "message-gateway"
            - name: "NOTIFICATION_LOCAL_HOST"
              value: "ph-ee-connector-notifications"
            - name: "NOTIFICATION_SUCCESS_ENABLED"
              value: "false"
            - name: "NOTIFICATION_FAILURE_ENABLED"
              value: "true"
            - name: "LOGGING_LEVEL_ROOT"
              value:  "INFO"        
          envFrom: 
            []
          securityContext: 
            privileged: false
            runAsUser: 0
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_app/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-operations-app
  labels:
    app: ph-ee-operations-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-operations-app
  template:
    metadata:
      labels:
        app: ph-ee-operations-app
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      terminationGracePeriodSeconds: 30
      containers:
        - name: ph-ee-operations-app
          image: "docker.io/openmf/ph-ee-operations-app:v1.14.0"
          imagePullPolicy: "Always"
          ports:
            - containerPort: 5000
          readinessProbe:
            httpGet:
              path: /oauth/token_key
              port: 5000
            periodSeconds: 180
            timeoutSeconds: 5
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
          env:
            - name: "FINERACT_DATASOURCE_CORE_USERNAME"
              value: "mifos" 
            - name: "FINERACT_DATASOURCE_CORE_PASSWORD"
              valueFrom:
                secretKeyRef:
                  name: "operations-app-secret"
                  key: "database-password"
            - name: "FINERACT_DATASOURCE_CORE_HOST"
              value: "operationsmysql"
            - name: "FINERACT_DATASOURCE_CORE_PORT"
              value: "3306"
            - name: "FINERACT_DATASOURCE_CORE_SCHEMA"
              value: "tenants"
            - name: "SPRING_PROFILES_ACTIVE"
              value: "bb"
            - name: "TOKEN_CLIENT_CHANNEL_SECRET"
              value: ""
            - name: "TENANTS"
              value: "rhino,gorilla,lion"
            - name: "LOGGING_LEVEL_ROOT"
              value: "INFO"
            - name: "LOGGING_PATTERN_CONSOLE"
              value: "%d{dd-MM-yyyy HH:mm:ss.SSS} %magenta([%thread]) %highlight(%-5level) %logger.%M - %msg%n"        
          envFrom: 
            []
          securityContext: 
            privileged: false
            runAsUser: 0              
          
          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      initContainers:
        - name: wait-db
          image: jwilder/dockerize
          imagePullPolicy: IfNotPresent
          args:
            - -timeout=120s
            - -wait
            - tcp://operationsmysql:3306
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operations_web/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-operations-web
  labels:
    app: ph-ee-operations-web
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-operations-web
  template:
    metadata:
      labels:
        app: ph-ee-operations-web
      annotations:
    spec:
      containers:
        - name: ph-ee-operations-web
          image: "docker.io/openmf/ph-ee-operations-web:v1.10.0"
          imagePullPolicy: "Always"
          ports:
            - containerPort: 4200
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
          volumeMounts:
            - mountPath: /usr/share/nginx/html/assets/configuration.properties
              name: ph-ee-operations-web-config
              subPath: configuration.properties
      volumes:
        - name: ph-ee-operations-web-config
          configMap:
            name: ph-ee-operations-web-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_ams_mifos/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-connector-ams-mifos
  labels:
    app: ph-ee-connector-ams-mifos
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-connector-ams-mifos
  template:
    metadata:
      labels:
        app: ph-ee-connector-ams-mifos
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec: 
      affinity: 
        {}
      nodeSelector: 
        {}
      tolerations: 
        []
      serviceAccountName: ph-ee-connector-ams-mifos
      terminationGracePeriodSeconds: 180  
      containers:
      - name: ph-ee-connector-ams-mifos
        image: "docker.io/openmf/ph-ee-connector-ams-mifos:v1.6.0"
        imagePullPolicy: "Always"
        ports:
          - containerPort: 5000
        resources: 
          limits:
            cpu: 500m
            memory: 2048M
          requests:
            cpu: 300m
            memory: 1024M  
        env:
        - name: "SPRING_PROFILES_ACTIVE"
          value: "fin12,bb"
        - name: "ams_local_enabled"
          value: "true"
        - name: "ams_local_interop_host"
          value: "https://fynams.sandbox.mifos.io/"
        - name: "ams_local_customer_host"
          value: "https://fynams.sandbox.mifos.io/"
        - name: "ams_local_account_host"
          value: "https://fynams.sandbox.mifos.io/"
        - name: "ams_local_auth_host"
          value: "https://fynams.sandbox.mifos.io/"
        - name: "ams_local_loan_host"
          value: ""
        - name: "DFSPIDS"
          value: "gorilla,lion"
        - name: "ZEEBE_BROKER_CONTACTPOINT"
          value: "release-name-zeebe-gateway:26500"        
        envFrom: 
          []
        securityContext: 
          privileged: false
          runAsUser: 0  

        volumeMounts:
        - name: ph-ee-config
          mountPath: "/config"
#            
      initContainers:
        - name: wait-db
          image: jwilder/dockerize
          imagePullPolicy: IfNotPresent
          args:
            - -timeout=120s
            - -wait
            - tcp://operationsmysql:3306         
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_gsma/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "ph-ee-connector-gsma"
  labels:
    app: "ph-ee-connector-gsma"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: "ph-ee-connector-gsma"
  template:
    metadata:
      labels:
        app: "ph-ee-connector-gsma"
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      containers:
        - name: "ph-ee-connector-gsma-mm"
          image: "docker.io/openmf/ph-ee-connector-gsma:v1.3.0"
          imagePullPolicy: "Always"
          ports:
            - containerPort: 5000 
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"              
          env:
            - name: "SPRING_PROFILES_ACTIVE"
              value: "bb"
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"        
          envFrom: 
            []
          securityContext: 
            privileged: false
            runAsUser: 0              
          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/ph_ee_connector_mojaloop/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-connector-mojaloop-java
  labels:
    app: ph-ee-connector-mojaloop-java
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-connector-mojaloop-java
  template:
    metadata:
      labels:
        app: ph-ee-connector-mojaloop-java
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      containers:
        - name: ph-ee-connector-mojaloop-java
          image: "docker.io/openmf/ph-ee-connector-mojaloop:v1.4.0"
          imagePullPolicy: "Always"
          ports:
            - containerPort: 5000
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
          env:
            - name: "SPRING_PROFILES_ACTIVE"
              value: "bb"

            - name: "DFSPIDS"
              value: "gorilla,lion"

            - name: "SWITCH_quotes-host"
              value: ""

            - name: "SWITCH_quote-service"
              value: ""

            - name: "SWITCH_als-host"
              value: ""

            - name: "SWITCH_account-lookup-service"
              value: ""

            - name: "SWITCH_transfers-host"
              value: ""

            - name: "SWITCH_transfers-service"
              value: ""

            - name: "SWITCH_transactions-host"
              value: ""

            - name: "SWITCH_transaction-request-service"
              value: ""

            - name: "SWITCH_oracle-host"
              value: ""

            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500" 
          envFrom: 
            []
          securityContext: 
            privileged: false
            runAsUser: 0              

          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/vouchers/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-vouchers
  labels:
    app: ph-ee-vouchers
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-vouchers
  template:
    metadata:
      labels:
        app: ph-ee-vouchers
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      containers:
        - name: ph-ee-vouchers
          image: "docker.io/openmf/ph-ee-vouchers:v1.2.0"
          ports:
            - containerPort: 8080
          imagePullPolicy: "Always"
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "512M"
              cpu: "100m"
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: 8080
            initialDelaySeconds: 20
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: 8080
            initialDelaySeconds: 20
            periodSeconds: 30

          env:
            - name: "SPRING_PROFILES_ACTIVE"
              value: "bb"
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"
            - name: "PAYER_TENANT"
              value: "rhino"
            - name: "PAYER_IDENTIFIER"
              value: "1.2345678e+07"
            - name: "PAYER_IDENTIFIER_TYPE"
              value: "MSISDN"
            - name: "VOUCHER_HOSTNAME"
              value: "http://ph-ee-vouchers:80"
            - name: "OPERATIONS_HOSTNAME"
              value: "http://ph-ee-operations-app:80"
            - name: "OPERATIONS_TRANSFERS_ENDPOINT"
              value: "/api/v1/transfers?size=1&page=0"
            - name: "SPRING_DATASOURCE_URL"
              value: "jdbc:mysql://operationsmysql:3306/voucher_management"
            - name: "SPRING_DATASOURCE_USERNAME"
              value: "mifos"
            - name: "SPRING_DATASOURCE_PASSWORD"
              value: "password"
            - name: "ASYNC_CORE_POOL_SIZE"
              value: "10"
            - name: "ASYNC_MAX_POOL_SIZE"
              value: "10"
            - name: "ASYNC_QUEUE_CAPACITY"
              value: "100"
            - name: "EXPIRY_TIME"
              value: "60"
            - name: "SALTING_ENABLED"
              value: "true"
            - name: "LOGGING_LEVEL_ROOT"
              value: "INFO"
            - name: MOCK_SCHEMA_HOSTNAME
              value: "http://ph-ee-connector-mock-payment-schema:8080"        
          envFrom: 
            null
          securityContext: 
            null
          volumeMounts:
            - name: ph-ee-config
              mountPath: "/config"
      volumes:
        - name: ph-ee-config
          configMap:
            name: ph-ee-config
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/zeebe_ops/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ph-ee-zeebe-ops
  labels:
    app: ph-ee-zeebe-ops
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ph-ee-zeebe-ops
  template:
    metadata:
      labels:
        app: ph-ee-zeebe-ops
      annotations:
        deployTime: '{{ .Values.deployTime }}'
    spec:
      containers:
        - name: ph-ee-zeebe-ops
          image: "docker.io/openmf/ph-ee-zeebe-ops:v1.4.0"
          imagePullPolicy: "Always"
          resources:
            limits:
              memory: "512M"
              cpu: "500m"
            requests:
              memory: "256M"
              cpu: "100m"
          ports:
            - containerPort: 5000
          env:
            - name: "ZEEBE_BROKER_CONTACTPOINT"
              value: "release-name-zeebe-gateway:26500"
            - name: "TENANTS"
              value: "rhino,gorilla,lion"
            - name: "elasticsearch_url"
              value: "http://ph-ee-elasticsearch:9200/"
            - name: "LOGGING_LEVEL_ROOT"
              value:  "INFO"
            - name: "SPRING_DATA_ELASTICSEARCH_CLIENT_REACTIVE_ENDPOINTS"
              value: "ph-ee-elasticsearch:9200"
            - name: "ELASTICSEARCH_SECURITY_ENABLED"
              value: "false"
            - name: "ELASTICSEARCH_SSLVERIFICATION"
              value: "false"
            - name: "ELASTICSEARCH_USERNAME"
              valueFrom:
                secretKeyRef:
                  name: elastic-credentials
                  key: username
            - name: "ELASTICSEARCH_PASSWORD"
              valueFrom:
                secretKeyRef:
                  name: elastic-credentials
                  key: password        
          envFrom: 
            []
          securityContext: 
            privileged: false
            runAsUser: 0
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/charts/zeebe/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "release-name-zeebe"
  labels:
    app: camunda-platform
    app.kubernetes.io/name: zeebe
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: zeebe-8.2.12
    app.kubernetes.io/version: "8.2.8"
    app.kubernetes.io/component: zeebe-broker
  annotations:
spec:
  replicas: 1
  selector:
    matchLabels:
      app: camunda-platform
      app.kubernetes.io/name: zeebe
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: camunda-platform
      app.kubernetes.io/component: zeebe-broker
  serviceName: "release-name-zeebe"
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: camunda-platform
        app.kubernetes.io/name: zeebe
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/part-of: camunda-platform
        helm.sh/chart: zeebe-8.2.12
        app.kubernetes.io/version: "8.2.8"
        app.kubernetes.io/component: zeebe-broker
      annotations:
    spec:
      imagePullSecrets:
        []
      initContainers:
        - args:
          - wget -O /exporters/ph-ee-kafka-exporter.jar "https://fynarfin.io/images/exporter-1.1.1-SNAPSHOT.jar";
            ls -al /exporters/
          command:
          - /bin/sh
          - -c
          image: busybox:1.28
          name: init-ph-ee-kafka-exporter
          volumeMounts:
          - mountPath: /exporters/
            name: exporters
      containers:
      - name: zeebe
        image: "camunda/zeebe:8.2.8"
        imagePullPolicy: IfNotPresent
        env:
        - name: LC_ALL
          value: C.UTF-8
        - name: K8S_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: K8S_SERVICE_NAME
          value: "release-name-zeebe"
        - name: K8S_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: ZEEBE_BROKER_NETWORK_ADVERTISEDHOST
          value: "$(K8S_NAME).$(K8S_SERVICE_NAME).$(K8S_NAMESPACE).svc"
        - name: ZEEBE_BROKER_CLUSTER_INITIALCONTACTPOINTS
          value:
            $(K8S_SERVICE_NAME)-0.$(K8S_SERVICE_NAME).$(K8S_NAMESPACE).svc:26502,
        - name: ZEEBE_BROKER_CLUSTER_CLUSTERNAME
          value: release-name-zeebe
        - name: ZEEBE_LOG_LEVEL
          value: "info"
        - name: ZEEBE_BROKER_CLUSTER_PARTITIONSCOUNT
          value: "1"
        - name: ZEEBE_BROKER_CLUSTER_CLUSTERSIZE
          value: "1"
        - name: ZEEBE_BROKER_CLUSTER_REPLICATIONFACTOR
          value: "1"
        - name: ZEEBE_BROKER_THREADS_CPUTHREADCOUNT
          value: "2"
        - name: ZEEBE_BROKER_THREADS_IOTHREADCOUNT
          value: "2"
        - name: ZEEBE_BROKER_GATEWAY_ENABLE
          value: "false"
        - name: ZEEBE_BROKER_NETWORK_COMMANDAPI_PORT
          value: "26501"
        - name: ZEEBE_BROKER_NETWORK_INTERNALAPI_PORT
          value: "26502"
        - name: ZEEBE_BROKER_NETWORK_MONITORINGAPI_PORT
          value: "9600"
        - name: K8S_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: JAVA_TOOL_OPTIONS
          value: "-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/zeebe/data -XX:ErrorFile=/usr/local/zeebe/data/zeebe_error%p.log -XX:+ExitOnOutOfMemoryError"
        - name: ZEEBE_BROKER_EXECUTION_METRICS_EXPORTER_ENABLED
          value: "true"
        - name: ZEEBE_BROKER_EXPORTERS_ELASTICSEARCH_CLASSNAME
          value: hu.dpc.rt.kafkastreamer.exporter.NoOpExporter
        - name: ZEEBE_BROKER_EXPORTERS_ELASTICSEARCH_JARPATH
          value: /exporters/ph-ee-kafka-exporter.jar
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_JARPATH
          value: /exporters/ph-ee-kafka-exporter.jar
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_CLASSNAME
          value: hu.dpc.rt.kafkastreamer.exporter.KafkaExporter
        - name: ZEEBE_BROKER_BACKPRESSURE_VEGAS_INITIALLIMIT
          value: "1000"
        - name: ZEEBE_BROKER_BACKPRESSURE_VEGAS_ALPHA
          value: "2"
        - name: ZEEBE_BROKER_BACKPRESSURE_VEGAS_BETA
          value: "8"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_EVENT
          value: "true"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_VARIABLE
          value: "true"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_PROCESSINSTANCE
          value: "true"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_PROCESS
          value: "true"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_INCIDENT
          value: "true"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_DEPLOYMENT
          value: "false"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_ERROR
          value: "false"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_JOB
          value: "false"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_VARIABLEDOCUMENT
          value: "false"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_WORKFLOWINSTANCE
          value: "false"
        - name: ZEEBE_BROKER_EXPORTERS_KAFKA_ARGS_INDEX_CREATETEMPLATE
          value: "false"
        ports:
        - containerPort: 9600
          name: http
        - containerPort: 26501
          name: command
        - containerPort: 26502
          name: internal
        readinessProbe:
          httpGet:
            path: /ready
            scheme: HTTP
            port: 9600
          initialDelaySeconds: 30
          periodSeconds: 30
          successThreshold: 1
          failureThreshold: 5
          timeoutSeconds: 1
        resources:
          limits:
            cpu: 960m
            memory: 1920Mi
          requests:
            cpu: 100m
            memory: 1200Mi
        volumeMounts:
        - name: config
          mountPath: /usr/local/bin/startup.sh
          subPath: startup.sh
        - name: data
          mountPath: /usr/local/zeebe/data
        - name: exporters
          mountPath: /exporters
      volumes:
        - name: config
          configMap:
            name: release-name-zeebe
            defaultMode: 492
        - name: exporters
          emptyDir: {}
      securityContext:
        fsGroup: 1000
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - zeebe-broker
            topologyKey: kubernetes.io/hostname
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ReadWriteOnce]
      storageClassName: 
      resources:
        requests:
          storage: "10Gi"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/elasticsearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ph-ee-elasticsearch
  labels:
    heritage: "Helm"
    release: "release-name"
    chart: "elasticsearch"
    app: "ph-ee-elasticsearch"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: ph-ee-elasticsearch-headless
  selector:
    matchLabels:
      app: "ph-ee-elasticsearch"
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: ph-ee-elasticsearch
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
      storageClassName: gp2
  template:
    metadata:
      name: "ph-ee-elasticsearch"
      labels:
        release: "release-name"
        chart: "elasticsearch"
        app: "ph-ee-elasticsearch"
      annotations:
        
        configchecksum: 72b0439b8066016c180907cb69c553e1adaa64bf56c361597870c917215bce3
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      automountServiceAccountToken: true
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "ph-ee-elasticsearch"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
        - name: elastic-certificates
          secret:
            secretName: elastic-certificates
        - name: esconfig
          configMap:
            name: ph-ee-elasticsearch-config
      enableServiceLinks: true
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.3"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.3"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - bash
              - -c
              - |
                set -e
                # If the node is starting up wait for the cluster to be ready (request params: "wait_for_status=yellow&timeout=100s" )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                # Disable nss cache to avoid filling dentry cache when calling curl
                # This is required with Elasticsearch Docker using nss < 3.52
                export NSS_SDB_USE_CACHE=no

                http () {
                  local path="${1}"
                  local args="${2}"
                  set -- -XGET -s

                  if [ "$args" != "" ]; then
                    set -- "$@" $args
                  fi

                  if [ -n "${ELASTIC_PASSWORD}" ]; then
                    set -- "$@" -u "elastic:${ELASTIC_PASSWORD}"
                  fi

                  curl --output /dev/null -k "$@" "http://127.0.0.1:9200${path}"
                }

                if [ -f "${START_FILE}" ]; then
                  echo 'Elasticsearch is already running, lets check the node is healthy'
                  HTTP_CODE=$(http "/" "-w %{http_code}")
                  RC=$?
                  if [[ ${RC} -ne 0 ]]; then
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}"
                    exit ${RC}
                  fi
                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x
                  if [[ ${HTTP_CODE} == "200" ]]; then
                    exit 0
                  elif [[ ${HTTP_CODE} == "503" && "7" == "6" ]]; then
                    exit 0
                  else
                    echo "curl --output /dev/null -k -XGET -s -w '%{http_code}' \${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}"
                    exit 1
                  fi

                else
                  echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=yellow&timeout=100s" )'
                  if http "/_cluster/health?wait_for_status=yellow&timeout=100s" "--fail" ; then
                    touch ${START_FILE}
                    exit 0
                  else
                    echo 'Cluster is not yet ready (request params: "wait_for_status=yellow&timeout=100s" )'
                    exit 1
                  fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 1024M
          requests:
            cpu: 100m
            memory: 1024M
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: cluster.initial_master_nodes
            value: "ph-ee-elasticsearch-0,"
          - name: discovery.seed_hosts
            value: "ph-ee-elasticsearch-headless"
          - name: cluster.name
            value: "ph-ee-elasticsearch"
          - name: network.host
            value: "0.0.0.0"
          - name: cluster.deprecation_indexing.enabled
            value: "false"
          - name: ES_JAVA_OPTS
            value: "-Xmx512m -Xms512m"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "true"
          - name: node.ml
            value: "true"
          - name: node.remote_cluster_client
            value: "true"
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: elastic-credentials
        volumeMounts:
          - name: "ph-ee-elasticsearch"
            mountPath: /usr/share/elasticsearch/data

          - name: elastic-certificates
            mountPath: /usr/share/elasticsearch/config/certs
          - name: esconfig
            mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
            subPath: elasticsearch.yml
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kafka/templates/controller-eligible/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka-controller
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-25.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller-eligible
    app.kubernetes.io/part-of: kafka
spec:
  podManagementPolicy: Parallel
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: controller-eligible
      app.kubernetes.io/part-of: kafka
  serviceName: kafka-controller-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-25.0.0
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: controller-eligible
        app.kubernetes.io/part-of: kafka
      annotations:
        checksum/configuration: 89f68d954023805ce86c4be343101188957d08022887cd392cb028fc9ec3fba3
    spec:
      
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: kafka
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: kafka
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: kafka
      enableServiceLinks: true
      initContainers:
        - name: kafka-init
          image: docker.io/bitnami/kafka:3.5.1-debian-11-r25
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -ec
            - |
              /scripts/kafka-init.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                    fieldPath: metadata.name
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_MIN_ID
              value: "0"
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: kafka-config
              mountPath: /config
            - name: kafka-configmaps
              mountPath: /configmaps
            - name: scripts
              mountPath: /scripts
            - name: tmp
              mountPath: /tmp
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:3.5.1-debian-11-r25
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_KRAFT_CLUSTER_ID
              valueFrom:
                secretKeyRef:
                  name: kafka-kraft-cluster-id
                  key: kraft-cluster-id
          ports:
            - name: controller
              containerPort: 9093
            - name: client
              containerPort: 9092
            - name: interbroker
              containerPort: 9094
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: "controller"
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: "controller"
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: kafka-config
              mountPath: /opt/bitnami/kafka/config/server.properties
              subPath: server.properties
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: kafka-configmaps
          configMap:
            name: kafka-controller-configuration
        - name: kafka-config
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: scripts
          configMap:
            name: kafka-scripts
            defaultMode: 0755
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/operationsmysql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: operationsmysql
  namespace: "default"
  labels:
    app.kubernetes.io/name: operationsmysql
    helm.sh/chart: operationsmysql-9.4.5
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  podManagementPolicy: ""
  selector:
    matchLabels: 
      app.kubernetes.io/name: operationsmysql
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: primary
  serviceName: operationsmysql
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/configuration: 1ca5cb967fa7077e8bbc35f10cf6ac6e8458e2800eae541234a8be775d20e3de
      labels:
        app.kubernetes.io/name: operationsmysql
        helm.sh/chart: operationsmysql-9.4.5
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: operationsmysql
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: operationsmysql
                    app.kubernetes.io/instance: release-name
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: mysql
          image: docker.io/bitnami/mysql:5.7
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: operationsmysql
                  key: mysql-root-password
            - name: MYSQL_USER
              value: "mifos"
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: operationsmysql
                  key: mysql-password
            - name: MYSQL_DATABASE
              value: "tenants"
          envFrom:
          ports:
            - name: mysql
              containerPort: 3306
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          startupProbe:
            failureThreshold: 10
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          resources: 
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/mysql
            - name: custom-init-scripts
              mountPath: /docker-entrypoint-initdb.d
            - name: config
              mountPath: /opt/bitnami/mysql/conf/my.cnf
              subPath: my.cnf
      volumes:
        - name: config
          configMap:
            name: operationsmysql
        - name: custom-init-scripts
          configMap:
            name: operationsmysql-init-scripts
  volumeClaimTemplates:
    - metadata:
        name: data
        labels: 
          app.kubernetes.io/name: operationsmysql
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/component: primary
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-redis-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.9.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: master
  serviceName: release-name-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-17.9.3
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: f40d2900e45d303cab9ad3f7cef8ccc31a7c62b24c33f487ab1bfcb09169020f
        checksum/health: e23f625b1a1c45d0df858791edd7a9dfc336a535fde33a059afdc632defb036b
        checksum/scripts: cd93c77270bcdb8a211d52eea83fb9e77481aaefdd4e04703bac9e8e18bf5f1f
        checksum/secret: 32d99381e239d5db0b8d7a0b76d6812dbf30f5188b1941d16816f1cea4b87329
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: release-name-redis
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.0.10-debian-11-r4
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-redis
                  key: redis-password
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc/
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: start-scripts
          configMap:
            name: release-name-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: release-name-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: release-name-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: tmp
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: redis
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/redis/templates/replicas/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-redis-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.9.3
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: replica
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: replica
  serviceName: release-name-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-17.9.3
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: replica
      annotations:
        checksum/configmap: f40d2900e45d303cab9ad3f7cef8ccc31a7c62b24c33f487ab1bfcb09169020f
        checksum/health: e23f625b1a1c45d0df858791edd7a9dfc336a535fde33a059afdc632defb036b
        checksum/scripts: cd93c77270bcdb8a211d52eea83fb9e77481aaefdd4e04703bac9e8e18bf5f1f
        checksum/secret: 45feeba035a7d448d9d300df8503785defb60520ccfc16ceef3bc89bc5bdee44
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: release-name-redis
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: replica
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.0.10-debian-11-r4
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-replica.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: replica
            - name: REDIS_MASTER_HOST
              value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-redis
                  key: redis-password
            - name: REDIS_MASTER_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-redis
                  key: redis-password
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: redis
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local_and_master.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local_and_master.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc
      volumes:
        - name: start-scripts
          configMap:
            name: release-name-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: release-name-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: release-name-redis-configuration
        - name: redis-tmp-conf
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: redis
          app.kubernetes.io/instance: release-name
          app.kubernetes.io/component: replica
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/templates/service-monitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-camunda-platform
  labels:
    app: camunda-platform
    app.kubernetes.io/name: camunda-platform
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: camunda-platform-8.2.12
    app.kubernetes.io/version: "8.2.12"
    release: metrics
spec:
  selector:
    matchLabels:
      app: camunda-platform
  endpoints:
    - honorLabels: true
      path: /actuator/prometheus
      port: http
      interval: 10s
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/camunda-platform/charts/zeebe/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-zeebe-test-connection"
  labels: 
    app: camunda-platform
    app.kubernetes.io/name: zeebe
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: camunda-platform
    helm.sh/chart: zeebe-8.2.12
    app.kubernetes.io/version: "8.2.8"
    app.kubernetes.io/component: zeebe-broker
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args:  ['release-name-zeebe:9600']
  restartPolicy: Never
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/elasticsearch/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-mzkuh-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  securityContext:
    fsGroup: 1000
    runAsUser: 1000
  containers:
  - name: "release-name-jaybf-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.3"
    imagePullPolicy: "IfNotPresent"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'ph-ee-elasticsearch:9200/_cluster/health?wait_for_status=yellow&timeout=100s'
  restartPolicy: Never
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/templates/tests/integration-test-job.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "g2p-sandbox-test-connection"
  labels:
    app: "integration-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
  - name: integration-test
    image: docker.io/openmf/ph-ee-integration-test:v1.5.0
    imagePullPolicy: "Always"
    command: [ "/bin/bash" , "-c" ]
    args: 
    # - ./gradlew test -Dcucumber.filter.tags="@bd012" ; echo 'Test complete' ; sleep 300 ; echo 'pod terminate'
      - ./gradlew test -i -Dcucumber.filter.tags="@gov and not @flaky" ; echo 'Test complete' ; sleep 300 ; echo 'pod terminate'
    env:
      - name: "OPERATIONS_APP_CONTACTPOINT"
        value: "http://ph-ee-operations-app:80"
      - name: "BULK_PROCESSOR_CONTACTPOINT"
        value: "https://ph-ee-connector-bulk:8443"
      - name: "CHANNEL_CONNECTOR_CONTACTPOINT"
        value: "https://ph-ee-connector-channel:8443"
      - name:  "LOGGING_LEVEL_ROOT"
        value: "INFO"
      - name: "MAX_RETRY_COUNT"
        value: "5"
      - name: "RETRY_INTERVAL"
        value: "15000"
      - name: "DEFAULTS_TENANT"
        value: "gorilla"
      - name: "DEFAULTS_AUTHORIZATION"
        value: "Basic bWlmb3M6cGFzc3dvcmQ="
      - name: "CHANNEL_BASE-URL"
        value: "https://ph-ee-connector-channel:8443"
      - name: CHANNEL_CONTACTPOINT
        value: "https://ph-ee-connector-channel:8443"
      - name: SAVINGS_CONTACTPOINT
        value: "https://fineract-server:8443"
      - name: LOAN_CONTACTPOINT
        value: "https://fineract-server:8443"
      - name: MOCK_SERVER_PORT
        value: "53013"
      - name: IDENTITY_ACCOUNT_MAPPER_CONTACTPOINT
        value: "http://ph-ee-identity-account-mapper:80"
      - name: MIFOS_CONNECTOR_CONTACTPOINT
        value: "http://ph-ee-connector-ams-mifos:80"
      - name: AMSMIFOS_MOCK_BASE_URL
        value: "http://ph-ee-connector-ams-mifos:80"
      - name: AMS_BASE_URL
        value: "http://ph-ee-connector-ams-mifos:80"
      - name: MOCK_PAYMENT_SCHEMA_CONTACTPOINT
        value: "http://ph-ee-connector-mock-payment-schema:8080"
      - name: MY_POD_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      - name: "CALLBACK_URL"
        value: http://$(MY_POD_IP):53013
      - name: PAYBILL_MPESA_CONNECTOR_CONTACTPOINT
        value: ""
      - name: VOUCHER_MANAGEMENT_CONTACTPOINT
        value: "http://ph-ee-vouchers:80"
      - name: GLOBAL_WAIT_TIME_MS
        value: "20000"
      - name: billPay_contactpoint
        value: "http://ph-ee-connector-bill-pay:8080"
    resources:
        limits:
          cpu: "500m"
          memory: "3Gi"
        requests:
          cpu: "100m"
          memory: "2Gi"
  restartPolicy: Never
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/kafka/templates/provisioning/job.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: kafka-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-25.0.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: kafka-provisioning
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-25.0.0
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: kafka-provisioning
      annotations:
    spec:
      serviceAccountName: default
      enableServiceLinks: true
      
      securityContext:
        fsGroup: 1001
        seccompProfile:
          type: RuntimeDefault
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      initContainers:
        - name: wait-for-available-kafka
          image: docker.io/bitnami/kafka:3.5.1-debian-11-r25
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -ec
            - |
              wait-for-port \
                --host=kafka \
                --state=inuse \
                --timeout=120 \
                9092;
              echo "Kafka is available";
          resources:
            limits: {}
            requests: {}
      containers:
        - name: kafka-provisioning
          image: docker.io/bitnami/kafka:3.5.1-debian-11-r25
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -ec
            - |
              echo "Configuring environment"
              . /opt/bitnami/scripts/libkafka.sh
              export CLIENT_CONF="${CLIENT_CONF:-/tmp/client.properties}"
              if [ ! -f "$CLIENT_CONF" ]; then
                touch $CLIENT_CONF

                kafka_common_conf_set "$CLIENT_CONF" security.protocol "PLAINTEXT"
              fi

              echo "Running pre-provisioning script if any given"
              
              

              kafka_provisioning_commands=(
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server ${KAFKA_SERVICE} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --command-config ${CLIENT_CONF} \
                    --topic zeebe-export"
              )

              echo "Starting provisioning"
              for ((index=0; index < ${#kafka_provisioning_commands[@]}; index+=1))
              do
                for j in $(seq ${index} $((${index}+1-1)))
                do
                    ${kafka_provisioning_commands[j]} & # Async command
                done
                wait  # Wait the end of the jobs
              done

              echo "Running post-provisioning script if any given"
              
              

              echo "Provisioning succeeded"
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KAFKA_SERVICE
              value: kafka:9092
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: tmp
          emptyDir: {}
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/charts/minio/templates/post-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: minio-post-job
  labels:
    app: minio-post-job
    chart: minio-5.0.14
    release: release-name
    heritage: Helm
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
spec:
  template:
    metadata:
      labels:
        app: minio-job
        release: release-name
    spec:
      restartPolicy: OnFailure      
      volumes:
        - name: etc-path
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: minio-configuration
          projected:
            sources:
              - configMap:
                  name: minio
              - secret:
                  name: minio
      serviceAccountName: minio-sa
      containers:
        - name: minio-make-bucket
          image: "quay.io/minio/mc:RELEASE.2023-09-29T16-41-22Z"
          imagePullPolicy: IfNotPresent
          command: [ "/bin/sh", "/config/initialize" ]
          env:
            - name: MINIO_ENDPOINT
              value: minio
            - name: MINIO_PORT
              value: "9000"
          volumeMounts:
            - name: etc-path
              mountPath: /etc/minio/mc
            - name: tmp
              mountPath: /tmp
            - name: minio-configuration
              mountPath: /config
          resources:
            requests:
              memory: 128Mi
        - name: minio-make-user
          image: "quay.io/minio/mc:RELEASE.2023-09-29T16-41-22Z"
          imagePullPolicy: IfNotPresent
          command: [ "/bin/sh", "/config/add-user" ]
          env:
            - name: MINIO_ENDPOINT
              value: minio
            - name: MINIO_PORT
              value: "9000"
          volumeMounts:
            - name: etc-path
              mountPath: /etc/minio/mc
            - name: tmp
              mountPath: /tmp
            - name: minio-configuration
              mountPath: /config
          resources:
            requests:
              memory: 128Mi
---
# Source: ph-ee-g2psandbox/charts/ph-ee-engine/templates/post-installation-job/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: post-installation-job
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
spec:
  template:
    spec:
      serviceAccountName: job-creator
      containers:
      - name: post-installation-job
        image: ubuntu:latest # Replace with the Docker image containing Git and other tools "cimg/base:2022.06"
        securityContext:
          privileged: true
        command:
        - "/bin/sh"
        - "-c"
        args:
        - |
          # Add Docker's official GPG key:
          apt-get update
          apt-get --assume-yes install ca-certificates curl gnupg 
          install -m 0755 -d /etc/apt/keyrings
          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
          chmod a+r /etc/apt/keyrings/docker.gpg

          # Add the repository to Apt sources:
          echo \
            "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
            $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
            tee /etc/apt/sources.list.d/docker.list > /dev/null
          apt-get update 
          apt-get --assume-yes install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 
          
          # starting docker-deamon
          service docker start
          
          #install make
          apt-get install --assume-yes make
          
          #install kubectl 
          apt-get update
          # apt-transport-https may be a dummy package; if so, you can skip that package
          apt-get install -y apt-transport-https ca-certificates curl gpg

          curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key |  gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

          # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
          echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' |  tee /etc/apt/sources.list.d/kubernetes.list

          apt-get update
          apt-get install -y kubelet kubeadm kubectl
          apt-mark hold kubelet kubeadm kubectl

          export ENV_NAMESPACE=default
          echo "check-namespace"
          echo $ENV_NAMESPACE
          git clone --depth 1 --branch v1.4.0 https://github.com/openMF/ph-ee-env-labs.git 
          cd ph-ee-env-labs/helm/es-secret/
          make secrets || echo "elastic-certificates" already exists

          cd ../../helm/kibana-secret/
          make secrets || echo "kibana" already exists

          #insatll netcat
          apt install -y netcat
          until nc -vz ph-ee-zeebe-ops 80; do echo "Waiting for zeebe-ops service"; sleep 2; done;
          
          #Deploy BPMN 
          kubectl port-forward service/ph-ee-zeebe-ops 5000:80 -n default & 

          cd ../../orchestration
          ls
          sed -i "/HOST=/c\HOST=http://localhost:5000/zeebe/upload" deployBpmn.sh
          cat deployBpmn.sh
          cd ..
          sleep 10
          sh orchestration/deployBpmn.sh
          echo "Configurations successful"
      restartPolicy: Never
  backoffLimit: 0 # Adjust as needed
  # successfulJobsHistoryLimit: 0  # Number of successful completions to retain
  # failedJobsHistoryLimit: 0 # Number of failed completions to retain

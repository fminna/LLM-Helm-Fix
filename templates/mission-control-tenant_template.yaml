---
# Source: mission-control-tenant/charts/vcluster/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vc-release-name
  namespace: default
  labels:
    app: vcluster
    chart: "vcluster-0.16.4"
    release: "release-name"
    heritage: "Helm"
---
# Source: mission-control-tenant/charts/vcluster/templates/workloadserviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vc-workload-release-name
  namespace: default
  labels:
    app: vcluster
    chart: "vcluster-0.16.4"
    release: "release-name"
    heritage: "Helm"
---
# Source: mission-control-tenant/charts/vcluster/templates/coredns.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-coredns
  namespace: default
data:
  coredns.yaml: |-
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: coredns
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system:coredns
    rules:
      - apiGroups:
          - ""
        resources:
          - endpoints
          - services
          - pods
          - namespaces
        verbs:
          - list
          - watch
      - apiGroups:
          - discovery.k8s.io
        resources:
          - endpointslices
        verbs:
          - list
          - watch
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system:coredns
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: system:coredns
    subjects:
      - kind: ServiceAccount
        name: coredns
        namespace: kube-system
    ---
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: coredns
      namespace: kube-system
    data:
      Corefile: |-
        .:1053 {
            errors
            health
            ready
            rewrite name regex .*\.nodes\.vcluster\.com kubernetes.default.svc.cluster.local
            kubernetes cluster.local in-addr.arpa ip6.arpa {
                pods insecure
                fallthrough in-addr.arpa ip6.arpa
            }
            hosts /etc/NodeHosts {
                ttl 60
                reload 15s
                fallthrough
            }
            prometheus :9153
            forward . /etc/resolv.conf
            cache 30
            loop
            loadbalance
        }
      
        import /etc/coredns/custom/*.server
      NodeHosts: ""
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: coredns
      namespace: kube-system
      labels:
        k8s-app: kube-dns
        kubernetes.io/name: "CoreDNS"
    spec:
      replicas: 1
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
      selector:
        matchLabels:
          k8s-app: kube-dns
      template:
        metadata:
          labels:
            k8s-app: kube-dns
        spec:
          priorityClassName: "system-cluster-critical"
          serviceAccountName: coredns
          nodeSelector:
            kubernetes.io/os: linux
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchLabels:
                  k8s-app: kube-dns
          containers:
            - name: coredns
              image: {{.IMAGE}}
              imagePullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: 1000m
                  memory: 170Mi
                requests:
                  cpu: 3m
                  memory: 16Mi
              args: [ "-conf", "/etc/coredns/Corefile" ]
              volumeMounts:
                - name: config-volume
                  mountPath: /etc/coredns
                  readOnly: true
                - name: custom-config-volume
                  mountPath: /etc/coredns/custom
                  readOnly: true
              securityContext:
                runAsNonRoot: true
                runAsUser: {{.RUN_AS_USER}}
                runAsGroup: {{.RUN_AS_GROUP}}
                allowPrivilegeEscalation: false
                capabilities:
                  add:
                    - NET_BIND_SERVICE
                  drop:
                    - ALL
                readOnlyRootFilesystem: true
              livenessProbe:
                httpGet:
                  path: /health
                  port: 8080
                  scheme: HTTP
                initialDelaySeconds: 60
                periodSeconds: 10
                timeoutSeconds: 1
                successThreshold: 1
                failureThreshold: 3
              readinessProbe:
                httpGet:
                  path: /ready
                  port: 8181
                  scheme: HTTP
                initialDelaySeconds: 0
                periodSeconds: 2
                timeoutSeconds: 1
                successThreshold: 1
                failureThreshold: 3
          dnsPolicy: Default
          volumes:
            - name: config-volume
              configMap:
                name: coredns
                items:
                  - key: Corefile
                    path: Corefile
                  - key: NodeHosts
                    path: NodeHosts
            - name: custom-config-volume
              configMap:
                name: coredns-custom
                optional: true
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: kube-dns
      namespace: kube-system
      annotations:
        prometheus.io/port: "9153"
        prometheus.io/scrape: "true"
      labels:
        k8s-app: kube-dns
        kubernetes.io/cluster-service: "true"
        kubernetes.io/name: "CoreDNS"
    spec:
      selector:
        k8s-app: kube-dns
      type: ClusterIP
      ports:
        - name: dns
          port: 53
          targetPort: 1053
          protocol: UDP
        - name: dns-tcp
          port: 53
          targetPort: 1053
          protocol: TCP
        - name: metrics
          port: 9153
          protocol: TCP
---
# Source: mission-control-tenant/charts/vcluster/templates/init-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-init-manifests
  namespace: default
  labels:
    app: vcluster
    chart: "vcluster-0.16.4"
    release: "release-name"
    heritage: "Helm"
data:
  manifests: |-
    ---
---
# Source: mission-control-tenant/charts/vcluster/templates/rbac/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vc-release-name-v-default
  labels:
    app: vcluster
    chart: "vcluster-0.16.4"
    release: "release-name"
    heritage: "Helm"
rules:
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingressclasses"]
    verbs: ["get", "watch", "list"]    
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["get", "watch", "list"]
---
# Source: mission-control-tenant/charts/vcluster/templates/rbac/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vc-release-name-v-default
  labels:
    app: vcluster
    chart: "vcluster-0.16.4"
    release: "release-name"
    heritage: "Helm"
subjects:
  - kind: ServiceAccount
    name: vc-release-name
    namespace: default
roleRef:
  kind: ClusterRole
  name: vc-release-name-v-default
  apiGroup: rbac.authorization.k8s.io
---
# Source: mission-control-tenant/charts/vcluster/templates/rbac/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name
  namespace: default
  labels:
    app: vcluster
    chart: "vcluster-0.16.4"
    release: "release-name"
    heritage: "Helm"
rules:
  - apiGroups: [""]
    resources: ["configmaps", "secrets", "services", "pods", "pods/attach", "pods/portforward", "pods/exec", "persistentvolumeclaims"]
    verbs: ["create", "delete", "patch", "update", "get", "list", "watch"]
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["create", "delete", "patch", "update"]
  - apiGroups: [""]
    resources: ["endpoints", "events", "pods/log"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses"]
    verbs: ["create", "delete", "patch", "update", "get", "list", "watch"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "deployments"]
    verbs: ["get", "list", "watch"]
---
# Source: mission-control-tenant/charts/vcluster/templates/rbac/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name
  namespace: default
  labels:
    app: vcluster
    chart: "vcluster-0.16.4"
    release: "release-name"
    heritage: "Helm"
subjects:
  - kind: ServiceAccount
    name: vc-release-name
    namespace: default
roleRef:
  kind: Role
  name: release-name
  apiGroup: rbac.authorization.k8s.io
---
# Source: mission-control-tenant/charts/vcluster/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name
  namespace: default
  labels:
    app: vcluster
    chart: "vcluster-0.16.4"
    release: "release-name"
    heritage: "Helm"
spec:
  type: ClusterIP
  ports:
    - name: https
      port: 443
      targetPort: 8443
      protocol: TCP
    - name: kubelet
      port: 10250
      targetPort: 8443
      protocol: TCP
  selector:
    app: vcluster
    release: release-name
---
# Source: mission-control-tenant/charts/vcluster/templates/statefulset-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-headless
  namespace: default
  labels:
    app: release-name-vcluster
    chart: "vcluster-0.16.4"
    release: "release-name"
    heritage: "Helm"
spec:
  ports:
    - name: https
      port: 443
      targetPort: 8443
      protocol: TCP
  clusterIP: None
  selector:
    app: vcluster
    release: "release-name"
---
# Source: mission-control-tenant/charts/vcluster/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name
  namespace: default
  labels:
    app: vcluster
    chart: "vcluster-0.16.4"
    release: "release-name"
    heritage: "Helm"
spec:
  serviceName: release-name-headless
  replicas: 1
  selector:
    matchLabels:
      app: vcluster
      release: release-name
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 15Gi
  template:
    metadata:
      labels:
        app: vcluster
        release: release-name
    spec:
      terminationGracePeriodSeconds: 10
      nodeSelector:
        {}
      tolerations:
        []
      serviceAccountName: vc-release-name
      volumes:
        - name: helm-cache
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: config
          emptyDir: {}
        - name: coredns
          configMap:
            name: release-name-coredns
        - name: custom-config-volume
          configMap:
            name: coredns-custom
            optional: true
      containers:
      - image: rancher/k3s:v1.28.2-k3s1
        name: vcluster
        # k3s has a problem running as pid 1 and disabled agents on cgroupv2
        # nodes as it will try to evacuate the cgroups there. Starting k3s
        # through a shell makes it non pid 1 and prevents this from happening
        command:
          - /bin/sh
        args:
          - -c
          - /bin/k3s
            server
            --write-kubeconfig=/data/k3s-config/kube-config.yaml
            --data-dir=/data
            --disable=traefik,servicelb,metrics-server,local-storage,coredns
            --disable-network-policy
            --disable-agent
            --disable-cloud-controller
            --flannel-backend=none
            --kube-apiserver-arg=bind-address=127.0.0.1
            --disable-scheduler
            --kube-controller-manager-arg=controllers=*,-nodeipam,-nodelifecycle,-persistentvolume-binder,-attachdetach,-persistentvolume-expander,-cloud-node-lifecycle,-ttl
            --kube-apiserver-arg=endpoint-reconciler-type=none
            --service-cidr=$(SERVICE_CIDR)
            && true
        env:
          - name: SERVICE_CIDR
            valueFrom:
              configMapKeyRef:
                name: "vc-cidr-release-name"
                key: cidr
        securityContext:
          allowPrivilegeEscalation: false
          runAsGroup: 0
          runAsUser: 0
        volumeMounts:
          - name: config
            mountPath: /etc/rancher
          - mountPath: /data
            name: data
        resources:
          limits:
            memory: 2Gi
          requests:
            cpu: 200m
            memory: 256Mi
      - name: syncer
        image: "ghcr.io/loft-sh/vcluster:0.16.4"
        args:
          - --name=release-name
          - --kube-config=/data/k3s-config/kube-config.yaml
          - --service-account=vc-workload-release-name
          - --plugins=sync-host-secrets
          - --kube-config-context-name=my-vcluster
          - --leader-elect=false          
          - --sync=ingresses
          - --sync-all-secrets=true          
          - '--map-host-service=monitoring/grafana-agent-traces=default/grafana-agent-traces'          
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8443
            scheme: HTTPS
          failureThreshold: 60
          initialDelaySeconds: 60
          periodSeconds: 2
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8443
            scheme: HTTPS
          failureThreshold: 60
          periodSeconds: 2
        securityContext:
          allowPrivilegeEscalation: false
          runAsGroup: 0
          runAsUser: 0
        env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VCLUSTER_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: CONFIG
            value: |-
              ---
          - name: VCLUSTER_TELEMETRY_CONFIG
            value: "{\"disabled\":false,\"instanceCreator\":\"helm\",\"instanceCreatorUID\":\"\"}"
        volumeMounts:
          - name: helm-cache
            mountPath: /.cache/helm
          - name: tmp
            mountPath: /tmp
          - name: coredns
            mountPath: /manifests/coredns
            readOnly: true
          - name: custom-config-volume
            mountPath: /etc/coredns/custom
            readOnly: true
          - mountPath: /data
            name: data
        resources:
          limits:
            cpu: 1000m
            memory: 512Mi
          requests:
            cpu: 20m
            memory: 64Mi
      - image: docker.io/flanksource/vcluster-sync-host-secrets:v0.1.6
        name: "sync-host-secrets"
        env:
          - name: VCLUSTER_PLUGIN_ADDRESS
            value: "localhost:14000"
          - name: VCLUSTER_PLUGIN_NAME
            value: "sync-host-secrets"
        envFrom:
          null
        securityContext:
          null
        lifecycle:
          null
        livenessProbe:
          null
        readinessProbe:
          null
        startupProbe:
          null
        volumeDevices:
          null
        volumeMounts:
          null
---
# Source: mission-control-tenant/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mission-control-release-name
  namespace: default
  annotations:
    kubernetes.io/tls-acme: "true"
    cert-manager.io/cluster-issuer: ingress-ca
spec:
  ingressClassName: ingress-nginx
  rules:
  - host: ""
    http:
      paths:
      - backend:
          service:
            name: mission-control-x-default-x-release-name
            port:
              number: 8080
        path: /
        pathType: ImplementationSpecific
  tls:
  - hosts:
    - ""
    secretName: mission-control-tls-release-name
---
# Source: mission-control-tenant/templates/topology.yaml
apiVersion: mission-control.flanksource.com/v1
kind: Connection
metadata:
  name: mission-control
  namespace: default
spec:
  type: postgres
  url:
    value: $(username)
  username:
    valueFrom:
      secretKeyRef:
        name: incident-commander-postgres
        key: DB_URL
---
# Source: mission-control-tenant/templates/helm-release.yaml
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: release-name-mission-control-tenant
  namespace: default
  labels:
    helm.sh/chart: mission-control-tenant-1.0.44
    app.kubernetes.io/name: mission-control-tenant
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.44"
    app.kubernetes.io/managed-by: Helm
spec:
  interval: 5m
  chart:
    spec:
      chart: mission-control
      version: ">=0.0.0"
      sourceRef:
        kind: HelmRepository
        name: release-name-mission-control-tenant
      interval: 1m
  targetNamespace: default
  storageNamespace: default
  install:
    createNamespace: true
    crds: CreateReplace
  upgrade:
    crds: CreateReplace
  kubeConfig:
    secretRef:
      name: vc-release-name
      key: config
  values:
    apm-hub:
      enabled: false
    authProvider: clerk
    clerkJWKSURL: ""
    clerkOrgID: ""
    db:
      create: false
    flanksource-ui:
      enabled: false
    global:
      otel:
        collector: grafana-agent-traces.default:4317
    kratos:
      enabled: false
    properties:
      incidents.disable: "true"
      logs.disable: "true"
      settings.logging_backends.disable: "true"
---
# Source: mission-control-tenant/templates/helm-repository.yaml
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: HelmRepository
metadata:
  name: release-name-mission-control-tenant
  namespace: default
  labels:
    helm.sh/chart: mission-control-tenant-1.0.44
    app.kubernetes.io/name: mission-control-tenant
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.0.44"
    app.kubernetes.io/managed-by: Helm
spec:
  interval: 1m
  url: http://flanksource.github.io/charts
---
# Source: mission-control-tenant/templates/topology.yaml
apiVersion: canaries.flanksource.com/v1
kind: Topology
metadata:
  name: t-
  namespace: default
spec:
  icon: mission-control
  schedule: '@every 10m'
  type: Topology
  properties:
  - name: URL
    type: url
    configLookup:
      config:
        name: mission-control-default
        type: Kubernetes::Ingress
      display:
        javascript: config.spec.rules[0].host
  - name: Users
    lookup:
      postgres:
      - connection: connection://postgres/mission-control
        query: SELECT COUNT(*) FROM people WHERE name != 'System' 
        display:
          expr: results.rows[0].count
  - name: CPU
    lookup:
      prometheus:
      - query: '1000 * max(rate(container_cpu_usage_seconds_total{container!="",namespace="default"}[5m]))'
        display:
          expr: |
            [{'name': 'CPU', 'value': int(results[0].value), 'headline': true, 'unit': 'millicores'}].toJSON()
  - name: Memory
    lookup:
      prometheus:
      - query: 'max(avg_over_time(container_memory_working_set_bytes{container!="",namespace="default"}[5m]))'
        display:
          expr: |
            [{'name': 'Memory', 'value': int(results[0].value), 'headline': true, 'unit': 'bytes'}].toJSON()
  - name: Org ID
    text: 
  - name: Last Login
    lookup:
      postgres:
      - connection: connection://postgres/mission-control
        query: SELECT MAX(last_login) FROM people
        display:
          expr: results.rows[0].max
  - name: Namespace
    text: default
  components:
  - components:
    - icon: group
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            javascript: |
              JSON.stringify(results.rows.map(function(r) {return {name: r.name}}))
          name: Team names query
          query: SELECT name FROM teams
      name: Teams
      type: lookup
    icon: incidents
    name: Incident Commander
    checks:
    - inline:
        http:
        - endpoint: http://mission-control-x-default-x-default.default:8080/health
          name: incident-commander-default
          responseCodes:
          - 200
        schedule: '@every 5m'
    properties:
    - headline: true
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            expr: results.rows[0].count
          name: Incident count query
          query: SELECT count(*) FROM incidents WHERE (resolved IS NULL) OR (closed IS NULL)
      name: Incidents
    - name: Docs
      text: https://docs.flanksource.com
      type: url
    - configLookup:
        config:
          tags:
            app.kubernetes.io/name: mission-control
          type: Kubernetes::Pod
          namespace: default
        display:
          javascript: |
            config.spec.containers[0]['image'].split(':')[1]
      headline: true
      name: Version
    selectors:
    - labelSelector: app.kubernetes.io/name=mission-control
      namespace: default
    type: API
  - checks:
    - inline:
        http:
        - endpoint: http://canary-checker-x-default-x-default.default:8080/health
          name: canary-checker-default
          responseCodes:
          - 200
        schedule: '@every 5m'
    components:
    - forEach:
        properties:
        - name: Type
          text: Topology
      icon: k8s-customresourcedefinition
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            javascript: |
              JSON.stringify(results.rows.map(function(r) {return {
                name: r.name,
                properties: [
                {
                  name: 'Last runtime',
                  text: r.time_end,
                },
                {
                  name: 'Duration',
                  text: r.duration_millis,
                  unit: 'ms',
                  headline: true,
                },
                {
                  name: 'Success Count',
                  text: r.success_count,
                  headline: true,
                },
                {
                  name: 'Error Count',
                  text: r.error_count,
                  headline: true,
                },
              ],
              }}))
          name: Topologies count query
          query: |
            SELECT * FROM (
              SELECT
                ROW_NUMBER() OVER (PARTITION BY t.name ORDER BY time_end DESC) AS rn, t.name, jh.status, jh.success_count,
                jh.error_count, jh.duration_millis, jh.time_end
              FROM topologies t
              INNER JOIN job_history jh ON t.id::text = jh.resource_id) AS s WHERE rn = 1
      name: Topologies
      type: lookup
    icon: heart
    name: Canary Checker
    properties:
    - headline: true
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            expr: results.rows[0].count
          name: Component count query
          query: SELECT count(*) FROM components WHERE deleted_at IS NULL
      name: Components
    - name: Checks
      headline: true
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            expr: results.rows[0].count
          query: SELECT COUNT(*) FROM checks WHERE deleted_at IS NULL
    - name: Canaries
      headline: true
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            expr: results.rows[0].count
          query: SELECT COUNT(*) FROM canaries WHERE deleted_at IS NULL
    - name: Check Executions / Hour
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            expr: results.rows[0].count
          query: SELECT ROUND(COUNT(*)/(30*24.0), 2) AS count FROM check_statuses WHERE (NOW() - created_at) < INTERVAL '30 days'
    - name: Check Executions / Day
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          query: SELECT ROUND(COUNT(*)/(30.0), 2) AS count FROM check_statuses WHERE (NOW() - created_at) < INTERVAL '30 days'
          display:
            expr: results.rows[0].count

    - name: Docs
      text: https://docs.flanksource.com/canary-checker/overview
      type: url
    - configLookup:
        config:
          tags:
            app.kubernetes.io/name: canary-checker
          type: Kubernetes::Pod
          namespace: default
        display:
          javascript: |
            config.spec.containers[0]['image'].split(':')[1]
      headline: true
      name: Version
    selectors:
    - labelSelector: app.kubernetes.io/name=canary-checker
      namespace: default
    type: API
  - icon: logs
    name: APM Hub
    properties:
    - headline: true
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            javascript: results.rows[0].count
          name: Backends count query
          query: SELECT count(*) FROM logging_backends WHERE deleted_at IS NULL
      name: Backends
    - configLookup:
        config:
          tags:
            app.kubernetes.io/name: apm-hub
          type: Kubernetes::Pod
          namespace: default
        display:
          javascript: |
            config.spec.containers[0]['image'].split(':')[1]
      headline: true
      name: Version
    - name: Docs
      text: https://docs.flanksource.com/apm-hub/overview
      type: url
    selectors:
    - labelSelector: app.kubernetes.io/name=apm-hub
      namespace: default
    type: API
  - checks:
    - inline:
        http:
        - endpoint: http://config-db-x-default-x-default.default:8080/live
          name: config-db-default
          responseCodes:
          - 200
        schedule: '@every 5m'
    components:
    - icon: k8s-customresourcedefinition
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            javascript: |
              JSON.stringify(results.rows.map(function(r) {return {
                name: r.name,
                properties: [
                {
                  name: 'Last runtime',
                  text: r.time_end,
                },
                {
                  name: 'Duration',
                  text: r.duration_millis,
                  unit: 'ms',
                  headline: true,
                },
                {
                  name: 'Success Count',
                  text: r.success_count,
                  headline: true,
                },
                {
                  name: 'Error Count',
                  text: r.error_count,
                  headline: true,
                },
              ],
              }}))
          name: Config scrapers count query
          query: |
            SELECT * FROM (
              SELECT
                ROW_NUMBER() OVER (PARTITION BY c.name ORDER BY time_end DESC) AS rn, c.name, jh.status, jh.success_count,
                jh.error_count, jh.duration_millis, jh.time_end
              FROM config_scrapers c
              INNER JOIN job_history jh ON c.id::text = jh.resource_id) AS s WHERE rn = 1
      name: ConfigScrapers
      type: lookup
    icon: config
    name: Config DB
    properties:
    - headline: true
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            expr: results.rows[0].count
          name: Config Items count query
          query: SELECT count(*) FROM config_items WHERE deleted_at IS NULL
      name: Config Items
    - configLookup:
        config:
          tags:
            app.kubernetes.io/name: config-db
          type: Kubernetes::Pod
          namespace: default
        display:
          javascript: |
            config.spec.containers[0]['image'].split(':')[1]
      headline: true
      name: Version
    - name: Docs
      text: https://docs.flanksource.com/config-db/overview
      type: url
    selectors:
    - labelSelector: app.kubernetes.io/name=config-db
      namespace: default
    type: API
  - icon: postgres
    name: PostgreSQL
    properties:
    - lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            expr: results.rows[0].version
          name: Version query
          query: SELECT VERSION()
      name: Version
    - headline: true
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            expr: results.rows[0].pg_size_pretty
          name: Size query
          query: SELECT pg_size_pretty(pg_database_size(''))
      name: Size
    - headline: true
      lookup:
        postgres:
        - connection: connection://postgres/mission-control
          display:
            expr: results.rows[0].sum
          name: Active connections query
          query: SELECT sum(numbackends) FROM pg_stat_database WHERE datname = ''
      name: Connections
    type: Database

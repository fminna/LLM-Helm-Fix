---
# Source: yugaware-openshift/templates/pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: release-name-yugaware-pdb
spec:
  maxUnavailable: 0
  selector:
    matchLabels:
      app: release-name-yugaware
---
# Source: yugaware-openshift/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name
  labels:
    k8s-app: yugaware
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
---
# Source: yugaware-openshift/templates/global-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-yugaware-global-config
  labels:
    app: yugaware-openshift
    chart: yugaware-openshift
    release: release-name
    heritage: "Helm"
data:
  postgres_db: "eXVnYXdhcmU="
  postgres_user: "cG9zdGdyZXM="
  postgres_password: "SVhTY1kweXQ="
  app_secret: "ZEVkWllVSk1RV040V0RCdmEyMDJSR2REY0Uxb2RGbGxPSEZEU214dWFtUlhXR05GTW5GWWJUaGpWbkZTYXpaUVNqQlZSMFJKV21Jd1drZFFRMmgyYmc9PQ=="
---
# Source: yugaware-openshift/templates/configs.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-yugaware-app-config
  labels:
    app: yugaware-openshift
    chart: yugaware-openshift
    release: release-name
    heritage: "Helm"
data:
  application.docker.conf: |
    include classpath("application.common.conf")
    play.crypto.secret=${APP_SECRET}
    play.i18n.langs = [ "en" ]
    pidfile.path = "/dev/null"
    play.logger.includeConfigProperties=true
    log.override.path = "/opt/yugabyte/yugaware/data/logs"

    db {
      default.dbname=${POSTGRES_DB}
  
      default.host="127.0.0.1"
  
      default.url="jdbc:postgresql://"${db.default.host}":"${db.default.port}"/"${db.default.dbname}${db.default.params}
      default.params=""
      default.username=${POSTGRES_USER}
      default.password=${POSTGRES_PASSWORD}
  
      perf_advisor.url="jdbc:postgresql://"${db.default.host}":"${db.default.port}"/"${db.perf_advisor.dbname}${db.default.params}
      perf_advisor.createDatabaseUrl="jdbc:postgresql://"${db.default.host}":"${db.default.port}"/"${db.default.dbname}${db.default.params}
  
    }

    yb {
      cloud.enabled = false
      cloud.requestIdHeader = "X-REQUEST-ID"
      devops.home = /opt/yugabyte/devops
      metrics.host = "127.0.0.1"
      metrics.url = "http://"${yb.metrics.host}":9090/api/v1"
      metrics.management.url = "http://"${yb.metrics.host}":9090/-"
      storage.path = /opt/yugabyte/yugaware/data
      docker.network = bridge
      seedData = false
      swamper.targetPath = /opt/yugabyte/prometheus/targets
      swamper.rulesPath = /opt/yugabyte/prometheus/rules
      security.enable_auth_for_proxy_metrics = true
      proxy_endpoint_timeout = 3 minute
      multiTenant = false
      releases.path = "/opt/yugabyte/releases"
      docker.release = "/opt/yugabyte/release"
      # TODO(bogdan): need this extra level for installing from local...
      thirdparty.packagePath = /opt/third-party
      helm.packagePath = "/opt/yugabyte/helm"
      helm.timeout_secs = 900
      health.check_interval_ms = 300000
      health.status_interval_ms = 43200000
      health.default_email = ""
      health.ses_email_username = ""
      health.ses_email_password = ""
      kubernetes.storageClass = ""
      kubernetes.pullSecretName = "yugabyte-k8s-pull-secret"
      url = "https://localhost"
      # GKE MCS takes 7 to 10 minutes to setup DNS
      wait_for_server_timeout = 15 minutes
    }

    play.filters {
      # CSRF config
      csrf {
        cookie {
          # If non null, the CSRF token will be placed in a cookie with this name
          name = "csrfCookie"
          # Whether the cookie should be set to secure
          secure = false
          # Whether the cookie should have the HTTP only flag set
          httpOnly = false
        }
        # Whether to bypass CSRF check if CORS check is satisfied
        bypassCorsTrustedOrigins = false
        header {
          # The name of the header to accept CSRF tokens from.
          name = "Csrf-Token"
        }
      }
      # CORS config
      cors {
        pathPrefixes = ["/"]
        allowedOrigins = ["http://localhost"]
        # Server allows cookies/credentials to be sent with cross-origin requests
        supportsCredentials=true
        allowedHttpMethods = ["GET", "POST", "PUT", "OPTIONS", "DELETE"]
        allowedHttpHeaders = ["Accept", "Origin", "Content-Type", "X-Auth-Token", "X-AUTH-YW-API-TOKEN", "X-REQUEST-ID", ${play.filters.csrf.header.name}]
      }
    }

    # string config entries from helm values additionalAppConf

    # boolean/int config entries from helm values additionalAppConf
---
# Source: yugaware-openshift/templates/configs.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-yugaware-prometheus-config
  labels:
    app: yugaware-openshift
    chart: yugaware-openshift
    release: release-name
    heritage: "Helm"
data:
  prometheus.yml: |
    global:
        scrape_interval:     10s
        evaluation_interval: 10s
    rule_files:
      - '/opt/yugabyte/prometheus/rules/yugaware.ad.*.yml'
      - '/opt/yugabyte/prometheus/rules/yugaware.recording-rules.yml'
    scrape_configs:
      - job_name: "ocp-prometheus-federated"
        scheme: https

        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        honor_labels: true
        metrics_path: "/federate"

        params:
          'match[]':
            # kubelet metrics
            - 'kubelet_volume_stats_used_bytes{persistentvolumeclaim=~"(.*)-yb-(.*)"}'
            - 'kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"(.*)-yb-(.*)"}'
            # kubelet cadvisor metrics
            - 'container_cpu_usage_seconds_total{pod=~"(.*)yb-(.*)"}'
            - 'container_memory_working_set_bytes{pod=~"(.*)yb-(.*)"}'
            # kube-state-metrics
            # Supports >= OCP v4.4
            # OCP v4.4 has upgraded the KSM from 1.8.0 to 1.9.5.
            # https://docs.openshift.com/container-platform/4.4/release_notes/ocp-4-4-release-notes.html#ocp-4-4-cluster-monitoring-version-updates
            # - 'kube_pod_container_resource_requests_cpu_cores{pod=~"(.*)yb-(.*)"}'
            - 'kube_pod_container_resource_requests{pod=~"(.*)yb-(.*)", unit="core"}'

        static_configs:
          - targets:
            - "prometheus-k8s.openshift-monitoring.svc:9091"

        metric_relabel_configs:
          # Save the name of the metric so we can group_by since we cannot by __name__ directly...
          - source_labels: ["__name__"]
            regex: "(.*)"
            target_label: "saved_name"
            replacement: "$1"
          - source_labels: ["pod"]
            regex: "(.*)"
            target_label: "pod_name"
            replacement: "$1"
          - source_labels: ["container"]
            regex: "(.*)"
            target_label: "container_name"
            replacement: "$1"
          # rename new name of the CPU metric to the old name and label
          # ref: https://github.com/kubernetes/kube-state-metrics/blob/master/CHANGELOG.md#v200-alpha--2020-09-16
          - source_labels: ["__name__", "unit"]
            regex: "kube_pod_container_resource_requests;core"
            target_label: "__name__"
            replacement: "kube_pod_container_resource_requests_cpu_cores"

      - job_name: 'platform'
        metrics_path: "/api/v1/prometheus_metrics"
        static_configs:
          - targets: [
            '127.0.0.1:9000'
          ]

      - job_name: 'node-agent'
        metrics_path: "/metrics"
        file_sd_configs:
          - files:
            - '/opt/yugabyte/prometheus/targets/node-agent.*.json'

      - job_name: "node"
        file_sd_configs:
          - files:
            - '/opt/yugabyte/prometheus/targets/node.*.json'
        metric_relabel_configs:
          # Below relabels are required for smooth migration from node_exporter 0.13.0 to the latest
          - source_labels: ["__name__"]
            regex: "node_cpu"
            target_label: "__name__"
            replacement: "node_cpu_seconds_total"
          - source_labels: ["__name__"]
            regex: "node_filesystem_free"
            target_label: "__name__"
            replacement: "node_filesystem_free_bytes"
          - source_labels: ["__name__"]
            regex: "node_filesystem_size"
            target_label: "__name__"
            replacement: "node_filesystem_size_bytes"
          - source_labels: ["__name__"]
            regex: "node_disk_reads_completed"
            target_label: "__name__"
            replacement: "node_disk_reads_completed_total"
          - source_labels: ["__name__"]
            regex: "node_disk_writes_completed"
            target_label: "__name__"
            replacement: "node_disk_writes_completed_total"
          - source_labels: ["__name__"]
            regex: "node_memory_MemTotal"
            target_label: "__name__"
            replacement: "node_memory_MemTotal_bytes"
          - source_labels: ["__name__"]
            regex: "node_memory_Slab"
            target_label: "__name__"
            replacement: "node_memory_Slab_bytes"
          - source_labels: ["__name__"]
            regex: "node_memory_Cached"
            target_label: "__name__"
            replacement: "node_memory_Cached_bytes"
          - source_labels: ["__name__"]
            regex: "node_memory_Buffers"
            target_label: "__name__"
            replacement: "node_memory_Buffers_bytes"
          - source_labels: ["__name__"]
            regex: "node_memory_MemFree"
            target_label: "__name__"
            replacement: "node_memory_MemFree_bytes"
          - source_labels: ["__name__"]
            regex: "node_network_receive_bytes"
            target_label: "__name__"
            replacement: "node_network_receive_bytes_total"
          - source_labels: ["__name__"]
            regex: "node_network_transmit_bytes"
            target_label: "__name__"
            replacement: "node_network_transmit_bytes_total"
          - source_labels: ["__name__"]
            regex: "node_network_receive_packets"
            target_label: "__name__"
            replacement: "node_network_receive_packets_total"
          - source_labels: ["__name__"]
            regex: "node_network_transmit_packets"
            target_label: "__name__"
            replacement: "node_network_transmit_packets_total"
          - source_labels: ["__name__"]
            regex: "node_network_receive_errs"
            target_label: "__name__"
            replacement: "node_network_receive_errs_total"
          - source_labels: ["__name__"]
            regex: "node_network_transmit_errs"
            target_label: "__name__"
            replacement: "node_network_transmit_errs_total"
          - source_labels: ["__name__"]
            regex: "node_disk_bytes_read"
            target_label: "__name__"
            replacement: "node_disk_read_bytes_total"
          - source_labels: ["__name__"]
            regex: "node_disk_bytes_written"
            target_label: "__name__"
            replacement: "node_disk_written_bytes_total"
          # Save the name of the metric so we can group_by since we cannot by __name__ directly...
          - source_labels: ["__name__"]
            regex: "(.*)"
            target_label: "saved_name"
            replacement: "$1"

      - job_name: "yugabyte"
        tls_config:
          insecure_skip_verify: true
        metrics_path: "/prometheus-metrics"
        file_sd_configs:
          - files:
            - '/opt/yugabyte/prometheus/targets/yugabyte.*.json'
        metric_relabel_configs:
          # Save the name of the metric so we can group_by since we cannot by __name__ directly...
          - source_labels: ["__name__"]
            regex: "(.*)"
            target_label: "saved_name"
            replacement: "$1"
          # The following basically retrofit the handler_latency_* metrics to label format.
          - source_labels: ["__name__"]
            regex: "handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)"
            target_label: "server_type"
            replacement: "$1"
          - source_labels: ["__name__"]
            regex: "handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)"
            target_label: "service_type"
            replacement: "$2"
          - source_labels: ["__name__"]
            regex: "handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?"
            target_label: "service_method"
            replacement: "$3"
          - source_labels: ["__name__"]
            regex: "handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?"
            target_label: "__name__"
            replacement: "rpc_latency$4"
---
# Source: yugaware-openshift/templates/volumes.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: release-name-yugaware-storage
  labels:
    app: yugaware-openshift
    chart: yugaware-openshift
    release: release-name
    heritage: "Helm"
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
---
# Source: yugaware-openshift/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-yugaware-ui
  labels:
    app: release-name-yugaware
    chart: yugaware-openshift
    release: release-name
    heritage: "Helm"
spec:
  ports:
  - name: ui
    port: 80
    targetPort: 9000
  - name: metrics
    port: 9090
  selector:
    app: release-name-yugaware
  type: "LoadBalancer"
---
# Source: yugaware-openshift/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-yugaware
  labels:
    app: release-name-yugaware
    chart: yugaware-openshift
    release: release-name
    heritage: "Helm"
spec:
  serviceName: release-name-yugaware
  replicas: 1
  selector:
    matchLabels:
      app: release-name-yugaware
  template:
    metadata:
      annotations:
        checksum/config: c7417d33d2d5529b8c252c6cbc756c384b7f736ddae0e8817f312c5869c3107f
      labels:
        app: release-name-yugaware
    spec:
      serviceAccountName: release-name
      imagePullSecrets:
      - name: yugabyte-k8s-pull-secret
      volumes:
        - name: yugaware-storage
          persistentVolumeClaim:
            claimName: release-name-yugaware-storage
        - name: yugaware-ui
          emptyDir: {}
        - name: yugaware-config
          projected:
            sources:
              - configMap:
                  name: release-name-yugaware-app-config
                  items:
                    - key: application.docker.conf
                      path: application.docker.conf
        - name: prometheus-config
          configMap:
            name: release-name-yugaware-prometheus-config
            items:
              - key: prometheus.yml
                path: prometheus.yml
        - name: pg-init
          configMap:
            name: release-name-yugaware-pg-prerun
            items:
              - key: pg-prerun.sh
                path: pg-prerun.sh
      initContainers:
        - image: quay.io/yugabyte/yugaware-ubi:2.18.4.0-b52
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: "0.25"
              memory: 500Mi
          name: prometheus-configuration
          command: ["cp", "/default_prometheus_config/prometheus.yml", "/prometheus_configs/prometheus.yml"]
          volumeMounts:
          - name: prometheus-config
            mountPath: /default_prometheus_config
          - name: yugaware-storage
            mountPath: /prometheus_configs
            subPath: prometheus.yml
      containers:
        
        - name: postgres
          image: registry.redhat.io/rhscl/postgresql-13-rhel7:1-88.1661531722
          imagePullPolicy: IfNotPresent
          args:
          - "run-postgresql"
          - "-c"
          - "huge_pages=off"
          env:
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: release-name-yugaware-global-config
                  key: postgres_user
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-yugaware-global-config
                  key: postgres_password
            - name: POSTGRES_DB
              valueFrom:
                secretKeyRef:
                  name: release-name-yugaware-global-config
                  key: postgres_db
            # Hardcoded the POSTGRESQL_USER because it's mandatory env var in RH PG image
            # It doesn't have access to create the DB, so YBA fails to create the perf_advisor DB.
            # Need to use admin user of RH PG image (postgres)
            # Changing the user name won't be possible moving forward for OpenShift certified chart
            - name: POSTGRESQL_USER
              value: pg-yba
              # valueFrom:
              #   secretKeyRef:
              #     name: release-name-yugaware-global-config
              #     key: postgres_user
            - name: POSTGRESQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-yugaware-global-config
                  key: postgres_password
            - name: POSTGRESQL_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-yugaware-global-config
                  key: postgres_password
            - name: POSTGRESQL_DATABASE
              valueFrom:
                secretKeyRef:
                  name: release-name-yugaware-global-config
                  key: postgres_db
          ports:
            - containerPort: 5432
              name: postgres
          resources:
            requests:
              cpu: "0.5"
              memory: 1Gi
          

          volumeMounts:
            - name: yugaware-storage
              mountPath: /var/lib/pgsql/data
              subPath: postgres_data_13
        - name: prometheus
          image: registry.redhat.io/openshift4/ose-prometheus:v4.11.0
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: "2"
              memory: 4Gi
          

          volumeMounts:
          - name: yugaware-storage
            mountPath: /prometheus_configs
            subPath: prometheus.yml
          - name: yugaware-storage
            mountPath: /prometheus/
          - mountPath: /opt/yugabyte/yugaware/data/keys/
            name: yugaware-storage
            subPath: data/keys
          - name: yugaware-storage
            mountPath: /opt/yugabyte/prometheus/targets
            subPath: swamper_targets
          - name: yugaware-storage
            mountPath: /opt/yugabyte/prometheus/rules
            subPath: swamper_rules
          args:
            - --config.file=/prometheus_configs/prometheus.yml
            - --storage.tsdb.path=/prometheus/
            - --web.enable-admin-api
            - --web.enable-lifecycle
            - --storage.tsdb.retention.time=15d
            - --query.max-concurrency=20
            - --query.max-samples=5e+06
            - --query.timeout=30s
          ports:
            - containerPort: 9090
        - name: yugaware
          image: quay.io/yugabyte/yugaware-ubi:2.18.4.0-b52
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: "2"
              memory: 4Gi
          args: ["bin/yugaware","-Dconfig.file=/data/application.docker.conf"]
          env:
          # Conditionally set these env variables, if runAsUser is not 0(root)
          # or 10001(yugabyte).
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: release-name-yugaware-global-config
                  key: postgres_user
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-yugaware-global-config
                  key: postgres_password
            - name: POSTGRES_DB
              valueFrom:
                secretKeyRef:
                  name: release-name-yugaware-global-config
                  key: postgres_db
            - name: APP_SECRET
              valueFrom:
                secretKeyRef:
                  name: release-name-yugaware-global-config
                  key: app_secret
          ports:
            - containerPort: 9000
              name: yugaware
          volumeMounts:
          - name: yugaware-config
            mountPath: /data
          - name: yugaware-storage
            mountPath: /opt/yugabyte/yugaware/data/
            subPath: data
          # old path for backward compatibility
          - name: yugaware-storage
            mountPath: /opt/yugaware_data/
            subPath: data
          - name: yugaware-storage
            mountPath: /opt/yugabyte/releases/
            subPath: releases
          - name: yugaware-storage
            mountPath: /opt/yugabyte/ybc/releases/
            subPath: ybc_releases
          # old path for backward compatibility
          - name: yugaware-storage
            mountPath: /opt/releases/
            subPath: releases
          - name: yugaware-storage
            mountPath: /opt/yugabyte/prometheus/targets
            subPath: swamper_targets
          - name: yugaware-storage
            mountPath: /opt/yugabyte/prometheus/rules
            subPath: swamper_rules
          - name: yugaware-storage
            mountPath: /prometheus_configs
            subPath: prometheus.yml
---
# Source: yugaware-openshift/templates/certificates.yaml
# Copyright (c) YugaByte, Inc.
---
# Source: yugaware-openshift/templates/configs.yaml
# Copyright (c) YugaByte, Inc.
---
# Source: yugaware-openshift/templates/global-config.yaml
# Copyright (c) YugaByte, Inc.
---
# Source: yugaware-openshift/templates/service.yaml
# Copyright (c) YugaByte, Inc.
---
# Source: yugaware-openshift/templates/statefulset.yaml
# Copyright (c) YugaByte, Inc.
---
# Source: yugaware-openshift/templates/universe-boot-script.yaml
# Copyright (c) YugaByte, Inc.
---
# Source: yugaware-openshift/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: release-name-yugaware-test
  labels:
    app: release-name-yugaware-test
    chart: yugaware-openshift
    release: release-name
  annotations:
    "helm.sh/hook": test
spec:
  imagePullSecrets:
  - name: yugabyte-k8s-pull-secret
  containers:
    - name: yugaware-test
      image: quay.io/yugabyte/yugaware-ubi:2.18.4.0-b52
      command:
        - '/bin/bash'
        - '-ec'
        - >
          sleep 60s;
        - >
            curl --head http://release-name-yugaware-ui
      # Hard coded resources to the test pod.
      resources:
        limits:
          cpu: "1"
          memory: "512Mi"
        requests:
          cpu: "0.5"
          memory: "256Mi"
  restartPolicy: Never

---
# Source: mw-kube-agent-v2/templates/serviceaccount-update.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: mw-app
  name: mw-service-account-update
  namespace: mw-agent-ns
---
# Source: mw-kube-agent-v2/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mw-service-account
  namespace: mw-agent-ns
  labels:
    helm.sh/chart: mw-kube-agent-v2-2.0.5
    app.kubernetes.io/name: mw-kube-agent-v2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.1.2"
    app.kubernetes.io/managed-by: Helm
---
# Source: mw-kube-agent-v2/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: middleware-secret
  namespace: mw-agent-ns
  labels:
    helm.sh/chart: mw-kube-agent-v2-2.0.5
    app.kubernetes.io/name: mw-kube-agent-v2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.1.2"
    app.kubernetes.io/managed-by: Helm
type: Opaque

data:
  api-key: "WFhYWFhYWFhY"
---
# Source: mw-kube-agent-v2/templates/configmap-daemonset.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mw-daemonset-otel-config
  namespace: mw-agent-ns
  labels:
    helm.sh/chart: mw-kube-agent-v2-2.0.5
    app.kubernetes.io/name: mw-kube-agent-v2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.1.2"
    app.kubernetes.io/managed-by: Helm
data:
  otel-config: |
    receivers:
      filelog:
        include: [ /var/log/pods/*/*/*.log, $MW_LOG_PATHS ]
        include_file_path: true
        include_file_name_resolved: true
        include_file_path_resolved: true
        start_at: beginning
        multiline:
          line_start_pattern: ^\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(\.\d+)?(Z|[+-]\d{2}:?\d{2})?\s+(stdout|stderr)?\s?(\[.+?\])?\s+(.+)$
        operators:
          # Find out which format is used by kubernetes
          - type: router
            id: get-format
            routes:
              - output: parser-docker
                expr: 'body matches "^\\{"'
              - output: parser-crio
                expr: 'body matches "^[^ Z]+ "'
              - output: parser-containerd
                expr: 'body matches "^[^ Z]+Z"'
          # Parse CRI-O format
          - type: regex_parser
            id: parser-crio
            regex: '^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$'
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout_type: gotime
              layout: '2006-01-02T15:04:05.000000000-07:00'
          # Parse CRI-Containerd format
          - type: regex_parser
            id: parser-containerd
            regex: '^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$'
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout: '%Y-%m-%dT%H:%M:%S.%LZ'
          # Parse Docker format
          - type: json_parser
            id: parser-docker
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.time
              layout: '%Y-%m-%dT%H:%M:%S.%LZ'
          # Extract metadata from file path
          - type: regex_parser
            id: extract_metadata_from_filepath
            regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
            parse_from: attributes["log.file.path"]
            output: move_log_to_body
          - type: move
            id: move_log_to_body
            from: attributes.log
            to: body
            output: adding_regex_to_attributes
          # Applying custom patterns for beautifying logs :
          # -------------------------------------
          # 1. Rules for beautifying systemd logs
          # -------------------------------------
          # If systemd pattern matches -- add systemd regex to attributes
          - type: add
            if: 'body matches "ts=.+ caller=.+ level=.+ msg=.+ name=systemd duration_seconds=.+ err=.+"'
            id: adding_regex_to_attributes
            field: attributes.regex_identified
            value: ts=.+ caller=.+ level=.+ msg=.+ name=systemd duration_seconds=.+ err=.+
            output: systemd_err
          # If systemd pattern matches parsing details from body to attributes
          - type: regex_parser
            if: 'body matches "ts=.+ caller=.+ level=.+ msg=.+ name=systemd duration_seconds=.+ err=.+"'
            id: systemd_err
            regex:  'ts=(?P<systemd_err_ts>.+) caller=(?P<systemd_err_caller>.+) level=(?P<level>.+) msg=(?P<systemd_err_msg>.+) name=(?P<name>.+) duration_seconds=(?P<systemd_err_duration_seconds>.+) err="(?P<regex_resolved_body>.+)"'
            parse_from: body
            output: backup_unresolved_body
          # Copying unresolved regex body to attributes
          - type: copy
            id: backup_unresolved_body
            if: "attributes.regex_resolved_body != nil"
            from: body
            to: attributes.regex_unresolved_body
            output: systemd_err_move
          #  Moving systemd error content from attributes to body
          - type: move
            id: systemd_err_move
            if: "attributes.regex_resolved_body != nil"
            from: attributes.regex_resolved_body
            to: body
      kubeletstats:
        collection_interval: 15s
        auth_type: serviceAccount
        endpoint: "${K8S_NODE_IP}:10250"
        insecure_skip_verify: true
        k8s_api_config:
          auth_type: serviceAccount
        extra_metadata_labels: [container.id, k8s.volume.type]
      hostmetrics:
        collection_interval: 5s
        scrapers:
          cpu:
            metrics:
              system.cpu.utilization:
                enabled: true
          load:
            cpu_average: true
          memory:
            metrics:
              system.memory.utilization:
                enabled: true
          paging: {}
          disk:
            metrics:
              system.disk.io.speed:
                enabled: true
          filesystem:
            metrics:
              system.filesystem.utilization:
                enabled: true
          network:
            metrics:
              system.network.io.bandwidth:
                enabled: true
          processes: {}
          process:
            mute_process_name_error: true
      
      prometheus:
        config:
          scrape_configs:
            - job_name: "otel-collector"
              scrape_interval: 5s
              static_configs:
                - targets: ["0.0.0.0:8888"]
      
      fluentforward:
        endpoint: 0.0.0.0:8006
      # kubeletstats:
      #   ceat_path:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:9319
            # max_recv_msg_size_mib: 8
          http:
            endpoint: 0.0.0.0:9320
    exporters:
      logging:
        loglevel: fatal
      otlp:
        endpoint: ${MW_TARGET}
    processors:
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        filter:
          node_from_env_var: KUBE_NODE_NAME
        extract:
          metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
        pod_association:
          - sources:
            - from: resource_attribute
              name: k8s.pod.ip
          - sources:
            - from: resource_attribute
              name: k8s.pod.uid
          - sources:
            - from: connection
      resource:
        attributes:
          - key: host.id
            from_attribute: host.name
            action: upsert
          - key: mw.account_key
            action: insert
            value: ${MW_API_KEY}
          - key: agent.installation.time
            from_attribute: host.name
            action: insert
          - key: agent.installation.time
            value: ${MW_AGENT_INSTALLATION_TIME}
            action: update
          - key: k8s.cluster.name
            from_attribute: k8s.node.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.namespace.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.pod.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.container.name
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.replicaset.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.statefulset.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.cronjob.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.job.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.daemonset.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.deployment.uid
            action: insert
      resource/hostmetrics:
        attributes:
          - key: is.k8s.node
            action: insert
            value: "yes"
      
      resource/cluster:
        attributes:
          - key: k8s.cluster.name
            action: update
            value: unknown
          - key: host.id
            action: update
            from_attribute: k8s.node.name
          - key: host.name
            action: update
            from_attribute: k8s.node.name
      
      resource/logs:
        attributes:
          - key: service.name
            action: insert
            value: middleware-logs

      resourcedetection:
        detectors: [ env, system, docker ]
        system:
          hostname_sources: ["os"]
        timeout: 2s
        override: false
      batch:
      batch/2:
        send_batch_size: 2000
        timeout: 10s
      attributes/traces:
        actions:
          - key: mw.service.name.derived
            from_attribute: db.system
            action: insert
          - key: mw.service.name.derived
            from_attribute: messaging.system
            action: insert
          - key: mw.service.name.derived
            from_attribute: rpc.system
            action: insert
          - key: mw.service.name.derived
            from_attribute: http.scheme
            action: insert
          - key: mw.service.name.derived
            from_attribute: faas.trigger
            action: insert
      attributes/logs:
        actions:
          - key: source
            from_attribute: name
            action: upsert
          - key: source
            from_attribute: operator_type
            action: upsert
          - key: source
            from_attribute: log.file.name
            action: upsert
          - key: source
            from_attribute: fluent.tag
            action: upsert
          - key: source
            from_attribute: service.name
            action: upsert
          - key: source
            from_attribute: project.name
            action: upsert
          - key: source
            from_attribute: serviceName
            action: upsert
          - key: source
            from_attribute: projectName
            action: upsert
          - key: source
            from_attribute: pod_name
            action: upsert
          - key: source
            from_attribute: container_name
            action: upsert
          - key: source
            from_attribute: namespace
            action: upsert 
    service:
        telemetry:
          logs:
            level: "fatal"
          metrics:
            address: 0.0.0.0:8888
        pipelines:
          traces/otlp:
            receivers: [ otlp ]
            processors: [ resourcedetection,resource, resource/cluster, attributes/traces, batch, batch/2 ]
            exporters: [  otlp ]
          logs/fluentforward:
            receivers: [ fluentforward ]
            processors: [ resourcedetection, resource, resource/cluster, attributes/logs, resource/logs, k8sattributes, batch, batch/2 ]
            exporters: [ otlp ]
          logs/otlp:
            receivers: [ otlp ]
            processors: [ resourcedetection, resource, resource/cluster, attributes/logs, resource/logs, k8sattributes, batch, batch/2 ]
            exporters: [ otlp ]
          logs/filelog:
            receivers: [ filelog ]
            processors: [ resourcedetection, resource, resource/cluster, attributes/logs, resource/logs, k8sattributes, batch, batch/2 ]
            exporters: [ otlp ]
          metrics/kubeletstats:
            receivers: [ kubeletstats ]
            processors: [ resourcedetection, resource, k8sattributes, resource/cluster, batch, batch/2]
            exporters: [ otlp, logging ]
          metrics/hostmetrics:
            receivers: [ hostmetrics ]
            processors: [ resourcedetection, resource, resource/hostmetrics, resource/cluster, k8sattributes, batch, batch/2]
            exporters: [ otlp ]
---
# Source: mw-kube-agent-v2/templates/configmap-deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mw-deployment-otel-config
  namespace: mw-agent-ns
  labels:
    helm.sh/chart: mw-kube-agent-v2-2.0.5
    app.kubernetes.io/name: mw-kube-agent-v2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.1.2"
    app.kubernetes.io/managed-by: Helm
data:
  otel-config: |
    receivers:
      k8s_cluster:
        auth_type: serviceAccount
        collection_interval: 15s
        node_conditions_to_report: [ Ready, DiskPressure, MemoryPressure, PIDPressure, NetworkUnavailable ]
        distribution: kubernetes
        allocatable_types_to_report: [ cpu, memory, ephemeral-storage, storage ]
      
      prometheus:
        config:
          scrape_configs:
            - job_name: "otel-collector"
              scrape_interval: 5s
              static_configs:
                - targets: ["0.0.0.0:8888"]
      
      fluentforward:
        endpoint: 0.0.0.0:8006
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:9319
          http:
            endpoint: 0.0.0.0:9320
    exporters:
      logging:
        loglevel: warn
      otlp:
        endpoint: ${MW_TARGET}
    processors:
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        filter:
          node_from_env_var: KUBE_NODE_NAME
        extract:
          metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
        pod_association:
          - sources:
            - from: resource_attribute
              name: k8s.pod.ip
          - sources:
            - from: resource_attribute
              name: k8s.pod.uid
          - sources:
            - from: connection
      resource:
        attributes:
          - key: host.id
            from_attribute: host.name
            action: upsert
          - key: mw.account_key
            action: insert
            value: ${MW_API_KEY}
          - key: agent.installation.time
            from_attribute: host.name
            action: insert
          - key: agent.installation.time
            value: ${MW_AGENT_INSTALLATION_TIME}
            action: update
          - key: k8s.cluster.name
            from_attribute: k8s.node.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.namespace.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.pod.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.container.name
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.replicaset.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.statefulset.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.cronjob.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.job.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.daemonset.uid
            action: insert
          - key: k8s.cluster.name
            from_attribute: k8s.deployment.uid
            action: insert
      resource/hostmetrics:
        attributes:
          - key: is.k8s.node
            action: insert
            value: "yes"
      
      resource/cluster:
        attributes:
          - key: k8s.cluster.name
            action: update
            value: unknown
          - key: host.id
            action: update
            from_attribute: k8s.node.name
          - key: host.name
            action: update
            from_attribute: k8s.node.name
      
      resource/logs:
        attributes:
          - key: service.name
            action: insert
            value: middleware-logs

      resourcedetection:
        detectors: [ env, system, docker ]
        system:
          hostname_sources: ["os"]
        timeout: 2s
        override: false
      batch:
      batch/2:
        send_batch_size: 2000
        timeout: 10s
      attributes/traces:
        actions:
          - key: mw.service.name.derived
            from_attribute: db.system
            action: insert
          - key: mw.service.name.derived
            from_attribute: messaging.system
            action: insert
          - key: mw.service.name.derived
            from_attribute: rpc.system
            action: insert
          - key: mw.service.name.derived
            from_attribute: http.scheme
            action: insert
          - key: mw.service.name.derived
            from_attribute: faas.trigger
            action: insert
      attributes/logs:
        actions:
          - key: source
            from_attribute: name
            action: upsert
          - key: source
            from_attribute: operator_type
            action: upsert
          - key: source
            from_attribute: log.file.name
            action: upsert
          - key: source
            from_attribute: fluent.tag
            action: upsert
          - key: source
            from_attribute: service.name
            action: upsert
          - key: source
            from_attribute: project.name
            action: upsert
          - key: source
            from_attribute: serviceName
            action: upsert
          - key: source
            from_attribute: projectName
            action: upsert
          - key: source
            from_attribute: pod_name
            action: upsert
          - key: source
            from_attribute: container_name
            action: upsert
          - key: source
            from_attribute: namespace
            action: upsert 
    service:
        telemetry:
          logs:
            level: "fatal"
          metrics:
            address: 0.0.0.0:8888
        pipelines:
          traces/otlp:
            receivers: [ otlp ]
            processors: [ resourcedetection,resource, resource/cluster, attributes/traces, batch, batch/2 ]
            exporters: [  otlp ]
          logs/fluentforward:
            receivers: [ fluentforward ]
            processors: [ resourcedetection, resource, resource/cluster, attributes/logs, resource/logs, k8sattributes, batch, batch/2 ]
            exporters: [ otlp ]
          logs/otlp:
            receivers: [ otlp ]
            processors: [ resourcedetection, resource, resource/cluster, attributes/logs, resource/logs, k8sattributes, batch, batch/2 ]
            exporters: [ otlp ]
          metrics/prometheus:
            receivers: [ prometheus ]
            processors: [ resourcedetection, resource, k8sattributes, resource/cluster, batch, batch/2]
            exporters: [ otlp ]
          metrics/otlp:
            receivers: [ otlp ]
            processors: [ resourcedetection, resource, k8sattributes, resource/cluster, batch, batch/2]
            exporters: [ otlp ]
          metrics/k8s_cluster:
            receivers: [ k8s_cluster ]
            processors: [ resourcedetection, resource, k8sattributes, resource/cluster, batch, batch/2]
            exporters: [ otlp ]
---
# Source: mw-kube-agent-v2/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: mw-kube-agent-v2-2.0.5
    app.kubernetes.io/name: mw-kube-agent-v2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.1.2"
    app.kubernetes.io/managed-by: Helm
  name: mw-cluster-role
  namespace: mw-agent-ns
rules:
  # Allow Metrics Scraper to get metrics from the Metrics server
  - apiGroups: ["metrics.k8s.io"]
    resources: ["pods", "nodes"]
    verbs: ["get", "list", "watch"]

  # Other resources
  - apiGroups: [""]
    resources: ["nodes", "nodes/stats", "namespaces", "pods", "serviceaccounts", "services", "configmaps", "endpoints", "persistentvolumeclaims", "replicationcontrollers", "replicationcontrollers/scale", "persistentvolumeclaims", "persistentvolumes", "bindings", "events", "limitranges", "namespaces/status", "pods/log", "pods/status", "replicationcontrollers/status", "resourcequotas", "resourcequotas/status"]
    verbs: ["get", "list", "watch"]
  
  - apiGroups: ["apps"]
    resources: ["daemonsets", "deployments", "deployments/scale", "replicasets", "replicasets/scale", "statefulsets"]
    verbs: ["get", "list", "watch", "patch"]

  - apiGroups: ["autoscaling"]
    resources: ["horizontalpodautoscalers"]
    verbs: ["get", "list", "watch"]

  - apiGroups: ["batch"]
    resources: ["cronjobs", "jobs"]
    verbs: ["get", "list", "watch"]

  - apiGroups: ["extensions"]
    resources: ["daemonsets", "deployments", "deployments/scale", "networkpolicies", "replicasets", "replicasets/scale", "replicationcontrollers/scale"]
    verbs: ["get", "list", "watch"]

  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses", "networkpolicies"]
    verbs: ["get", "list", "watch"]

  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["get", "list", "watch"]

  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "volumeattachments"]
    verbs: ["get", "list", "watch"]

  - apiGroups: ["rbac.authorization.k8s.io"]
    resources: ["clusterrolebindings", "clusterroles", "roles", "rolebindings", ]
    verbs: ["get", "list", "watch"]
---
# Source: mw-kube-agent-v2/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: mw-cluster-role-binding
  namespace: mw-agent-ns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: mw-cluster-role
subjects:
  - kind: ServiceAccount
    name: mw-service-account
    namespace: mw-agent-ns
---
# Source: mw-kube-agent-v2/templates/role-update.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: mw-app
  name: mw-role-update
  namespace: mw-agent-ns
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get", "update"]
- apiGroups: ["apps"]
  resources: ["daemonsets", "deployments"]
  verbs: ["get", "update"]
---
# Source: mw-kube-agent-v2/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: mw-kube-agent-v2-2.0.5
    app.kubernetes.io/name: mw-kube-agent-v2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.1.2"
    app.kubernetes.io/managed-by: Helm
  name: mw-role
  namespace: mw-agent-ns
rules:
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: [mw-app-certs, mw-app-csrf, mw-app-key-holder]
    verbs: ["get", "update", "delete"]
  - apiGroups: [""]
    resources: ["pods", "pods/exec"]
    verbs: ["get", "list", "delete", "patch", "create"]
    # Allow Dashboard to get and update 'mw-app-settings' config map.
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: [mw-daemonset-otel-config, mw-deployment-otel-config]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
    # Allow Dashboard to get metrics.
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["heapster", "dashboard-metrics-scraper"]
    verbs: ["proxy"]
  - apiGroups: [""]
    resources: ["services/proxy"]
    resourceNames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]
    verbs: ["get"]
---
# Source: mw-kube-agent-v2/templates/rolebinding-update.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: mw-app
  name: mw-role-binding-update
  namespace: mw-agent-ns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: mw-role-update
subjects:
  - kind: ServiceAccount
    name: mw-service-account-update
    namespace: mw-agent-ns
---
# Source: mw-kube-agent-v2/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: mw-role-binding
  namespace: mw-agent-ns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: mw-role
subjects:
  - kind: ServiceAccount
    name: mw-service-account
    namespace: mw-agent-ns
---
# Source: mw-kube-agent-v2/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mw-kube-agent-svc
  namespace: mw-agent-ns
  labels:
    helm.sh/chart: mw-kube-agent-v2-2.0.5
    app.kubernetes.io/name: mw-kube-agent-v2
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.1.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      name: grpc
    - port: 9319
      targetPort: 9319
      name: grpc2
    - port: 9320
      targetPort: 9320
      name: http
    - port: 8006
      targetPort: 8006
      name: fluent
  selector:
    app.kubernetes.io/name: mw-kube-agent-v2
    app.kubernetes.io/instance: release-name
---
# Source: mw-kube-agent-v2/templates/daemonset.yaml
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: mw-kube-agent
  namespace: mw-agent-ns
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mw-kube-agent-v2
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mw-kube-agent-v2
        app.kubernetes.io/instance: release-name
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - operator: Exists
        effect: NoSchedule
      - operator: Exists
        effect: NoExecute
      hostNetwork: true
      volumes:
      - name: mw-daemonset-otel-config-volume
        configMap:
          name: mw-daemonset-otel-config
          items:
            - key: otel-config
              path: otel-config-daemonset.yaml
      # volume binding for log collection 
      - name: varlog
        hostPath:
          path: /var/log
      - name: varrun
        hostPath:
          path: /var/run/docker.sock
      - name: runcontainerd
        hostPath:
          path: /run/containerd/containerd.sock
      # volume binding for log collection
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      containers:
        - args:
          - mw-agent
          - start
          - --otel-config-file
          - /app/otel-config-daemonset.yaml
          env:
            - name: MW_TARGET
              value: XXXXXXXXX
            - name: MW_KUBE_CLUSTER_NAME
              value: unknown
            - name: MW_API_KEY
              valueFrom:
                secretKeyRef:
                  name: middleware-secret
                  key: api-key
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          image: "ghcr.io/middleware-labs/mw-kube-agent:1.6.4"
          imagePullPolicy: IfNotPresent
          name: mw-kube-agent-v2
          securityContext:
            privileged: true
          volumeMounts:
          - name: mw-daemonset-otel-config-volume
            mountPath: /app
          - mountPath: /var/log
            name: varlog
            readOnly: true
          - mountPath: /var/run/docker.sock
            name: varrun
            readOnly: true
          - mountPath: /run/containerd/containerd.sock
            name: runcontainerd
            readOnly: true
          - mountPath: /var/lib/docker/containers
            name: varlibdockercontainers
            readOnly: true
      restartPolicy: Always
      serviceAccountName: mw-service-account
---
# Source: mw-kube-agent-v2/templates/deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: mw-kube-agent
  namespace: mw-agent-ns
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mw-kube-agent-v2
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mw-kube-agent-v2
        app.kubernetes.io/instance: release-name
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - operator: Exists
        effect: NoSchedule
      - operator: Exists
        effect: NoExecute

      volumes:
        - name: mw-deployment-otel-config-volume
          configMap:
            name: mw-deployment-otel-config
            items:
              - key: otel-config
                path: otel-config-deployment.yaml
      containers:
        - args:
            - mw-agent
            - start
            - --otel-config-file
            - /app/otel-config-deployment.yaml
          volumeMounts:
            - name: mw-deployment-otel-config-volume
              mountPath: /app
          env:
            - name: MW_TARGET
              value: XXXXXXXXX
            - name: MW_KUBE_CLUSTER_NAME
              value: unknown
            - name: MW_API_KEY
              valueFrom:
                secretKeyRef:
                  name: middleware-secret
                  key: api-key
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          image: "ghcr.io/middleware-labs/mw-kube-agent:1.6.4"
          imagePullPolicy: IfNotPresent
          name: mw-kube-agent-v2
          securityContext:
            privileged: true
      restartPolicy: Always
      serviceAccountName: mw-service-account
---
# Source: mw-kube-agent-v2/templates/cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mw-kube-agent-update
  namespace: mw-agent-ns
spec:
  schedule: "*/1 * * * *"  # Adjust the schedule as needed
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: mw-app
            k8s-app: mw-app
        spec:
          tolerations:
            - key: node-role.kubernetes.io/control-plane
              effect: NoSchedule
            - key: node-role.kubernetes.io/master
              effect: NoSchedule
          containers:
            - name: mw-kube-agent
              image: "ghcr.io/middleware-labs/mw-kube-agent:1.6.4"
              imagePullPolicy: Always
              args:
                - mw-agent
                - update
              env:
                - name: MW_TARGET
                  value: XXXXXXXXX
                - name: MW_API_URL_FOR_CONFIG_CHECK
                  value: https://app.middleware.io
                - name: MW_CONFIG_CHECK_INTERVAL
                  value: "60s"
                - name: MW_KUBE_CLUSTER_NAME
                  value: unknown
                - name: MW_API_KEY
                  value: XXXXXXXXX
              securityContext:
                privileged: true
          restartPolicy: OnFailure
          serviceAccountName: mw-service-account-update
